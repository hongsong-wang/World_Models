<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.10960.pdf' target='_blank'>https://arxiv.org/pdf/2510.10960.pdf</a></span>   <span><a href='https://github.com/DanielHu197/GTR2L' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Hu, Fenqing Hu, Lidong Yang, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10960">Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety in autonomous driving (AD) remains a significant challenge, especially in highly dynamic and complex traffic environments where diverse agents interact and unexpected hazards frequently emerge. Traditional reinforcement learning (RL) methods often struggle to balance safety, efficiency, and adaptability, as they primarily focus on reward maximization without explicitly modeling risk or safety constraints. To address these limitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L) framework for safe AD. GTR2L incorporates a multi-level game-theoretic world model that jointly predicts the interactive behaviors of surrounding vehicles and their associated risks, along with an adaptive rollout horizon that adjusts dynamically based on predictive uncertainty. Furthermore, an uncertainty-aware barrier mechanism enables flexible modulation of safety boundaries. A dedicated risk modeling approach is also proposed, explicitly capturing both epistemic and aleatoric uncertainty to guide constrained policy optimization and enhance decision-making in complex environments. Extensive evaluations across diverse and safety-critical traffic scenarios show that GTR2L significantly outperforms state-of-the-art baselines, including human drivers, in terms of success rate, collision and violation reduction, and driving efficiency. The code is available at https://github.com/DanielHu197/GTR2L.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.09036.pdf' target='_blank'>https://arxiv.org/pdf/2510.09036.pdf</a></span>   <span><a href='https://xingyoujun.github.io/imowm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanrui Zhang, Zhengxian Wu, Guanxing Lu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09036">iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learned world models hold significant potential for robotic manipulation, as they can serve as simulator for real-world interactions. While extensive progress has been made in 2D video-based world models, these approaches often lack geometric and spatial reasoning, which is essential for capturing the physical structure of the 3D world. To address this limitation, we introduce iMoWM, a novel interactive world model designed to generate color images, depth maps, and robot arm masks in an autoregressive manner conditioned on actions. To overcome the high computational cost associated with three-dimensional information, we propose MMTokenizer, which unifies multi-modal inputs into a compact token representation. This design enables iMoWM to leverage large-scale pretrained VideoGPT models while maintaining high efficiency and incorporating richer physical information. With its multi-modal representation, iMoWM not only improves the visual quality of future predictions but also serves as an effective simulator for model-based reinforcement learning (MBRL) and facilitates real-world imitation learning. Extensive experiments demonstrate the superiority of iMoWM across these tasks, showcasing the advantages of multi-modal world modeling for robotic manipulation. Homepage: https://xingyoujun.github.io/imowm/
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.08713.pdf' target='_blank'>https://arxiv.org/pdf/2510.08713.pdf</a></span>   <span><a href='https://github.com/F1y1113/UniWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Dong, Fengyi Wu, Guangyu Chen, Zhi-Qi Cheng, Qiyu Hu, Yuxuan Zhou, Jingdong Sun, Jun-Yan He, Qi Dai, Alexander G Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08713">Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.08553.pdf' target='_blank'>https://arxiv.org/pdf/2510.08553.pdf</a></span>   <span><a href='https://github.com/xyz9911/Memoir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Xu, Yiyuan Pan, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08553">Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2510.08553.pdf' target='_blank'>https://arxiv.org/pdf/2510.08553.pdf</a></span>   <span><a href='https://github.com/xyz9911/Memoir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Xu, Yiyuan Pan, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08553">Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2510.04978.pdf' target='_blank'>https://arxiv.org/pdf/2510.04978.pdf</a></span>   <span><a href='https://github.com/AI4Phys/Awesome-AI-for-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04978">Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2510.04978.pdf' target='_blank'>https://arxiv.org/pdf/2510.04978.pdf</a></span>   <span><a href='https://github.com/AI4Phys/Awesome-AI-for-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04978">Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.04978.pdf' target='_blank'>https://arxiv.org/pdf/2510.04978.pdf</a></span>   <span><a href='https://github.com/AI4Phys/Awesome-AI-for-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04978">Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2510.04390.pdf' target='_blank'>https://arxiv.org/pdf/2510.04390.pdf</a></span>   <span><a href='https://github.com/eric-ai-lab/Morph4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04390">MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.04390.pdf' target='_blank'>https://arxiv.org/pdf/2510.04390.pdf</a></span>   <span><a href='https://github.com/eric-ai-lab/Morph4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04390">MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.02110.pdf' target='_blank'>https://arxiv.org/pdf/2510.02110.pdf</a></span>   <span><a href='https://koichi-saito-sony.github.io/soundreactor/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Koichi Saito, Julian Tanke, Christian Simon, Masato Ishii, Kazuki Shimada, Zachary Novack, Zhi Zhong, Akio Hayakawa, Takashi Shibuya, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02110">SoundReactor: Frame-level Online Video-to-Audio Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prevailing Video-to-Audio (V2A) generation models operate offline, assuming an entire video sequence or chunks of frames are available beforehand. This critically limits their use in interactive applications such as live content creation and emerging generative world models. To address this gap, we introduce the novel task of frame-level online V2A generation, where a model autoregressively generates audio from video without access to future video frames. Furthermore, we propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task. Our design enforces end-to-end causality and targets low per-frame latency with audio-visual synchronization. Our model's backbone is a decoder-only causal transformer over continuous audio latents. For vision conditioning, it leverages grid (patch) features extracted from the smallest variant of the DINOv2 vision encoder, which are aggregated into a single token per frame to maintain end-to-end causality and efficiency. The model is trained through a diffusion pre-training followed by consistency fine-tuning to accelerate the diffusion head decoding. On a benchmark of diverse gameplay videos from AAA titles, our model successfully generates semantically and temporally aligned, high-quality full-band stereo audio, validated by both objective and human evaluations. Furthermore, our model achieves low per-frame waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on 30FPS, 480p videos using a single H100. Demo samples are available at https://koichi-saito-sony.github.io/soundreactor/.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.02110.pdf' target='_blank'>https://arxiv.org/pdf/2510.02110.pdf</a></span>   <span><a href='https://koichi-saito-sony.github.io/soundreactor/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Koichi Saito, Julian Tanke, Christian Simon, Masato Ishii, Kazuki Shimada, Zachary Novack, Zhi Zhong, Akio Hayakawa, Takashi Shibuya, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02110">SoundReactor: Frame-level Online Video-to-Audio Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prevailing Video-to-Audio (V2A) generation models operate offline, assuming an entire video sequence or chunks of frames are available beforehand. This critically limits their use in interactive applications such as live content creation and emerging generative world models. To address this gap, we introduce the novel task of frame-level online V2A generation, where a model autoregressively generates audio from video without access to future video frames. Furthermore, we propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task. Our design enforces end-to-end causality and targets low per-frame latency with audio-visual synchronization. Our model's backbone is a decoder-only causal transformer over continuous audio latents. For vision conditioning, it leverages grid (patch) features extracted from the smallest variant of the DINOv2 vision encoder, which are aggregated into a single token per frame to maintain end-to-end causality and efficiency. The model is trained through a diffusion pre-training followed by consistency fine-tuning to accelerate the diffusion head decoding. On a benchmark of diverse gameplay videos from AAA titles, our model successfully generates semantically and temporally aligned, high-quality full-band stereo audio, validated by both objective and human evaluations. Furthermore, our model achieves low per-frame waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on 30FPS, 480p videos using a single H100. Demo samples are available at https://koichi-saito-sony.github.io/soundreactor/.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.01183.pdf' target='_blank'>https://arxiv.org/pdf/2510.01183.pdf</a></span>   <span><a href='https://github.com/JiahaoPlus/EvoWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01183">EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.01183.pdf' target='_blank'>https://arxiv.org/pdf/2510.01183.pdf</a></span>   <span><a href='https://github.com/JiahaoPlus/EvoWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01183">EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.00406.pdf' target='_blank'>https://arxiv.org/pdf/2510.00406.pdf</a></span>   <span><a href='https://vla-rft.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00406">VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.00406.pdf' target='_blank'>https://arxiv.org/pdf/2510.00406.pdf</a></span>   <span><a href='https://vla-rft.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, Weihua Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00406">VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2509.25161.pdf' target='_blank'>https://arxiv.org/pdf/2509.25161.pdf</a></span>   <span><a href='https://kunhao-liu.github.io/Rolling_Forcing_Webpage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25161">Rolling Forcing: Autoregressive Long Video Diffusion in Real Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2509.25161.pdf' target='_blank'>https://arxiv.org/pdf/2509.25161.pdf</a></span>   <span><a href='https://kunhao-liu.github.io/Rolling_Forcing_Webpage/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25161">Rolling Forcing: Autoregressive Long Video Diffusion in Real Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2509.24804.pdf' target='_blank'>https://arxiv.org/pdf/2509.24804.pdf</a></span>   <span><a href='https://github.com/Ultraman-Tiga1/DyMoDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxuan Zhang, Runqing Wang, Wei Xiao, Weipu Zhang, Jian Sun, Gao Huang, Jie Chen, Gang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24804">DyMoDreamer: World Modeling with Dynamic Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A critical bottleneck in deep reinforcement learning (DRL) is sample inefficiency, as training high-performance agents often demands extensive environmental interactions. Model-based reinforcement learning (MBRL) mitigates this by building world models that simulate environmental dynamics and generate synthetic experience, improving sample efficiency. However, conventional world models process observations holistically, failing to decouple dynamic objects and temporal features from static backgrounds. This approach is computationally inefficient, especially for visual tasks where dynamic objects significantly influence rewards and decision-making performance. To address this, we introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic modulation mechanism to improve the extraction of dynamic features and enrich the temporal information. DyMoDreamer employs differential observations derived from a novel inter-frame differencing mask, explicitly encoding object-level motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic categorical distributions and integrated into a recurrent state-space model (RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k benchmark with a $156.6$\% mean human-normalized score, establishes a new record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\% performance improvement after $1$M steps on the Crafter benchmark. Our code is released at https://github.com/Ultraman-Tiga1/DyMoDreamer.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2509.24804.pdf' target='_blank'>https://arxiv.org/pdf/2509.24804.pdf</a></span>   <span><a href='https://github.com/Ultraman-Tiga1/DyMoDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boxuan Zhang, Runqing Wang, Wei Xiao, Weipu Zhang, Jian Sun, Gao Huang, Jie Chen, Gang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24804">DyMoDreamer: World Modeling with Dynamic Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A critical bottleneck in deep reinforcement learning (DRL) is sample inefficiency, as training high-performance agents often demands extensive environmental interactions. Model-based reinforcement learning (MBRL) mitigates this by building world models that simulate environmental dynamics and generate synthetic experience, improving sample efficiency. However, conventional world models process observations holistically, failing to decouple dynamic objects and temporal features from static backgrounds. This approach is computationally inefficient, especially for visual tasks where dynamic objects significantly influence rewards and decision-making performance. To address this, we introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic modulation mechanism to improve the extraction of dynamic features and enrich the temporal information. DyMoDreamer employs differential observations derived from a novel inter-frame differencing mask, explicitly encoding object-level motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic categorical distributions and integrated into a recurrent state-space model (RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k benchmark with a $156.6$\% mean human-normalized score, establishes a new record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\% performance improvement after $1$M steps on the Crafter benchmark. Our code is released at https://github.com/Ultraman-Tiga1/DyMoDreamer.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.24591.pdf' target='_blank'>https://arxiv.org/pdf/2509.24591.pdf</a></span>   <span><a href='https://haozhuo-zhang.github.io/PoseDiff-project-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24591">PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2509.24591.pdf' target='_blank'>https://arxiv.org/pdf/2509.24591.pdf</a></span>   <span><a href='https://haozhuo-zhang.github.io/PoseDiff-project-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhuo Zhang, Michele Caprio, Jing Shao, Qiang Zhang, Jian Tang, Shanghang Zhang, Wei Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24591">PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2509.24313.pdf' target='_blank'>https://arxiv.org/pdf/2509.24313.pdf</a></span>   <span><a href='https://github.com/TUM-AVS/Learning-to-Sample' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Korbinian Moller, Roland Stroop, Mattia Piccinini, Alexander Langmann, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24313">Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling-based motion planning is a well-established approach in autonomous driving, valued for its modularity and analytical tractability. In complex urban scenarios, however, uniform or heuristic sampling often produces many infeasible or irrelevant trajectories. We address this limitation with a hybrid framework that learns where to sample while keeping trajectory generation and evaluation fully analytical and verifiable. A reinforcement learning (RL) agent guides the sampling process toward regions of the action space likely to yield feasible trajectories, while evaluation and final selection remains governed by deterministic feasibility checks and cost functions. We couple the RL sampler with a world model (WM) based on a decodable deep set encoder, enabling both variable numbers of traffic participants and reconstructable latent representations. The approach is evaluated in the CommonRoad simulation environment, showing up to 99% fewer required samples and a runtime reduction of up to 84% while maintaining planning quality in terms of success and collision-free rates. These improvements lead to faster, more reliable decision-making for autonomous vehicles in urban environments, achieving safer and more responsive navigation under real-world constraints. Code and trained artifacts are publicly available at: https://github.com/TUM-AVS/Learning-to-Sample
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2509.24313.pdf' target='_blank'>https://arxiv.org/pdf/2509.24313.pdf</a></span>   <span><a href='https://github.com/TUM-AVS/Learning-to-Sample' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Korbinian Moller, Roland Stroop, Mattia Piccinini, Alexander Langmann, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24313">Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sampling-based motion planning is a well-established approach in autonomous driving, valued for its modularity and analytical tractability. In complex urban scenarios, however, uniform or heuristic sampling often produces many infeasible or irrelevant trajectories. We address this limitation with a hybrid framework that learns where to sample while keeping trajectory generation and evaluation fully analytical and verifiable. A reinforcement learning (RL) agent guides the sampling process toward regions of the action space likely to yield feasible trajectories, while evaluation and final selection remains governed by deterministic feasibility checks and cost functions. We couple the RL sampler with a world model (WM) based on a decodable deep set encoder, enabling both variable numbers of traffic participants and reconstructable latent representations. The approach is evaluated in the CommonRoad simulation environment, showing up to 99% fewer required samples and a runtime reduction of up to 84% while maintaining planning quality in terms of success and collision-free rates. These improvements lead to faster, more reliable decision-making for autonomous vehicles in urban environments, achieving safer and more responsive navigation under real-world constraints. Code and trained artifacts are publicly available at: https://github.com/TUM-AVS/Learning-to-Sample
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2509.21797.pdf' target='_blank'>https://arxiv.org/pdf/2509.21797.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/MoWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21797">MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.21797.pdf' target='_blank'>https://arxiv.org/pdf/2509.21797.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/MoWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21797">MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2509.21797.pdf' target='_blank'>https://arxiv.org/pdf/2509.21797.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/MoWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21797">MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2509.21797.pdf' target='_blank'>https://arxiv.org/pdf/2509.21797.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/MoWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21797">MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2509.21790.pdf' target='_blank'>https://arxiv.org/pdf/2509.21790.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/Longscape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21790">LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2509.21790.pdf' target='_blank'>https://arxiv.org/pdf/2509.21790.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/Longscape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21790">LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2509.19080.pdf' target='_blank'>https://arxiv.org/pdf/2509.19080.pdf</a></span>   <span><a href='https://world4rl.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhennan Jiang, Kai Liu, Yuxin Qin, Shuai Tian, Yupeng Zheng, Mingcai Zhou, Chao Yu, Haoran Li, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19080">World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2509.19080.pdf' target='_blank'>https://arxiv.org/pdf/2509.19080.pdf</a></span>   <span><a href='https://world4rl.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhennan Jiang, Kai Liu, Yuxin Qin, Shuai Tian, Yupeng Zheng, Mingcai Zhou, Chao Yu, Haoran Li, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19080">World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2509.12201.pdf' target='_blank'>https://arxiv.org/pdf/2509.12201.pdf</a></span>   <span><a href='https://yangzhou24.github.io/OmniWorld/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Kaipeng Zhang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12201">OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2509.12201.pdf' target='_blank'>https://arxiv.org/pdf/2509.12201.pdf</a></span>   <span><a href='https://yangzhou24.github.io/OmniWorld/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Kaipeng Zhang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12201">OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2509.07996.pdf' target='_blank'>https://arxiv.org/pdf/2509.07996.pdf</a></span>   <span><a href='https://github.com/worldbench/survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07996">3D and 4D World Modeling: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2509.07996.pdf' target='_blank'>https://arxiv.org/pdf/2509.07996.pdf</a></span>   <span><a href='https://github.com/worldbench/survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07996">3D and 4D World Modeling: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2509.07945.pdf' target='_blank'>https://arxiv.org/pdf/2509.07945.pdf</a></span>   <span><a href='https://github.com/opendilab/LightZero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Pu, Yazhe Niu, Jia Tang, Junyu Xiong, Shuai Hu, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07945">One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In heterogeneous multi-task decision-making, tasks not only exhibit diverse observation and action spaces but also vary substantially in their underlying complexities. While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling a broad and diverse suite of tasks, gradient conflicts and the loss of model plasticity often constrain their sample efficiency. In this work, we address these challenges from two complementary perspectives: the single learning iteration and the overall learning process. First, to mitigate the gradient conflicts, we systematically investigate key architectural designs for extending UniZero. Our investigation identifies a Mixture-of-Experts (MoE) architecture as the most effective approach. We demonstrate, both theoretically and empirically, that this architecture alleviates gradient conflicts by routing task-specific representations to specialized sub-networks. This finding leads to our proposed model, \textit{ScaleZero}. Second, to dynamically allocate model capacity throughout the learning process, we introduce an online Dynamic Parameter Scaling (DPS) strategy. This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion. Evaluations on a diverse set of standard benchmarks (Atari, DMC, Jericho) demonstrate that ScaleZero, utilizing solely online reinforcement learning with one model, performs on par with specialized single-task agents. With the DPS strategy, it remains competitive while using just 71.5% of the environment interactions. These findings underscore the potential of ScaleZero for effective multi-task planning. Our code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2509.07945.pdf' target='_blank'>https://arxiv.org/pdf/2509.07945.pdf</a></span>   <span><a href='https://github.com/opendilab/LightZero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Pu, Yazhe Niu, Jia Tang, Junyu Xiong, Shuai Hu, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07945">One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In heterogeneous multi-task decision-making, tasks not only exhibit diverse observation and action spaces but also vary substantially in their underlying complexities. While conventional multi-task world models like UniZero excel in single-task settings, we find that when handling a broad and diverse suite of tasks, gradient conflicts and the loss of model plasticity often constrain their sample efficiency. In this work, we address these challenges from two complementary perspectives: the single learning iteration and the overall learning process. First, to mitigate the gradient conflicts, we systematically investigate key architectural designs for extending UniZero. Our investigation identifies a Mixture-of-Experts (MoE) architecture as the most effective approach. We demonstrate, both theoretically and empirically, that this architecture alleviates gradient conflicts by routing task-specific representations to specialized sub-networks. This finding leads to our proposed model, \textit{ScaleZero}. Second, to dynamically allocate model capacity throughout the learning process, we introduce an online Dynamic Parameter Scaling (DPS) strategy. This strategy progressively integrates LoRA adapters in response to task-specific progress, enabling adaptive knowledge retention and parameter expansion. Evaluations on a diverse set of standard benchmarks (Atari, DMC, Jericho) demonstrate that ScaleZero, utilizing solely online reinforcement learning with one model, performs on par with specialized single-task agents. With the DPS strategy, it remains competitive while using just 71.5% of the environment interactions. These findings underscore the potential of ScaleZero for effective multi-task planning. Our code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2509.05735.pdf' target='_blank'>https://arxiv.org/pdf/2509.05735.pdf</a></span>   <span><a href='https://github.com/swsychen/Offline_vs_Online_in_MBRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Ji Shi, Cansu Sancaktar, Jonas Frey, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05735">Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2509.05735.pdf' target='_blank'>https://arxiv.org/pdf/2509.05735.pdf</a></span>   <span><a href='https://github.com/swsychen/Offline_vs_Online_in_MBRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Ji Shi, Cansu Sancaktar, Jonas Frey, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05735">Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2509.01793.pdf' target='_blank'>https://arxiv.org/pdf/2509.01793.pdf</a></span>   <span><a href='https://github.com/ARY2260/stori,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ARY2260/stori' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Amit Barsainyan, Jing Yu Lim, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01793">STORI: A Benchmark and Taxonomy for Stochastic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) techniques have achieved impressive performance on simulated benchmarks such as Atari100k, yet recent advances remain largely confined to simulation and show limited transfer to real-world domains. A central obstacle is environmental stochasticity, as real systems involve noisy observations, unpredictable dynamics, and non-stationary conditions that undermine the stability of current methods. Existing benchmarks rarely capture these uncertainties and favor simplified settings where algorithms can be tuned to succeed. The absence of a well-defined taxonomy of stochasticity further complicates evaluation, as robustness to one type of stochastic perturbation, such as sticky actions, does not guarantee robustness to other forms of uncertainty. To address this critical gap, we introduce STORI (STOchastic-ataRI), a benchmark that systematically incorporates diverse stochastic effects and enables rigorous evaluation of RL techniques under different forms of uncertainty. We propose a comprehensive five-type taxonomy of environmental stochasticity and demonstrate systematic vulnerabilities in state-of-the-art model-based RL algorithms through targeted evaluation of DreamerV3 and STORM. Our findings reveal that world models dramatically underestimate environmental variance, struggle with action corruption, and exhibit unreliable dynamics under partial observability. We release the code and benchmark publicly at https://github.com/ARY2260/stori, providing a unified framework for developing more robust RL systems.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2509.01793.pdf' target='_blank'>https://arxiv.org/pdf/2509.01793.pdf</a></span>   <span><a href='https://github.com/ARY2260/stori,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ARY2260/stori' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Amit Barsainyan, Jing Yu Lim, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01793">STORI: A Benchmark and Taxonomy for Stochastic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) techniques have achieved impressive performance on simulated benchmarks such as Atari100k, yet recent advances remain largely confined to simulation and show limited transfer to real-world domains. A central obstacle is environmental stochasticity, as real systems involve noisy observations, unpredictable dynamics, and non-stationary conditions that undermine the stability of current methods. Existing benchmarks rarely capture these uncertainties and favor simplified settings where algorithms can be tuned to succeed. The absence of a well-defined taxonomy of stochasticity further complicates evaluation, as robustness to one type of stochastic perturbation, such as sticky actions, does not guarantee robustness to other forms of uncertainty. To address this critical gap, we introduce STORI (STOchastic-ataRI), a benchmark that systematically incorporates diverse stochastic effects and enables rigorous evaluation of RL techniques under different forms of uncertainty. We propose a comprehensive five-type taxonomy of environmental stochasticity and demonstrate systematic vulnerabilities in state-of-the-art model-based RL algorithms through targeted evaluation of DreamerV3 and STORM. Our findings reveal that world models dramatically underestimate environmental variance, struggle with action corruption, and exhibit unreliable dynamics under partial observability. We release the code and benchmark publicly at https://github.com/ARY2260/stori, providing a unified framework for developing more robust RL systems.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2508.19320.pdf' target='_blank'>https://arxiv.org/pdf/2508.19320.pdf</a></span>   <span><a href='https://chenmingthu.github.io/milm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, Xiaoqiang Liu, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19320">MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with heavy computational cost and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2508.18269.pdf' target='_blank'>https://arxiv.org/pdf/2508.18269.pdf</a></span>   <span><a href='https://irpn-lab.github.io/FlowVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18269">FlowVLA: Thinking in Motion with a Visual Chain of Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Vision-Language-Action (VLA) models are built upon an internal world model trained via direct next-frame prediction ($v_t \rightarrow v_{t+1}$). This paradigm, however, presents a fundamental challenge: it \textbf{conflates} the task of predicting physical motion with that of rendering static appearance, forcing a single mechanism to handle both. This inherent coupling often leads to physically implausible forecasts and inefficient policy learning. To address this limitation, we introduce the \textbf{Visual Chain of Thought (Visual CoT)}, a framework that disentangles these processes by compelling the model to first reason about \textbf{motion dynamics} before generating the future frame's \textbf{visual appearance}. We instantiate this principle by proposing \textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction. By forcing the model to first commit to a motion plan ($f_t$), FlowVLA learns disentangled dynamics, resulting in more coherent visual predictions and significantly more efficient policy learning. Experiments on challenging robotics manipulation benchmarks demonstrate that FlowVLA achieves state-of-the-art performance with substantially improved sample efficiency, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2508.18269.pdf' target='_blank'>https://arxiv.org/pdf/2508.18269.pdf</a></span>   <span><a href='https://irpn-lab.github.io/FlowVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18269">FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2508.18269.pdf' target='_blank'>https://arxiv.org/pdf/2508.18269.pdf</a></span>   <span><a href='https://irpn-lab.github.io/FlowVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, Haoang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18269">FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Vision-Language-Action (VLA) models are built upon an internal world model trained via next-frame prediction ``$v_t \rightarrow v_{t+1}$''. However, this paradigm attempts to predict the future frame's appearance directly, without explicitly reasoning about the underlying dynamics. \textbf{This lack of an explicit motion reasoning step} often leads to physically implausible visual forecasts and inefficient policy learning. To address this limitation, we introduce the \textbf{Visual Chain of Thought (Visual CoT)}, a paradigm that compels the model to first reason about \textbf{motion dynamics} before generating the future frame. We instantiate this paradigm by proposing \textbf{FlowVLA}, an autoregressive Transformer that explicitly materializes this reasoning process as ``$v_t \rightarrow f_t \rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction that inherently encodes motion. By forcing the model to first follow the motion plan encoded by $f_t$, this process inherently \textbf{aligns the pre-training objective of dynamics prediction with the downstream task of action generation.} We conduct experiments on challenging robotics manipulation benchmarks, as well as real-robot evaluations. Our FlowVLA not only generates \textbf{more coherent and physically plausible visual predictions}, but also achieves state-of-the-art policy performance with \textbf{substantially improved sample efficiency}, pointing toward a more principled foundation for world modeling in VLAs. Project page: https://irpn-lab.github.io/FlowVLA/
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2508.17298.pdf' target='_blank'>https://arxiv.org/pdf/2508.17298.pdf</a></span>   <span><a href='https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fucai Ke, Joy Hsu, Zhixi Cai, Zixian Ma, Xin Zheng, Xindi Wu, Sukai Huang, Weiqing Wang, Pari Delir Haghighi, Gholamreza Haffari, Ranjay Krishna, Jiajun Wu, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17298">Explain Before You Answer: A Survey on Compositional Visual Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2508.13073.pdf' target='_blank'>https://arxiv.org/pdf/2508.13073.pdf</a></span>   <span><a href='https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13073">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2508.13073.pdf' target='_blank'>https://arxiv.org/pdf/2508.13073.pdf</a></span>   <span><a href='https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13073">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2508.02159.pdf' target='_blank'>https://arxiv.org/pdf/2508.02159.pdf</a></span>   <span><a href='https://github.com/hggforget/PIGDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongchi Huang, Jiaqi Wang, Yang Li, Chunhe Xia, Tianle Zhang, Kaige Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02159">PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial observability presents a significant challenge for Safe Reinforcement Learning (Safe RL), as it impedes the identification of potential risks and rewards. Leveraging specific types of privileged information during training to mitigate the effects of partial observability has yielded notable empirical successes. In this paper, we propose Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs) to theoretically examine the advantages of incorporating privileged information in Safe RL. Building upon ACPOMDPs, we propose the Privileged Information Guided Dreamer (PIGDreamer), a model-based RL approach that leverages privileged information to enhance the agent's safety and performance through privileged representation alignment and an asymmetric actor-critic structure. Our empirical results demonstrate that PIGDreamer significantly outperforms existing Safe RL methods. Furthermore, compared to alternative privileged RL methods, our approach exhibits enhanced performance, robustness, and efficiency. Codes are available at: https://github.com/hggforget/PIGDreamer.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2508.02159.pdf' target='_blank'>https://arxiv.org/pdf/2508.02159.pdf</a></span>   <span><a href='https://github.com/hggforget/PIGDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongchi Huang, Jiaqi Wang, Yang Li, Chunhe Xia, Tianle Zhang, Kaige Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02159">PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial observability presents a significant challenge for Safe Reinforcement Learning (Safe RL), as it impedes the identification of potential risks and rewards. Leveraging specific types of privileged information during training to mitigate the effects of partial observability has yielded notable empirical successes. In this paper, we propose Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs) to theoretically examine the advantages of incorporating privileged information in Safe RL. Building upon ACPOMDPs, we propose the Privileged Information Guided Dreamer (PIGDreamer), a model-based RL approach that leverages privileged information to enhance the agent's safety and performance through privileged representation alignment and an asymmetric actor-critic structure. Our empirical results demonstrate that PIGDreamer significantly outperforms existing Safe RL methods. Furthermore, compared to alternative privileged RL methods, our approach exhibits enhanced performance, robustness, and efficiency. Codes are available at: https://github.com/hggforget/PIGDreamer.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2507.16038.pdf' target='_blank'>https://arxiv.org/pdf/2507.16038.pdf</a></span>   <span><a href='https://neuroailab.github.io/spelke_net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rahul Venkatesh, Klemen Kotar, Lilian Naing Chen, Seungwoo Kim, Luca Thomas Wheeler, Jared Watrous, Ashley Xu, Gia Ancone, Wanhee Lee, Honglin Chen, Daniel Bear, Stefan Stojanov, Daniel Yamins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16038">Discovering and using Spelke segments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objects--groupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains a wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, a class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under a poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for "statistical counterfactual probing", where diverse "virtual pokes" are applied on regions of high motion-affordance, and the resultant expected displacement maps are used define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in a variety of off-the-shelf object manipulation models.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2507.13162.pdf' target='_blank'>https://arxiv.org/pdf/2507.13162.pdf</a></span>   <span><a href='https://lmb-freiburg.github.io/orbis.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://lmb-freiburg.github.io/orbis.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arian Mousakhan, Sudhanshu Mittal, Silvio Galesso, Karim Farid, Thomas Brox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13162">Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2507.12508.pdf' target='_blank'>https://arxiv.org/pdf/2507.12508.pdf</a></span>   <span><a href='https://umass-embodied-agi.github.io/MindJourney' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12508">MindJourney: Test-Time Scaling with World Models for Spatial Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2507.10539.pdf' target='_blank'>https://arxiv.org/pdf/2507.10539.pdf</a></span>   <span><a href='https://github.com/ulab-uiuc/GWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Feng, Yexin Wu, Guanyu Lin, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10539">Graph World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our code for GWM is released at https://github.com/ulab-uiuc/GWM.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2507.09144.pdf' target='_blank'>https://arxiv.org/pdf/2507.09144.pdf</a></span>   <span><a href='https://github.com/lzzzzzm/II-World' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimin Liao, Ping Wei, Ruijie Zhang, Shuaijia Chen, Haoxuan Wang, Ziyang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09144">$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Forecasting the evolution of 3D scenes and generating unseen scenarios via occupancy-based world models offers substantial potential for addressing corner cases in autonomous driving systems. While tokenization has revolutionized image and video generation, efficiently tokenizing complex 3D scenes remains a critical challenge for 3D world models. To address this, we propose $I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method decouples scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer employs a multi-scale residual quantization strategy to hierarchically compress 3D scenes while preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. This dual design preserves the compactness of 3D tokenizers while retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder architecture. The encoder aggregates spatial context from the current scene and predicts a transformation matrix to enable high-level control over scene generation. The decoder, conditioned on this matrix and historical tokens, ensures temporal consistency during generation. Experiments demonstrate that $I^{2}$-World achieves state-of-the-art performance, outperforming existing methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while exhibiting exceptional computational efficiency: it requires merely 2.9 GB of training memory and achieves real-time inference at 37.0 FPS. Our code is available on https://github.com/lzzzzzm/II-World.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2507.09082.pdf' target='_blank'>https://arxiv.org/pdf/2507.09082.pdf</a></span>   <span><a href='https://neuroailab.github.io/projects/kl_tracing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungwoo Kim, Khai Loong Aw, Klemen Kotar, Cristobal Eyzaguirre, Wanhee Lee, Yunong Liu, Jared Watrous, Stefan Stojanov, Juan Carlos Niebles, Jiajun Wu, Daniel L. K. Yamins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09082">Taming generative video models for zero-shot optical flow extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2507.08885.pdf' target='_blank'>https://arxiv.org/pdf/2507.08885.pdf</a></span>   <span><a href='https://embodiedcity.github.io/AirScape/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baining Zhao, Rongze Tang, Mingyuan Jia, Ziyou Wang, Fanghang Man, Xin Zhang, Yu Shang, Weichen Zhang, Wei Wu, Chen Gao, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08885">AirScape: An Aerial Generative World Model with Motion Controllability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to enable agents to predict the outcomes of their own motion intentions in three-dimensional space has been a fundamental problem in embodied intelligence. To explore general spatial imagination capability, we present AirScape, the first world model designed for six-degree-of-freedom aerial agents. AirScape predicts future observation sequences based on current visual inputs and motion intentions. Specifically, we construct a dataset for aerial world model training and testing, which consists of 11k video-intention pairs. This dataset includes first-person-view videos capturing diverse drone actions across a wide range of scenarios, with over 1,000 hours spent annotating the corresponding motion intentions. Then we develop a two-phase schedule to train a foundation model--initially devoid of embodied spatial knowledge--into a world model that is controllable by motion intentions and adheres to physical spatio-temporal constraints. Experimental results demonstrate that AirScape significantly outperforms existing foundation models in 3D spatial imagination capabilities, especially with over a 50% improvement in metrics reflecting motion alignment. The project is available at: https://embodiedcity.github.io/AirScape/.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2507.04002.pdf' target='_blank'>https://arxiv.org/pdf/2507.04002.pdf</a></span>   <span><a href='https://github.com/lynn-yu/NRSeg' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lynn-yu/NRSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Li, Fei Teng, Yihong Cao, Kailun Yang, Zhiyong Li, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04002">NRSeg: Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Birds' Eye View (BEV) semantic segmentation is an indispensable perception task in end-to-end autonomous driving systems. Unsupervised and semi-supervised learning for BEV tasks, as pivotal for real-world applications, underperform due to the homogeneous distribution of the labeled data. In this work, we explore the potential of synthetic data from driving world models to enhance the diversity of labeled data for robustifying BEV segmentation. Yet, our preliminary findings reveal that generation noise in synthetic data compromises efficient BEV model learning. To fully harness the potential of synthetic data from world models, this paper proposes NRSeg, a noise-resilient learning framework for BEV semantic segmentation. Specifically, a Perspective-Geometry Consistency Metric (PGCM) is proposed to quantitatively evaluate the guidance capability of generated data for model learning. This metric originates from the alignment measure between the perspective road mask of generated data and the mask projected from the BEV labels. Moreover, a Bi-Distribution Parallel Prediction (BiDPP) is designed to enhance the inherent robustness of the model, where the learning process is constrained through parallel prediction of multinomial and Dirichlet distributions. The former efficiently predicts semantic probabilities, whereas the latter adopts evidential deep learning to realize uncertainty quantification. Furthermore, a Hierarchical Local Semantic Exclusion (HLSE) module is designed to address the non-mutual exclusivity inherent in BEV semantic segmentation tasks. Experimental results demonstrate that NRSeg achieves state-of-the-art performance, yielding the highest improvements in mIoU of 13.8% and 11.4% in unsupervised and semi-supervised BEV segmentation tasks, respectively. The source code will be made publicly available at https://github.com/lynn-yu/NRSeg.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2507.01823.pdf' target='_blank'>https://arxiv.org/pdf/2507.01823.pdf</a></span>   <span><a href='https://github.com/dmytro-kuzmenko/td-mpc-opt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01823">TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach to knowledge transfer in model-based reinforcement learning, addressing the critical challenge of deploying large world models in resource-constrained environments. Our method efficiently distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) on the MT30 benchmark, significantly improving performance across diverse tasks. Our distilled model achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93. This improvement demonstrates the ability of our distillation technique to capture and consolidate complex multi-task knowledge. We further optimize the distilled model through FP16 post-training quantization, reducing its size by $\sim$50\%. Our approach addresses practical deployment limitations and offers insights into knowledge representation in large world models, paving the way for more efficient and accessible multi-task reinforcement learning systems in robotics and other resource-constrained applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2507.01424.pdf' target='_blank'>https://arxiv.org/pdf/2507.01424.pdf</a></span>   <span><a href='https://zhenyangliu.github.io/TriVLA/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyang Liu, Yongchong Gu, Sixiao Zheng, Yanwei Fu, Xiangyang Xue, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01424">TriVLA: A Triple-System-Based Unified Vision-Language-Action Model with Episodic World Modeling for General Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language models (VLMs) have enabled robots to follow open-ended instructions and demonstrate impressive commonsense reasoning. However, current vision-language-action (VLA) frameworks primarily rely on static representations and limited temporal context, restricting agents to short-horizon, reactive behaviors and hindering robust generalization in dynamic embodied environments. Inspired by cognitive neuroscience theories of episodic memory, we propose, to our knowledge, one of the first formalized episodic world models in VLA, enabling embodied robots to accumulate, recall, and predict sequential experiences. As an instantiation of this concept, our unified TriVLA realizes the episodic world model through a triple-system architecture: integrating multimodal grounding from a pretrained VLM (System 2) and temporally rich dynamics perception from a video diffusion model (System 3). This enables the agent to accumulate and recall sequential experiences, interpret current contexts, and predict future environmental evolution. Guided by episodic representations that span both the past and anticipated future, the downstream policy (System 1) generates coherent, context-aware action sequences through flow-matching and cross-modal attention mechanisms. Experimental results show that TriVLA operates efficiently at approximately 36 Hz and consistently outperforms baseline models on standard benchmarks and challenging real-world manipulation tasks. It demonstrates strong long-horizon planning and open-ended intent understanding, showcasing the advantages of episodic world model-inspired reasoning for robust, generalizable robot intelligence. Project Page: https://zhenyangliu.github.io/TriVLA/.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2507.00917.pdf' target='_blank'>https://arxiv.org/pdf/2507.00917.pdf</a></span>   <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00917">A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2507.00917.pdf' target='_blank'>https://arxiv.org/pdf/2507.00917.pdf</a></span>   <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00917">A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2507.00603.pdf' target='_blank'>https://arxiv.org/pdf/2507.00603.pdf</a></span>   <span><a href='https://github.com/ucaszyp/World4Drive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00603">World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving directly generates planning trajectories from raw sensor data, yet it typically relies on costly perception supervision to extract scene information. A critical research challenge arises: constructing an informative driving world model to enable perception annotation-free, end-to-end planning via self-supervised learning. In this paper, we present World4Drive, an end-to-end autonomous driving framework that employs vision foundation models to build latent world models for generating and evaluating multi-modal planning trajectories. Specifically, World4Drive first extracts scene features, including driving intention and world latent representations enriched with spatial-semantic priors provided by vision foundation models. It then generates multi-modal planning trajectories based on current scene features and driving intentions and predicts multiple intention-driven future states within the latent space. Finally, it introduces a world model selector module to evaluate and select the best trajectory. We achieve perception annotation-free, end-to-end planning through self-supervised alignment between actual future observations and predicted observations reconstructed from the latent space. World4Drive achieves state-of-the-art performance without manual perception annotations on both the open-loop nuScenes and closed-loop NavSim benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower collision rate, and 3.75 faster training convergence. Codes will be accessed at https://github.com/ucaszyp/World4Drive.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2506.24113.pdf' target='_blank'>https://arxiv.org/pdf/2506.24113.pdf</a></span>   <span><a href='https://kevin-thu.github.io/Epona/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kevin-thu/Epona/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, Wei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24113">Epona: Autoregressive Diffusion World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2506.23919.pdf' target='_blank'>https://arxiv.org/pdf/2506.23919.pdf</a></span>   <span><a href='https://world4omni.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Chen, Bangjun Wang, Jingxiang Guo, Tianrui Zhang, Yiwen Hou, Xuchuan Huang, Chenrui Tie, Lin Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23919">World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improving data efficiency and generalization in robotic manipulation remains a core challenge. We propose a novel framework that leverages a pre-trained multimodal image-generation model as a world model to guide policy learning. By exploiting its rich visual-semantic representations and strong generalization across diverse scenes, the model generates open-ended future state predictions that inform downstream manipulation. Coupled with zero-shot low-level control modules, our approach enables general-purpose robotic manipulation without task-specific training. Experiments in both simulation and real-world environments demonstrate that our method achieves effective performance across a wide range of manipulation tasks with no additional data collection or fine-tuning. Supplementary materials are available on our website: https://world4omni.github.io/.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2506.23513.pdf' target='_blank'>https://arxiv.org/pdf/2506.23513.pdf</a></span>   <span><a href='https://becauseimbatman0.github.io/ViewPoint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixun Fang, Kai Zhu, Zhiheng Liu, Yu Liu, Wei Zhai, Yang Cao, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23513">ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic video generation aims to synthesize 360-degree immersive videos, holding significant importance in the fields of VR, world models, and spatial intelligence. Existing works fail to synthesize high-quality panoramic videos due to the inherent modality gap between panoramic data and perspective data, which constitutes the majority of the training data for modern diffusion models. In this paper, we propose a novel framework utilizing pretrained perspective video models for generating panoramic videos. Specifically, we design a novel panorama representation named ViewPoint map, which possesses global spatial continuity and fine-grained visual details simultaneously. With our proposed Pano-Perspective attention mechanism, the model benefits from pretrained perspective priors and captures the panoramic spatial correlations of the ViewPoint map effectively. Extensive experiments demonstrate that our method can synthesize highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance and surpassing previous methods.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2506.23468.pdf' target='_blank'>https://arxiv.org/pdf/2506.23468.pdf</a></span>   <span><a href='https://github.com/Feliciaxyao/NavMorph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Yao, Junyu Gao, Changsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23468">NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at https://github.com/Feliciaxyao/NavMorph.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2506.23135.pdf' target='_blank'>https://arxiv.org/pdf/2506.23135.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/RoboScape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23135">RoboScape: Physics-informed Embodied World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2506.23126.pdf' target='_blank'>https://arxiv.org/pdf/2506.23126.pdf</a></span>   <span><a href='https://suninghuang19.github.io/particleformer_page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suning Huang, Qianzhong Chen, Xiaohan Zhang, Jiankai Sun, Mac Schwager
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23126">ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D world models (i.e., learning-based 3D dynamics models) offer a promising approach to generalizable robotic manipulation by capturing the underlying physics of environment evolution conditioned on robot actions. However, existing 3D world models are primarily limited to single-material dynamics using a particle-based Graph Neural Network model, and often require time-consuming 3D scene reconstruction to obtain 3D particle tracks for training. In this work, we present ParticleFormer, a Transformer-based point cloud world model trained with a hybrid point cloud reconstruction loss, supervising both global and local dynamics features in multi-material, multi-object robot interactions. ParticleFormer captures fine-grained multi-object interactions between rigid, deformable, and flexible materials, trained directly from real-world robot perception data without an elaborate scene reconstruction. We demonstrate the model's effectiveness both in 3D scene forecasting tasks, and in downstream manipulation tasks using a Model Predictive Control (MPC) policy. In addition, we extend existing dynamics learning benchmarks to include diverse multi-material, multi-object interaction scenarios. We validate our method on six simulation and three real-world experiments, where it consistently outperforms leading baselines by achieving superior dynamics prediction accuracy and less rollout error in downstream visuomotor tasks. Experimental videos are available at https://suninghuang19.github.io/particleformer_page/.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2506.23074.pdf' target='_blank'>https://arxiv.org/pdf/2506.23074.pdf</a></span>   <span><a href='https://github.com/yzheng97/CDAL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yzheng97/CDAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zheng, Boyang Gong, Fanye Kong, Yueqi Duan, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jiwen Lu, Jie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23074">Learning Counterfactually Decoupled Attention for Open-World Model Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a Counterfactually Decoupled Attention Learning (CDAL) method for open-world model attribution. Existing methods rely on handcrafted design of region partitioning or feature space, which could be confounded by the spurious statistical correlations and struggle with novel attacks in open-world scenarios. To address this, CDAL explicitly models the causal relationships between the attentional visual traces and source model attribution, and counterfactually decouples the discriminative model-specific artifacts from confounding source biases for comparison. In this way, the resulting causal effect provides a quantification on the quality of learned attention maps, thus encouraging the network to capture essential generation patterns that generalize to unseen source models by maximizing the effect. Extensive experiments on existing open-world model attribution benchmarks show that with minimal computational overhead, our method consistently improves state-of-the-art models by large margins, particularly for unseen novel attacks. Source code: https://github.com/yzheng97/CDAL.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2506.21539.pdf' target='_blank'>https://arxiv.org/pdf/2506.21539.pdf</a></span>   <span><a href='https://github.com/alibaba-damo-academy/WorldVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21539">WorldVLA: Towards Autoregressive Action World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2506.19842.pdf' target='_blank'>https://arxiv.org/pdf/2506.19842.pdf</a></span>   <span><a href='https://github.com/April-Yz/ManiGaussian_Bimanual' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengbo Yu, Guanxing Lu, Zaijia Yang, Haoyuan Deng, Season Si Chen, Jiwen Lu, Wenbo Ding, Guoqiang Hu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19842">ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2506.18701.pdf' target='_blank'>https://arxiv.org/pdf/2506.18701.pdf</a></span>   <span><a href='https://github.com/SkyworkAI/Matrix-Game' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, Yahui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18701">Matrix-Game: Interactive World Foundation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2506.14198.pdf' target='_blank'>https://arxiv.org/pdf/2506.14198.pdf</a></span>   <span><a href='https://amplify-robotics.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy A. Collins, LorÃ¡nd Cheng, Kunal Aneja, Albert Wilcox, Benjamin Joffe, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14198">AMPLIFY: Actionless Motion Priors for Robot Learning from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2506.13260.pdf' target='_blank'>https://arxiv.org/pdf/2506.13260.pdf</a></span>   <span><a href='https://github.com/synsin0/COME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Shi, Kun Jiang, Qiang Meng, Ke Wang, Jiabao Wang, Wenchao Sun, Tuopu Wen, Mengmeng Yang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13260">COME: Adding Scene-Centric Forecasting Control to Occupancy World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are critical for autonomous driving to simulate environmental dynamics and generate synthetic data. Existing methods struggle to disentangle ego-vehicle motion (perspective shifts) from scene evolvement (agent interactions), leading to suboptimal predictions. Instead, we propose to separate environmental changes from ego-motion by leveraging the scene-centric coordinate systems. In this paper, we introduce COME: a framework that integrates scene-centric forecasting Control into the Occupancy world ModEl. Specifically, COME first generates ego-irrelevant, spatially consistent future features through a scene-centric prediction branch, which are then converted into scene condition using a tailored ControlNet. These condition features are subsequently injected into the occupancy world model, enabling more accurate and controllable future occupancy predictions. Experimental results on the nuScenes-Occ3D dataset show that COME achieves consistent and significant improvements over state-of-the-art (SOTA) methods across diverse configurations, including different input sources (ground-truth, camera-based, fusion-based occupancy) and prediction horizons (3s and 8s). For example, under the same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7% better mIoU metric than UniScene. These results highlight the efficacy of disentangled representation learning in enhancing spatio-temporal prediction fidelity for world models. Code and videos will be available at https://github.com/synsin0/COME.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2506.11526.pdf' target='_blank'>https://arxiv.org/pdf/2506.11526.pdf</a></span>   <span><a href='https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, Jan Frederik Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11526">Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2506.10975.pdf' target='_blank'>https://arxiv.org/pdf/2506.10975.pdf</a></span>   <span><a href='https://chen-wl20.github.io/GenWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiliang Chen, Wenzhao Zheng, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu, Yueqi Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10975">GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The flourishing of video generation technologies has endangered the credibility of real-world information and intensified the demand for AI-generated video detectors. Despite some progress, the lack of high-quality real-world datasets hinders the development of trustworthy detectors. In this paper, we propose GenWorld, a large-scale, high-quality, and real-world simulation dataset for AI-generated video detection. GenWorld features the following characteristics: (1) Real-world Simulation: GenWorld focuses on videos that replicate real-world scenarios, which have a significant impact due to their realism and potential influence; (2) High Quality: GenWorld employs multiple state-of-the-art video generation models to provide realistic and high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes videos generated from diverse generators and various prompt modalities (e.g., text, image, video), offering the potential to learn more generalizable forensic features. We analyze existing methods and find they fail to detect high-quality videos generated by world models (i.e., Cosmos), revealing potential drawbacks of ignoring real-world clues. To address this, we propose a simple yet effective model, SpannDetector, to leverage multi-view consistency as a strong criterion for real-world AI-generated video detection. Experiments show that our method achieves superior results, highlighting a promising direction for explainable AI-generated video detection based on physical plausibility. We believe that GenWorld will advance the field of AI-generated video detection. Project Page: https://chen-wl20.github.io/GenWorld
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2506.05419.pdf' target='_blank'>https://arxiv.org/pdf/2506.05419.pdf</a></span>   <span><a href='https://github.com/JeongsooHa/DrG.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongsoo Ha, Kyungsoo Kim, Yusung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05419">Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) has been used to efficiently solve vision-based control tasks in highdimensional image observations. Although recent MBRL algorithms perform well in trained observations, they fail when faced with visual distractions in observations. These task-irrelevant distractions (e.g., clouds, shadows, and light) may be constantly present in real-world scenarios. In this study, we propose a novel self-supervised method, Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and world model with dual contrastive learning which efficiently captures task-relevant features among multi-view data augmentations. We also introduce a recurrent state inverse dynamics model that helps the world model to better understand the temporal structure. The proposed methods can enhance the robustness of the world model against visual distractions. To evaluate the generalization performance, we first train Dr. G on simple backgrounds and then test it on complex natural video backgrounds in the DeepMind Control suite, and the randomizing environments in Robosuite. Dr. G yields a performance improvement of 117% and 14% over prior works, respectively. Our code is open-sourced and available at https://github.com/JeongsooHa/DrG.git
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2506.05294.pdf' target='_blank'>https://arxiv.org/pdf/2506.05294.pdf</a></span>   <span><a href='https://github.com/arnavkj1995/SAILOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, Gokul Swamy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05294">A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fundamental limitation of the behavioral cloning (BC) approach to imitation learning is that it only teaches an agent what the expert did at states the expert visited. This means that when a BC agent makes a mistake which takes them out of the support of the demonstrations, they often don't know how to recover from it. In this sense, BC is akin to giving the agent the fish -- giving them dense supervision across a narrow set of states -- rather than teaching them to fish: to be able to reason independently about achieving the expert's outcome even when faced with unseen situations at test-time. In response, we explore learning to search (L2S) from expert demonstrations, i.e. learning the components required to, at test time, plan to match expert outcomes, even after making a mistake. These include (1) a world model and (2) a reward model. We carefully ablate the set of algorithmic and design decisions required to combine these and other components for stable and sample/interaction-efficient learning of recovery behavior without additional human corrections. Across a dozen visual manipulation tasks from three benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs state-of-the-art Diffusion Policies trained via BC on the same data. Furthermore, scaling up the amount of demonstrations used for BC by 5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can identify nuanced failures and is robust to reward hacking. Our code is available at https://github.com/arnavkj1995/SAILOR .
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2506.03706.pdf' target='_blank'>https://arxiv.org/pdf/2506.03706.pdf</a></span>   <span><a href='https://github.com/adityagandhamal/OV-COAST/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Gandhamal, Aniruddh Sikdar, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03706">OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) entails assigning semantic labels to each pixel in an image using textual descriptions, typically leveraging world models such as CLIP. To enhance out-of-domain generalization, we propose Cost Aggregation with Optimal Transport (OV-COAST) for open-vocabulary semantic segmentation. To align visual-language features within the framework of optimal transport theory, we employ cost volume to construct a cost matrix, which quantifies the distance between two distributions. Our approach adopts a two-stage optimization strategy: in the first stage, the optimal transport problem is solved using cost volume via Sinkhorn distance to obtain an alignment solution; in the second stage, this solution is used to guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS models on the MESS benchmark, where our approach notably improves the performance of the cost-aggregation model CAT-Seg with ViT-B backbone, achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 % mIoU. The code is available at https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2506.02612.pdf' target='_blank'>https://arxiv.org/pdf/2506.02612.pdf</a></span>   <span><a href='https://github.com/jrobine/sgf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Robine, Marc HÃ¶ftmann, Stefan Harmeling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02612">Simple, Good, Fast: Self-Supervised World Models Free of Baggage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGF's connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2506.01546.pdf' target='_blank'>https://arxiv.org/pdf/2506.01546.pdf</a></span>   <span><a href='https://Wang-Xiaodong1899.github.io/longdwm/' target='_blank'>  GitHub</a></span> <span><a href='https://wang-xiaodong1899.github.io/longdwm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Wang, Zhirong Wu, Peixi Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01546">LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving world models are used to simulate futures by video generation based on the condition of the current state and actions. However, current models often suffer serious error accumulations when predicting the long-term future, which limits the practical application. Recent studies utilize the Diffusion Transformer (DiT) as the backbone of driving world models to improve learning flexibility. However, these models are always trained on short video clips (high fps and short duration), and multiple roll-out generations struggle to produce consistent and reasonable long videos due to the training-inference gap. To this end, we propose several solutions to build a simple yet effective long-term driving world model. First, we hierarchically decouple world model learning into large motion learning and bidirectional continuous motion learning. Then, considering the continuity of driving scenes, we propose a simple distillation method where fine-grained video flows are self-supervised signals for coarse-grained flows. The distillation is designed to improve the coherence of infinite video generation. The coarse-grained and fine-grained modules are coordinated to generate long-term and temporally coherent videos. In the public benchmark NuScenes, compared with the state-of-the-art front-view model, our model improves FVD by $27\%$ and reduces inference time by $85\%$ for the video task of generating 110+ frames. More videos (including 90s duration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2506.01299.pdf' target='_blank'>https://arxiv.org/pdf/2506.01299.pdf</a></span>   <span><a href='https://github.com/NJU-RL/SICQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmei Liu, Fuhong Liu, Jianye Hao, Bo Wang, Huaxiong Li, Chunlin Chen, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01299">Scalable In-Context Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2506.01299.pdf' target='_blank'>https://arxiv.org/pdf/2506.01299.pdf</a></span>   <span><a href='https://github.com/NJU-RL/SICQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmei Liu, Fuhong Liu, Jianye Hao, Bo Wang, Huaxiong Li, Chunlin Chen, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01299">Scalable In-Context Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2506.01299.pdf' target='_blank'>https://arxiv.org/pdf/2506.01299.pdf</a></span>   <span><a href='https://github.com/NJU-RL/SICQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmei Liu, Fuhong Liu, Jianye Hao, Bo Wang, Huaxiong Li, Chunlin Chen, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01299">Scalable In-Context Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2505.22421.pdf' target='_blank'>https://arxiv.org/pdf/2505.22421.pdf</a></span>   <span><a href='https://github.com/antonioo-c/GeoDrive' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22421">GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2505.19717.pdf' target='_blank'>https://arxiv.org/pdf/2505.19717.pdf</a></span>   <span><a href='https://hucebot.github.io/extremum_flow_matching_website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quentin Rouxel, Clemente Donoso, Fei Chen, Serena Ivaldi, Jean-Baptiste Mouret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19717">Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning is a promising approach for enabling generalist capabilities in humanoid robots, but its scaling is fundamentally constrained by the scarcity of high-quality expert demonstrations. This limitation can be mitigated by leveraging suboptimal, open-ended play data, often easier to collect and offering greater diversity. This work builds upon recent advances in generative modeling, specifically Flow Matching, an alternative to Diffusion models. We introduce a method for estimating the minimum or maximum of the learned distribution by leveraging the unique properties of Flow Matching, namely, deterministic transport and support for arbitrary source distributions. We apply this method to develop several goal-conditioned imitation and reinforcement learning algorithms based on Flow Matching, where policies are conditioned on both current and goal observations. We explore and compare different architectural configurations by combining core components, such as critic, planner, actor, or world model, in various ways. We evaluated our agents on the OGBench benchmark and analyzed how different demonstration behaviors during data collection affect performance in a 2D non-prehensile pushing task. Furthermore, we validated our approach on real hardware by deploying it on the Talos humanoid robot to perform complex manipulation tasks based on high-dimensional image observations, featuring a sequence of pick-and-place and articulated object manipulation in a realistic kitchen environment. Experimental videos and code are available at: https://hucebot.github.io/extremum_flow_matching_website/
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2505.16377.pdf' target='_blank'>https://arxiv.org/pdf/2505.16377.pdf</a></span>   <span><a href='https://ys-qu.github.io/vlsafe-website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16377">VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of "safety" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: https://ys-qu.github.io/vlsafe-website/
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2505.14357.pdf' target='_blank'>https://arxiv.org/pdf/2505.14357.pdf</a></span>   <span><a href='http://knightnemo.github.io/vid2world/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14357">Vid2World: Crafting Video Diffusion Models to Interactive World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models, which predict transitions based on history observation and action sequences, have shown great promise in improving data efficiency for sequential decision making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments. In contrast, video diffusion models trained on large, internet-scale datasets have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World performs casualization of a pre-trained video diffusion model by crafting its architecture and training objective to enable autoregressive generation. Furthermore, it introduces a causal action guidance mechanism to enhance action controllability in the resulting interactive world model. Extensive experiments in robot manipulation and game simulation domains show that our method offers a scalable and effective approach for repurposing highly capable video diffusion models to interactive world models.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2505.14357.pdf' target='_blank'>https://arxiv.org/pdf/2505.14357.pdf</a></span>   <span><a href='http://knightnemo.github.io/vid2world/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14357">Vid2World: Crafting Video Diffusion Models to Interactive World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models, which predict future transitions from past observation and action sequences, have shown great promise for improving data efficiency in sequential decision-making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their usefulness in complex environments. In contrast, video diffusion models trained on large-scale internet data have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World systematically explores video diffusion causalization, reshaping both the architecture and training objective of pre-trained models to enable autoregressive generation. Additionally, it incorporates a causal action guidance mechanism to enhance action controllability in the resulting interactive world models. Extensive experiments across multiple domains, including robot manipulation, 3D game simulation, and open-world navigation, demonstrate that our method offers a scalable and effective pathway for repurposing highly capable video diffusion models into interactive world models.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2505.14357.pdf' target='_blank'>https://arxiv.org/pdf/2505.14357.pdf</a></span>   <span><a href='http://knightnemo.github.io/vid2world/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqiao Huang, Jialong Wu, Qixing Zhou, Shangchen Miao, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14357">Vid2World: Crafting Video Diffusion Models to Interactive World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models, which predict future transitions from past observation and action sequences, have shown great promise for improving data efficiency in sequential decision-making. However, existing world models often require extensive domain-specific training and still produce low-fidelity, coarse predictions, limiting their usefulness in complex environments. In contrast, video diffusion models trained on large-scale internet data have demonstrated impressive capabilities in generating high-quality videos that capture diverse real-world dynamics. In this work, we present Vid2World, a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. To bridge the gap, Vid2World systematically explores video diffusion causalization, reshaping both the architecture and training objective of pre-trained models to enable autoregressive generation. Additionally, it incorporates a causal action guidance mechanism to enhance action controllability in the resulting interactive world models. Extensive experiments across multiple domains, including robot manipulation, 3D game simulation, and open-world navigation, demonstrate that our method offers a scalable and effective pathway for repurposing highly capable video diffusion models into interactive world models.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2505.13934.pdf' target='_blank'>https://arxiv.org/pdf/2505.13934.pdf</a></span>   <span><a href='https://thuml.github.io/RLVR-World/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Wu, Shaofeng Yin, Ningya Feng, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13934">RLVR-World: Training World Models with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models predict state transitions in response to actions and are increasingly developed across diverse modalities. However, standard training objectives such as maximum likelihood estimation (MLE) often misalign with task-specific goals of world models, i.e., transition prediction metrics like accuracy or perceptual quality. In this paper, we present RLVR-World, a unified framework that leverages reinforcement learning with verifiable rewards (RLVR) to directly optimize world models for such metrics. Despite formulating world modeling as autoregressive prediction of tokenized sequences, RLVR-World evaluates metrics of decoded predictions as verifiable rewards. We demonstrate substantial performance gains on both language- and video-based world models across domains, including text games, web navigation, and robot manipulation. Our work indicates that, beyond recent advances in reasoning language models, RLVR offers a promising post-training paradigm for enhancing the utility of generative models more broadly.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2505.13211.pdf' target='_blank'>https://arxiv.org/pdf/2505.13211.pdf</a></span>   <span><a href='https://github.com/SandAI-org/MagiAttention' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SandAI-org/MAGI-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, Yuqiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13211">MAGI-1: Autoregressive Video Generation at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MAGI-1, a world model that generates videos by autoregressively predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 facilitates controllable generation via chunk-wise prompting and supports real-time, memory-efficient deployment by maintaining constant peak inference cost, regardless of video length. The largest variant of MAGI-1 comprises 24 billion parameters and supports context lengths of up to 4 million tokens, demonstrating the scalability and robustness of our approach. The code and models are available at https://github.com/SandAI-org/MAGI-1 and https://github.com/SandAI-org/MagiAttention. The product can be accessed at https://sand.ai.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2505.12705.pdf' target='_blank'>https://arxiv.org/pdf/2505.12705.pdf</a></span>   <span><a href='https://github.com/NVIDIA/GR00T-Dreams' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, Linxi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12705">DreamGen: Unlocking Generalization in Robot Learning through Video World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DreamGen, a simple yet highly effective 4-stage pipeline for training robot policies that generalize across behaviors and environments through neural trajectories - synthetic robot data generated from video world models. DreamGen leverages state-of-the-art image-to-video generative models, adapting them to the target robot embodiment to produce photorealistic synthetic videos of familiar or novel tasks in diverse environments. Since these models generate only videos, we recover pseudo-action sequences using either a latent action model or an inverse-dynamics model (IDM). Despite its simplicity, DreamGen unlocks strong behavior and environment generalization: a humanoid robot can perform 22 new behaviors in both seen and unseen environments, while requiring teleoperation data from only a single pick-and-place task in one environment. To evaluate the pipeline systematically, we introduce DreamGen Bench, a video generation benchmark that shows a strong correlation between benchmark performance and downstream policy success. Our work establishes a promising new axis for scaling robot learning well beyond manual data collection. Code available at https://github.com/NVIDIA/GR00T-Dreams.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2505.10819.pdf' target='_blank'>https://arxiv.org/pdf/2505.10819.pdf</a></span>   <span><a href='https://topwasu.github.io/poe-world' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10819">PoE-World: Compositional World Modeling with Products of Programmatic Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2505.10075.pdf' target='_blank'>https://arxiv.org/pdf/2505.10075.pdf</a></span>   <span><a href='https://sharinka0715.github.io/FlowDreamer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Guo, Xiaojian Ma, Yikai Wang, Min Yang, Huaping Liu, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10075">FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates training better visual world models for robot manipulation, i.e., models that can predict future visual observations by conditioning on past frames and robot actions. Specifically, we consider world models that operate on RGB-D frames (RGB-D world models). As opposed to canonical approaches that handle dynamics prediction mostly implicitly and reconcile it with visual rendering in a single model, we introduce FlowDreamer, which adopts 3D scene flow as explicit motion representations. FlowDreamer first predicts 3D scene flow from past frame and action conditions with a U-Net, and then a diffusion model will predict the future frame utilizing the scene flow. FlowDreamer is trained end-to-end despite its modularized nature. We conduct experiments on 4 different benchmarks, covering both video prediction and visual planning tasks. The results demonstrate that FlowDreamer achieves better performance compared to other baseline RGB-D world models by 7% on semantic similarity, 11% on pixel quality, and 6% on success rate in various robot manipulation domains.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2505.09723.pdf' target='_blank'>https://arxiv.org/pdf/2505.09723.pdf</a></span>   <span><a href='https://annaj2178.github.io/EnerverseAC.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, Guanghui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09723">EnerVerse-AC: Envisioning Embodied Environments with Action Condition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world model that generates future visual observations based on an agent's predicted actions, enabling realistic and controllable robotic inference. Building on prior architectures, EVAC introduces a multi-level action-conditioning mechanism and ray map encoding for dynamic multi-view image generation while expanding training data with diverse failure trajectories to improve generalization. As both a data engine and evaluator, EVAC augments human-collected trajectories into diverse datasets and generates realistic, action-conditioned video observations for policy testing, eliminating the need for physical robots or complex simulations. This approach significantly reduces costs while maintaining high fidelity in robotic manipulation evaluation. Extensive experiments validate the effectiveness of our method. Code, checkpoints, and datasets can be found at <https://annaj2178.github.io/EnerverseAC.github.io>.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2505.09694.pdf' target='_blank'>https://arxiv.org/pdf/2505.09694.pdf</a></span>   <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AgibotTech/EWMBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09694">EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2505.09305.pdf' target='_blank'>https://arxiv.org/pdf/2505.09305.pdf</a></span>   <span><a href='https://github.com/jackyzengl/EIIR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/jackyzengl/EIIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Zhang, Chenhao Zhang, Zhaobo Xu, Qinghongbing Xie, Jinliang Hou, Pingfa Feng, Long Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09305">Embodied intelligent industrial robotics: Concepts and techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to work more efficiently, accurately, reliably, and safely in industrial scenarios, robots should have at least general knowledge, working-environment knowledge, and operating-object knowledge. These pose significant challenges to existing embodied intelligent robotics (EIR) techniques. Thus, this paper first briefly reviews the history of industrial robotics and analyzes the limitations of mainstream EIR frameworks. Then, a knowledge-driven technical framework of embodied intelligent industrial robotics (EIIR) is proposed for various industrial environments. It has five modules: a world model, a high-level task planner, a low-level skill controller, a simulator, and a physical system. The development of techniques related to each module are also thoroughly reviewed, and recent progress regarding their adaption to industrial applications are discussed. A case study is given to demonstrate the newly proposed EIIR framework's applicability to real-world assembly system. Finally, the key challenges that EIIR encounters in industrial scenarios are summarized and future research directions are suggested. The authors believe that EIIR technology is shaping the next generation of industrial robotics and EIIR-based industrial systems supply a new technological paradigm for intelligent manufacturing. It is expected that this review could serve as a valuable reference for scholars and engineers that are interested in industrial embodied intelligence. Together, scholars can use this research to drive their rapid advancement and application of EIIR techniques. The interested authors would continue to track and contribute new studies in the project page https://github.com/jackyzengl/EIIR.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2505.07257.pdf' target='_blank'>https://arxiv.org/pdf/2505.07257.pdf</a></span>   <span><a href='https://github.com/ArronDZhang/DARLR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Ruihong Qiu, Xuwei Xu, Jiajun Liu, Sen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07257">DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based offline reinforcement learning (RL) has emerged as a promising approach for recommender systems, enabling effective policy learning by interacting with frozen world models. However, the reward functions in these world models, trained on sparse offline logs, often suffer from inaccuracies. Specifically, existing methods face two major limitations in addressing this challenge: (1) deterministic use of reward functions as static look-up tables, which propagates inaccuracies during policy learning, and (2) static uncertainty designs that fail to effectively capture decision risks and mitigate the impact of these inaccuracies. In this work, a dual-agent framework, DARLR, is proposed to dynamically update world models to enhance recommendation policies. To achieve this, a \textbf{\textit{selector}} is introduced to identify reference users by balancing similarity and diversity so that the \textbf{\textit{recommender}} can aggregate information from these users and iteratively refine reward estimations for dynamic reward shaping. Further, the statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty to better align with evolving recommendation requirements. Extensive experiments on four benchmark datasets demonstrate the superior performance of DARLR, validating its effectiveness. The code is available at https://github.com/ArronDZhang/DARLR.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2505.00779.pdf' target='_blank'>https://arxiv.org/pdf/2505.00779.pdf</a></span>   <span><a href='https://cmu-intentlab.github.io/UNISafe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwon Seo, Kensuke Nakamura, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00779">Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model's epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space-spanning both the latent representation and the epistemic uncertainty-we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions. Video results can be found on the project website at https://cmu-intentlab.github.io/UNISafe
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2505.00779.pdf' target='_blank'>https://arxiv.org/pdf/2505.00779.pdf</a></span>   <span><a href='https://cmu-intentlab.github.io/UNISafe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwon Seo, Kensuke Nakamura, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00779">Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model's epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space-spanning both the latent representation and the epistemic uncertainty-we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions. Video results can be found on the project website at https://cmu-intentlab.github.io/UNISafe
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2504.21024.pdf' target='_blank'>https://arxiv.org/pdf/2504.21024.pdf</a></span>   <span><a href='https://github.com/Tencent/SelfEvolvingAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21024">WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability. Code is available at https://github.com/Tencent/SelfEvolvingAgent
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2504.15785.pdf' target='_blank'>https://arxiv.org/pdf/2504.15785.pdf</a></span>   <span><a href='https://github.com/elated-sawyer/WALL-E' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15785">WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environment's dynamics usually bottlenecks LLMs' performance as world models. To bridge the gap, we propose a training-free "world alignment" that learns an environment's symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agents' policies. We further propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future steps' actions by interacting with the neurosymbolic world model. While the LLM agent's strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2504.15485.pdf' target='_blank'>https://arxiv.org/pdf/2504.15485.pdf</a></span>   <span><a href='https://github.com/atinpothiraj/CAPTURe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15485">CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty in counting in images. Code and data: https://github.com/atinpothiraj/CAPTURe
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2504.15046.pdf' target='_blank'>https://arxiv.org/pdf/2504.15046.pdf</a></span>   <span><a href='https://github.com/NJU-RL/T2DA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Zhang, Zican Hu, Wenhao Wu, Xinyi Xie, Jianxiang Tang, Chunlin Chen, Daoyi Dong, Yu Cheng, Zhenhong Sun, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15046">Text-to-Decision Agent: Offline Meta-Reinforcement Learning from Natural Language Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline meta-RL usually tackles generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose \textbf{T}ext-to-\textbf{D}ecision \textbf{A}gent (\textbf{T2DA}), a simple and scalable framework that supervises offline meta-RL with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines. Our code is available at https://github.com/NJU-RL/T2DA.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2504.13936.pdf' target='_blank'>https://arxiv.org/pdf/2504.13936.pdf</a></span>   <span><a href='https://ai-agents-2030.github.io/ViMo/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dezhao Luo, Bohan Tang, Kang Li, Georgios Papoudakis, Jifei Song, Shaogang Gong, Jianye Hao, Jun Wang, Kun Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13936">ViMo: A Generative Visual GUI World Model for App Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>App agents, which autonomously operate mobile Apps through Graphical User Interfaces (GUIs), have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first visual world model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation~(STR) to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs' graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of different action options. Experiments show ViMo's ability to generate visually plausible and functionally effective GUIs that enable App agents to make more informed decisions.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2504.13820.pdf' target='_blank'>https://arxiv.org/pdf/2504.13820.pdf</a></span>   <span><a href='https://github.com/LeapLabTHU/CheXWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yue, Yulin Wang, Chenxin Tao, Pan Liu, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13820">CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2504.13065.pdf' target='_blank'>https://arxiv.org/pdf/2504.13065.pdf</a></span>   <span><a href='https://github.com/LeapLabTHU/EchoWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yue, Yulin Wang, Haojun Jiang, Pan Liu, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13065">EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Echocardiography is crucial for cardiovascular disease detection but relies heavily on experienced sonographers. Echocardiography probe guidance systems, which provide real-time movement instructions for acquiring standard plane images, offer a promising solution for AI-assisted or fully autonomous scanning. However, developing effective machine learning models for this task remains challenging, as they must grasp heart anatomy and the intricate interplay between probe motion and visual signals. To address this, we present EchoWorld, a motion-aware world modeling framework for probe guidance that encodes anatomical knowledge and motion-induced visual dynamics, while effectively leveraging past visual-motion sequences to enhance guidance precision. EchoWorld employs a pre-training strategy inspired by world modeling principles, where the model predicts masked anatomical regions and simulates the visual outcomes of probe adjustments. Built upon this pre-trained model, we introduce a motion-aware attention mechanism in the fine-tuning stage that effectively integrates historical visual-motion data, enabling precise and adaptive probe guidance. Trained on more than one million ultrasound images from over 200 routine scans, EchoWorld effectively captures key echocardiographic knowledge, as validated by qualitative analysis. Moreover, our method significantly reduces guidance errors compared to existing visual backbones and guidance frameworks, excelling in both single-frame and sequential evaluation protocols. Code is available at https://github.com/LeapLabTHU/EchoWorld.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2504.02792.pdf' target='_blank'>https://arxiv.org/pdf/2504.02792.pdf</a></span>   <span><a href='https://weirdlabuw.github.io/uwm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, Abhishek Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02792">Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning has emerged as a promising approach towards building generalist robots. However, scaling imitation learning for large robot foundation models remains challenging due to its reliance on high-quality expert demonstrations. Meanwhile, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data directly for imitation learning, however, has proven difficult due to the lack of action annotation. In this work, we present Unified World Models (UWM), a framework that allows for leveraging both video and action data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. By controlling each diffusion timestep, UWM can flexibly represent a policy, a forward dynamics, an inverse dynamics, and a video generator. Through simulated and real-world experiments, we show that: (1) UWM enables effective pretraining on large-scale multitask robot datasets with both dynamics and action predictions, resulting in more generalizable and robust policies than imitation learning, (2) UWM naturally facilitates learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWM offers a promising step toward harnessing large, heterogeneous datasets for scalable robot learning, and provides a simple unification between the often disparate paradigms of imitation learning and world modeling. Videos and code are available at https://weirdlabuw.github.io/uwm/.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2504.02515.pdf' target='_blank'>https://arxiv.org/pdf/2504.02515.pdf</a></span>   <span><a href='https://github.com/insait-institute/GenieRedux' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nedko Savov, Naser Kazemi, Mohammad Mahdi, Danda Pani Paudel, Xi Wang, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02515">Exploration-Driven Generative Interactive Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern world models require costly and time-consuming collection of large video datasets with action demonstrations by people or by environment-specific agents. To simplify training, we focus on using many virtual environments for inexpensive, automatically collected interaction data. Genie, a recent multi-environment world model, demonstrates simulation abilities of many environments with shared behavior. Unfortunately, training their model requires expensive demonstrations. Therefore, we propose a training framework merely using a random agent in virtual environments. While the model trained in this manner exhibits good controls, it is limited by the random exploration possibilities. To address this limitation, we propose AutoExplore Agent - an exploration agent that entirely relies on the uncertainty of the world model, delivering diverse data from which it can learn the best. Our agent is fully independent of environment-specific rewards and thus adapts easily to new environments. With this approach, the pretrained multi-environment model can quickly adapt to new environments achieving video fidelity and controllability improvement. In order to obtain automatically large-scale interaction datasets for pretraining, we group environments with similar behavior and controls. To this end, we annotate the behavior and controls of 974 virtual environments - a dataset that we name RetroAct. For building our model, we first create an open implementation of Genie - GenieRedux and apply enhancements and adaptations in our version GenieRedux-G. Our code and data are available at https://github.com/insait-institute/GenieRedux.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2504.01941.pdf' target='_blank'>https://arxiv.org/pdf/2504.01941.pdf</a></span>   <span><a href='https://github.com/liyingyanUCAS/WoTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01941">End-to-End Driving with Online Trajectory Evaluation via BEV World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2503.21755.pdf' target='_blank'>https://arxiv.org/pdf/2503.21755.pdf</a></span>   <span><a href='https://vchitect.github.io/VBench-2.0-project/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Vchitect/VBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21755">VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real "world models" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored to individual dimensions, our evaluation framework integrates generalists such as SOTA VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive human annotations to ensure evaluation alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2503.19713.pdf' target='_blank'>https://arxiv.org/pdf/2503.19713.pdf</a></span>   <span><a href='https://github.com/xieyuser/Semi-SMD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusen Xie, Zhengmin Huang, Shaojie Shen, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19713">Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Semi-SMD, a novel metric depth estimation framework tailored for surrounding cameras equipment in autonomous driving. In this work, the input data consists of adjacent surrounding frames and camera parameters. We propose a unified spatial-temporal-semantic fusion module to construct the visual fused features. Cross-attention components for surrounding cameras and adjacent frames are utilized to focus on metric scale information refinement and temporal feature matching. Building on this, we propose a pose estimation framework using surrounding cameras, their corresponding estimated depths, and extrinsic parameters, which effectively address the scale ambiguity in multi-camera setups. Moreover, semantic world model and monocular depth estimation world model are integrated to supervised the depth estimation, which improve the quality of depth estimation. We evaluate our algorithm on DDAD and nuScenes datasets, and the results demonstrate that our method achieves state-of-the-art performance in terms of surrounding camera based depth estimation quality. The source code will be available on https://github.com/xieyuser/Semi-SMD.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2503.19713.pdf' target='_blank'>https://arxiv.org/pdf/2503.19713.pdf</a></span>   <span><a href='https://github.com/xieyuser/Semi-SMD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusen Xie, Zhengmin Huang, Shaojie Shen, Jun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19713">Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Semi-SMD, a novel metric depth estimation framework tailored for surrounding cameras equipment in autonomous driving. In this work, the input data consists of adjacent surrounding frames and camera parameters. We propose a unified spatial-temporal-semantic fusion module to construct the visual fused features. Cross-attention components for surrounding cameras and adjacent frames are utilized to focus on metric scale information refinement and temporal feature matching. Building on this, we propose a pose estimation framework using surrounding cameras, their corresponding estimated depths, and extrinsic parameters, which effectively address the scale ambiguity in multi-camera setups. Moreover, semantic world model and monocular depth estimation world model are integrated to supervised the depth estimation, which improve the quality of depth estimation. We evaluate our algorithm on DDAD and nuScenes datasets, and the results demonstrate that our method achieves state-of-the-art performance in terms of surrounding camera based depth estimation quality. The source code will be available on https://github.com/xieyuser/Semi-SMD.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2503.18938.pdf' target='_blank'>https://arxiv.org/pdf/2503.18938.pdf</a></span>   <span><a href='https://github.com/Little-Podi/AdaWorld,' target='_blank'>  GitHub</a></span> <span><a href='https://adaptable-world-model.github.io/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18938">AdaWorld: Learning Adaptable World Models with Latent Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2503.18871.pdf' target='_blank'>https://arxiv.org/pdf/2503.18871.pdf</a></span>   <span><a href='https://github.com/wertyuilife2/bmpc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Wang, Hanwei Guo, Sizhe Wang, Long Qian, Xuguang Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18871">Bootstrapped Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model Predictive Control (MPC) has been demonstrated to be effective in continuous control tasks. When a world model and a value function are available, planning a sequence of actions ahead of time leads to a better policy. Existing methods typically obtain the value function and the corresponding policy in a model-free manner. However, we find that such an approach struggles with complex tasks, resulting in poor policy learning and inaccurate value estimation. To address this problem, we leverage the strengths of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control (BMPC), a novel algorithm that performs policy learning in a bootstrapped manner. BMPC learns a network policy by imitating an MPC expert, and in turn, uses this policy to guide the MPC process. Combined with model-based TD-learning, our policy learning yields better value estimation and further boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism, which enables computationally efficient imitation learning. Our method achieves superior performance over prior works on diverse continuous control tasks. In particular, on challenging high-dimensional locomotion tasks, BMPC significantly improves data efficiency while also enhancing asymptotic performance and training stability, with comparable training time and smaller network sizes. Code is available at https://github.com/wertyuilife2/bmpc.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2503.17109.pdf' target='_blank'>https://arxiv.org/pdf/2503.17109.pdf</a></span>   <span><a href='https://github.com/Pter61/predicir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Gaopeng Gou, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17109">Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent across domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to modify a reference image according to manipulation text to accurately retrieve a target image, especially when the reference image is missing essential target content. In this paper, we propose a novel prediction-based mapping network, named PrediCIR, to adaptively predict the missing target visual content in reference images in the latent space before mapping for accurate ZS-CIR. Specifically, a world view generation module first constructs a source view by omitting certain visual content of a target view, coupled with an action that includes the manipulation intent derived from existing image-caption pairs. Then, a target content prediction module trains a world model as a predictor to adaptively predict the missing visual information guided by user intention in manipulating text at the latent space. The two modules map an image with the predicted relevant information to a pseudo-word token without extra supervision. Our model shows strong generalization ability on six ZS-CIR tasks. It obtains consistent and significant performance boosts ranging from 1.73% to 4.45% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://github.com/Pter61/predicir.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2503.15875.pdf' target='_blank'>https://arxiv.org/pdf/2503.15875.pdf</a></span>   <span><a href='https://github.com/xiaomi-mlab/mila.github.io' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xiaomi-mlab/mila.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiguang Wang, Daqi Liu, Hongwei Xie, Haisong Liu, Enhui Ma, Kaicheng Yu, Limin Wang, Bing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15875">MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, data-driven techniques have greatly advanced autonomous driving systems, but the need for rare and diverse training data remains a challenge, requiring significant investment in equipment and labor. World models, which predict and generate future environmental states, offer a promising solution by synthesizing annotated video data for training. However, existing methods struggle to generate long, consistent videos without accumulating errors, especially in dynamic scenes. To address this, we propose MiLA, a novel framework for generating high-fidelity, long-duration videos up to one minute. MiLA utilizes a Coarse-to-Re(fine) approach to both stabilize video generation and correct distortion of dynamic objects. Additionally, we introduce a Temporal Progressive Denoising Scheduler and Joint Denoising and Correcting Flow modules to improve the quality of generated videos. Extensive experiments on the nuScenes dataset show that MiLA achieves state-of-the-art performance in video generation quality. For more information, visit the project website: https://github.com/xiaomi-mlab/mila.github.io.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2503.15629.pdf' target='_blank'>https://arxiv.org/pdf/2503.15629.pdf</a></span>   <span><a href='https://github.com/CAV-Research-Lab/SACLA.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luc McCutcheon, Bahman Gharesifard, Saber Fallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15629">Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Control Lyapunov functions are traditionally used to design a controller which ensures convergence to a desired state, yet deriving these functions for nonlinear systems remains a complex challenge. This paper presents a novel, sample-efficient method for neural approximation of nonlinear Lyapunov functions, leveraging self-supervised Reinforcement Learning (RL) to enhance training data generation, particularly for inaccurately represented regions of the state space. The proposed approach employs a data-driven World Model to train Lyapunov functions from off-policy trajectories. The method is validated on both standard and goal-conditioned robotic tasks, demonstrating faster convergence and higher approximation accuracy compared to the state-of-the-art neural Lyapunov approximation baseline. The code is available at: https://github.com/CAV-Research-Lab/SACLA.git
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2503.13952.pdf' target='_blank'>https://arxiv.org/pdf/2503.13952.pdf</a></span>   <span><a href='https://github.com/Li-Zn-H/SimWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqing Li, Ruiqi Song, Qingyu Xie, Ye Wu, Nanxin Zeng, Yunfeng Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13952">SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of autonomous driving technology, a lack of data has become a major obstacle to enhancing perception model accuracy. Researchers are now exploring controllable data generation using world models to diversify datasets. However, previous work has been limited to studying image generation quality on specific public datasets. There is still relatively little research on how to build data generation engines for real-world application scenes to achieve large-scale data generation for challenging scenes. In this paper, a simulator-conditioned scene generation engine based on world model is proposed. By constructing a simulation system consistent with real-world scenes, simulation data and labels, which serve as the conditions for data generation in the world model, for any scenes can be collected. It is a novel data generation pipeline by combining the powerful scene simulation capabilities of the simulation engine with the robust data generation capabilities of the world model. In addition, a benchmark with proportionally constructed virtual and real data, is provided for exploring the capabilities of world models in real-world scenes. Quantitative results show that these generated images significantly improve downstream perception models performance. Finally, we explored the generative performance of the world model in urban autonomous driving scenarios. All the data and code will be available at https://github.com/Li-Zn-H/SimWorld.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2503.13814.pdf' target='_blank'>https://arxiv.org/pdf/2503.13814.pdf</a></span>   <span><a href='https://github.com/Cimy-wang/FusDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinping Wang, Weiwei Song, Hao Chen, Jinchang Ren, Huimin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13814">FusDreamer: Label-efficient Remote Sensing World Model for Multimodal Data Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models significantly enhance hierarchical understanding, improving data integration and learning efficiency. To explore the potential of the world model in the remote sensing (RS) field, this paper proposes a label-efficient remote sensing world model for multimodal data fusion (FusDreamer). The FusDreamer uses the world model as a unified representation container to abstract common and high-level knowledge, promoting interactions across different types of data, \emph{i.e.}, hyperspectral (HSI), light detection and ranging (LiDAR), and text data. Initially, a new latent diffusion fusion and multimodal generation paradigm (LaMG) is utilized for its exceptional information integration and detail retention capabilities. Subsequently, an open-world knowledge-guided consistency projection (OK-CP) module incorporates prompt representations for visually described objects and aligns language-visual features through contrastive learning. In this way, the domain gap can be bridged by fine-tuning the pre-trained world models with limited samples. Finally, an end-to-end multitask combinatorial optimization (MuCO) strategy can capture slight feature bias and constrain the diffusion process in a collaboratively learnable direction. Experiments conducted on four typical datasets indicate the effectiveness and advantages of the proposed FusDreamer. The corresponding code will be released at https://github.com/Cimy-wang/FusDreamer.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2503.13587.pdf' target='_blank'>https://arxiv.org/pdf/2503.13587.pdf</a></span>   <span><a href='https://github.com/dk-liang/UniFuture' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/dk-liang/UniFuture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingkang Liang, Dingyuan Zhang, Xin Zhou, Sifan Tu, Tianrui Feng, Xiaofan Li, Yumeng Zhang, Mingyang Du, Xiao Tan, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13587">Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present UniFuture, a simple yet effective driving world model that seamlessly integrates future scene generation and perception within a single framework. Unlike existing models focusing solely on pixel-level future prediction or geometric reasoning, our approach jointly models future appearance (i.e., RGB image) and geometry (i.e., depth), ensuring coherent predictions. Specifically, during the training, we first introduce a Dual-Latent Sharing scheme, which transfers image and depth sequence in a shared latent space, allowing both modalities to benefit from shared feature learning. Additionally, we propose a Multi-scale Latent Interaction mechanism, which facilitates bidirectional refinement between image and depth features at multiple spatial scales, effectively enhancing geometry consistency and perceptual alignment. During testing, our UniFuture can easily predict high-consistency future image-depth pairs by only using the current image as input. Extensive experiments on the nuScenes dataset demonstrate that UniFuture outperforms specialized models on future generation and perception tasks, highlighting the advantages of a unified, structurally-aware world model. The project page is at https://github.com/dk-liang/UniFuture.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2503.12955.pdf' target='_blank'>https://arxiv.org/pdf/2503.12955.pdf</a></span>   <span><a href='https://github.com/ZJHTerry18/HumanInScene' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahe Zhao, Ruibing Hou, Zejie Tian, Hong Chang, Shiguang Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12955">HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new task to benchmark human-in-scene understanding for embodied agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors, reason about its surrounding environment, and answer human-related questions within the scene. To support this new task, we present HIS-Bench, a multimodal benchmark that systematically evaluates HIS understanding across a broad spectrum, from basic perception to commonsense reasoning and planning. Our evaluation of various vision-language models on HIS-Bench reveals significant limitations in their ability to handle HIS-QA tasks. To this end, we propose HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates 3D scene context and human motion dynamics into large language models while incorporating specialized mechanisms to capture human-scene interactions. Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on HIS-QA tasks. We hope this work inspires future research on human behavior analysis in 3D scenes, advancing embodied AI and world models. The codes and data: https://github.com/ZJHTerry18/HumanInScene.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2503.12531.pdf' target='_blank'>https://arxiv.org/pdf/2503.12531.pdf</a></span>   <span><a href='https://mkturkcan.github.io/suturingmodels/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Kerem Turkcan, Mattia Ballo, Filippo Filicori, Zoran Kostic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12531">Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce specialized diffusion-based generative models that capture the spatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions through supervised learning on annotated laparoscopic surgery footage. The proposed models form a foundation for data-driven world models capable of simulating the biomechanical interactions and procedural dynamics of surgical suturing with high temporal fidelity. Annotating a dataset of $\sim2K$ clips extracted from simulation videos, we categorize surgical actions into fine-grained sub-stitch classes including ideal and non-ideal executions of needle positioning, targeting, driving, and withdrawal. We fine-tune two state-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to generate high-fidelity surgical action sequences at $\ge$768x512 resolution and $\ge$49 frames. For training our models, we explore both Low-Rank Adaptation (LoRA) and full-model fine-tuning approaches. Our experimental results demonstrate that these world models can effectively capture the dynamics of suturing, potentially enabling improved training simulators, surgical skill assessment tools, and autonomous surgical systems. The models also display the capability to differentiate between ideal and non-ideal technique execution, providing a foundation for building surgical training and evaluation systems. We release our models for testing and as a foundation for future research. Project Page: https://mkturkcan.github.io/suturingmodels/
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2503.06580.pdf' target='_blank'>https://arxiv.org/pdf/2503.06580.pdf</a></span>   <span><a href='https://github.com/ADaM-BJTU/AutoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, Jitao Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06580">Agent models: Internalizing Chain-of-Action Generation into Reasoning models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position \emph{Large Agent Models (LAMs)} that internalize the generation of \emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2503.04641.pdf' target='_blank'>https://arxiv.org/pdf/2503.04641.pdf</a></span>   <span><a href='https://github.com/ALEEEHU/World-Simulator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04641">Simulating the Real World: A Unified Survey of Multimodal Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2503.02904.pdf' target='_blank'>https://arxiv.org/pdf/2503.02904.pdf</a></span>   <span><a href='https://github.com/bhattarailab/Surgical-Vision-World-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02904">Surgical Vision World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and interactive surgical simulation has the potential to facilitate crucial applications, such as medical professional training and autonomous surgical agent training. In the natural visual domain, world models have enabled action-controlled data generation, demonstrating the potential to train autonomous agents in interactive simulated environments when large-scale real data acquisition is infeasible. However, such works in the surgical domain have been limited to simplified computer simulations, and lack realism. Furthermore, existing literature in world models has predominantly dealt with action-labeled data, limiting their applicability to real-world surgical data, where obtaining action annotation is prohibitively expensive. Inspired by the recent success of Genie in leveraging unlabeled video game data to infer latent actions and enable action-controlled data generation, we propose the first surgical vision world model. The proposed model can generate action-controllable surgical data and the architecture design is verified with extensive experiments on the unlabeled SurgToolLoc-2022 dataset. Codes and implementation details are available at https://github.com/bhattarailab/Surgical-Vision-World-Model
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2503.02904.pdf' target='_blank'>https://arxiv.org/pdf/2503.02904.pdf</a></span>   <span><a href='https://github.com/bhattarailab/Surgical-Vision-World-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02904">Surgical Vision World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and interactive surgical simulation has the potential to facilitate crucial applications, such as medical professional training and autonomous surgical agent training. In the natural visual domain, world models have enabled action-controlled data generation, demonstrating the potential to train autonomous agents in interactive simulated environments when large-scale real data acquisition is infeasible. However, such works in the surgical domain have been limited to simplified computer simulations, and lack realism. Furthermore, existing literature in world models has predominantly dealt with action-labeled data, limiting their applicability to real-world surgical data, where obtaining action annotation is prohibitively expensive. Inspired by the recent success of Genie in leveraging unlabeled video game data to infer latent actions and enable action-controlled data generation, we propose the first surgical vision world model. The proposed model can generate action-controllable surgical data and the architecture design is verified with extensive experiments on the unlabeled SurgToolLoc-2022 dataset. Codes and implementation details are available at https://github.com/bhattarailab/Surgical-Vision-World-Model
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2503.02904.pdf' target='_blank'>https://arxiv.org/pdf/2503.02904.pdf</a></span>   <span><a href='https://github.com/bhattarailab/Surgical-Vision-World-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02904">Surgical Vision World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic and interactive surgical simulation has the potential to facilitate crucial applications, such as medical professional training and autonomous surgical agent training. In the natural visual domain, world models have enabled action-controlled data generation, demonstrating the potential to train autonomous agents in interactive simulated environments when large-scale real data acquisition is infeasible. However, such works in the surgical domain have been limited to simplified computer simulations, and lack realism. Furthermore, existing literature in world models has predominantly dealt with action-labeled data, limiting their applicability to real-world surgical data, where obtaining action annotation is prohibitively expensive. Inspired by the recent success of Genie in leveraging unlabeled video game data to infer latent actions and enable action-controlled data generation, we propose the first surgical vision world model. The proposed model can generate action-controllable surgical data and the architecture design is verified with extensive experiments on the unlabeled SurgToolLoc-2022 dataset. Codes and implementation details are available at https://github.com/bhattarailab/Surgical-Vision-World-Model
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2503.02247.pdf' target='_blank'>https://arxiv.org/pdf/2503.02247.pdf</a></span>   <span><a href='https://b0b8k1ng.github.io/WMNav/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02247">WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2503.01837.pdf' target='_blank'>https://arxiv.org/pdf/2503.01837.pdf</a></span>   <span><a href='https://adrialopezescoriza.github.io/demo3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>AdriÃ  LÃ³pez Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01837">Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable subgoals. In this work, we propose DEMO3, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult tasks compared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2503.01837.pdf' target='_blank'>https://arxiv.org/pdf/2503.01837.pdf</a></span>   <span><a href='https://adrialopezescoriza.github.io/demo3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrià López Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01837">Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable subgoals. In this work, we propose DEMO3, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult tasks compared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2503.01837.pdf' target='_blank'>https://arxiv.org/pdf/2503.01837.pdf</a></span>   <span><a href='https://adrialopezescoriza.github.io/demo3/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrià López Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01837">Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable subgoals. In this work, we propose DEMO3, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult tasks compared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2502.13092.pdf' target='_blank'>https://arxiv.org/pdf/2502.13092.pdf</a></span>   <span><a href='https://text-to-world.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13092">Text2World: Benchmarking Large Language Models for Symbolic World Model Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2502.11537.pdf' target='_blank'>https://arxiv.org/pdf/2502.11537.pdf</a></span>   <span><a href='https://github.com/leor-c/Simulus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lior Cohen, Kaixin Wang, Bingyi Kang, Uri Gadot, Shie Mannor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11537">Uncovering Untapped Potential in Sample-Efficient World Model Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World model (WM) agents enable sample-efficient reinforcement learning by learning policies entirely from simulated experience. However, existing token-based world models (TBWMs) are limited to visual inputs and discrete actions, restricting their adoption and applicability. Moreover, although both intrinsic motivation and prioritized WM replay have shown promise in improving WM performance and generalization, they remain underexplored in this setting, particularly in combination. We introduce Simulus, a highly modular TBWM agent that integrates (1) a modular multi-modality tokenization framework, (2) intrinsic motivation, (3) prioritized WM replay, and (4) regression-as-classification for reward and return prediction. Simulus achieves state-of-the-art sample efficiency for planning-free WMs across three diverse benchmarks. Ablation studies reveal the individual contribution of each component while highlighting their synergy. Our code and model weights are publicly available at https://github.com/leor-c/Simulus.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2502.10498.pdf' target='_blank'>https://arxiv.org/pdf/2502.10498.pdf</a></span>   <span><a href='https://github.com/LMD0311/Awesome-World-Model' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LMD0311/Awesome-World-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifan Tu, Xin Zhou, Dingkang Liang, Xingyu Jiang, Yumeng Zhang, Xiaofan Li, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10498">The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving World Model (DWM), which focuses on predicting scene evolution during the driving process, has emerged as a promising paradigm in pursuing autonomous driving. These methods enable autonomous driving systems to better perceive, understand, and interact with dynamic driving environments. In this survey, we provide a comprehensive overview of the latest progress in DWM. We categorize existing approaches based on the modalities of the predicted scenes and summarize their specific contributions to autonomous driving. In addition, high-impact datasets and various metrics tailored to different tasks within the scope of DWM research are reviewed. Finally, we discuss the potential limitations of current research and propose future directions. This survey provides valuable insights into the development and application of DWM, fostering its broader adoption in autonomous driving. The relevant papers are collected at https://github.com/LMD0311/Awesome-World-Model.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2502.04296.pdf' target='_blank'>https://arxiv.org/pdf/2502.04296.pdf</a></span>   <span><a href='https://liruiw.github.io/hma' target='_blank'>  GitHub</a></span> <span><a href='https://liruiw.github.io/hma/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lirui Wang, Kevin Zhao, Chaoqi Liu, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04296">Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2502.01828.pdf' target='_blank'>https://arxiv.org/pdf/2502.01828.pdf</a></span>   <span><a href='https://yilin-wu98.github.io/forewarn/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wu, Ran Tian, Gokul Swamy, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01828">From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM's burden of predicting action outcomes (foresight) from evaluation (forethought). For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. For forethought, we align the VLM with these predicted latent states to reason about the consequences of actions in its native representation--natural language--and effectively filter proposed plans. We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering. Videos can be found on the project website: https://yilin-wu98.github.io/forewarn/.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2502.01366.pdf' target='_blank'>https://arxiv.org/pdf/2502.01366.pdf</a></span>   <span><a href='https://github.com/thuml/TrajWorld' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/thuml/TrajWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofeng Yin, Jialong Wu, Siqiao Huang, Xingjian Su, Xu He, Jianye Hao, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01366">Trajectory World Models for Heterogeneous Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneity in sensors and actuators across environments poses a significant challenge to building large-scale pre-trained world models on top of this low-dimensional sensor information. In this work, we explore pre-training world models for heterogeneous environments by addressing key transfer barriers in both data diversity and model flexibility. We introduce UniTraj, a unified dataset comprising over one million trajectories from 80 environments, designed to scale data while preserving critical diversity. Additionally, we propose TrajWorld, a novel architecture capable of flexibly handling varying sensor and actuator information and capturing environment dynamics in-context. Pre-training TrajWorld on UniTraj yields substantial gains in transition prediction, achieves a new state-of-the-art for off-policy evaluation, and also delivers superior online performance of model predictive control. To the best of our knowledge, this work, for the first time, demonstrates the transfer benefits of world models across heterogeneous and complex control environments. Code and data are available at https://github.com/thuml/TrajWorld.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2501.16733.pdf' target='_blank'>https://arxiv.org/pdf/2501.16733.pdf</a></span>   <span><a href='https://github.com/gaoyinfeng/PIWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinfeng Gao, Qichao Zhang, Da-wei Ding, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16733">Dream to Drive with Predictive Individual World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is still a challenging topic to make reactive driving behaviors in complex urban environments as road users' intentions are unknown. Model-based reinforcement learning (MBRL) offers great potential to learn a reactive policy by constructing a world model that can provide informative states and imagination training. However, a critical limitation in relevant research lies in the scene-level reconstruction representation learning, which may overlook key interactive vehicles and hardly model the interactive features among vehicles and their long-term intentions. Therefore, this paper presents a novel MBRL method with a predictive individual world model (PIWM) for autonomous driving. PIWM describes the driving environment from an individual-level perspective and captures vehicles' interactive relations and their intentions via trajectory prediction task. Meanwhile, a behavior policy is learned jointly with PIWM. It is trained in PIWM's imagination and effectively navigates in the urban driving scenes leveraging intention-aware latent states. The proposed method is trained and evaluated on simulation environments built upon real-world challenging interactive scenarios. Compared with popular model-free and state-of-the-art model-based reinforcement learning methods, experimental results show that the proposed method achieves the best performance in terms of safety and efficiency.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2501.14729.pdf' target='_blank'>https://arxiv.org/pdf/2501.14729.pdf</a></span>   <span><a href='https://github.com/LMD0311/HERMES' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LMD0311/HERMES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14729">HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving World Models (DWMs) have become essential for autonomous driving by enabling future scene prediction. However, existing DWMs are limited to scene generation and fail to incorporate scene understanding, which involves interpreting and reasoning about the driving environment. In this paper, we present a unified Driving World Model named HERMES. We seamlessly integrate 3D scene understanding and future scene evolution (generation) through a unified framework in driving scenarios. Specifically, HERMES leverages a Bird's-Eye View (BEV) representation to consolidate multi-view spatial information while preserving geometric relationships and interactions. We also introduce world queries, which incorporate world knowledge into BEV features via causal attention in the Large Language Model, enabling contextual enrichment for understanding and generation tasks. We conduct comprehensive studies on nuScenes and OmniDrive-nuScenes datasets to validate the effectiveness of our method. HERMES achieves state-of-the-art performance, reducing generation error by 32.4% and improving understanding metrics such as CIDEr by 8.0%. The model and code will be publicly released at https://github.com/LMD0311/HERMES.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2501.11260.pdf' target='_blank'>https://arxiv.org/pdf/2501.11260.pdf</a></span>   <span><a href='https://github.com/FengZicai/WMAD-Benchmarks' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/FengZicai/AwesomeWMAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Feng, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11260">A Survey of World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2501.11260.pdf' target='_blank'>https://arxiv.org/pdf/2501.11260.pdf</a></span>   <span><a href='https://github.com/FengZicai/WMAD-Benchmarks' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/FengZicai/AwesomeWMAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Feng, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11260">A Survey of World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in autonomous driving have been propelled by advances in robust world modeling, fundamentally transforming how vehicles interpret dynamic scenes and execute safe decision-making. World models have emerged as a linchpin technology, offering high-fidelity representations of the driving environment that integrate multi-sensor data, semantic cues, and temporal dynamics. This paper systematically reviews recent advances in world models for autonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future Physical World, covering Image-, BEV-, OG-, and PC-based generation methods that enhance scene evolution modeling through diffusion models and 4D occupancy forecasting; (ii) Behavior Planning for Intelligent Agents, combining rule-driven and learning-based paradigms with cost map optimization and reinforcement learning for trajectory generation in complex traffic conditions; (ii) Interaction between Prediction and Planning, achieving multi-agent collaborative decision-making through latent space diffusion and memory-augmented architectures. The study further analyzes training paradigms, including self-supervised learning, multimodal pretraining, and generative data augmentation, while evaluating world models' performance in scene understanding and motion prediction tasks. Future research must address key challenges in self-supervised representation learning, multimodal fusion, and advanced simulation to advance the practical deployment of world models in complex urban environments. Overall, the comprehensive analysis provides a technical roadmap for harnessing the transformative potential of world models in advancing safe and reliable autonomous driving solutions.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2501.09038.pdf' target='_blank'>https://arxiv.org/pdf/2501.09038.pdf</a></span>   <span><a href='https://github.com/google-deepmind/physics-IQ-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09038">Do generative video models understand physical principles?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn "world models" that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at https://physics-iq.github.io; code at https://github.com/google-deepmind/physics-IQ-benchmark.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2501.07108.pdf' target='_blank'>https://arxiv.org/pdf/2501.07108.pdf</a></span>   <span><a href='https://github.com/ALT-JS/OthelloSAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Du, Kelly Hong, Alishba Imran, Erfan Jahanparast, Mehdi Khfifi, Kaichun Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07108">How GPT learns layer by layer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) excel at tasks like language processing, strategy games, and reasoning but struggle to build generalizable internal representations essential for adaptive decision-making in agents. For agents to effectively navigate complex environments, they must construct reliable world models. While LLMs perform well on specific benchmarks, they often fail to generalize, leading to brittle representations that limit their real-world effectiveness. Understanding how LLMs build internal world models is key to developing agents capable of consistent, adaptive behavior across tasks. We analyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a controlled testbed for studying representation learning. Despite being trained solely on next-token prediction with random valid moves, OthelloGPT shows meaningful layer-wise progression in understanding board state and gameplay. Early layers capture static attributes like board edges, while deeper layers reflect dynamic tile changes. To interpret these representations, we compare Sparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more robust, disentangled insights into compositional features, whereas linear probes mainly detect features useful for classification. We use SAEs to decode features related to tile color and tile stability, a previously unexamined feature that reflects complex gameplay concepts like board control and long-term planning. We study the progression of linear probe accuracy and tile color using both SAE's and linear probes to compare their effectiveness at capturing what the model is learning. Although we begin with a smaller language model, OthelloGPT, this study establishes a framework for understanding the internal representations learned by GPT models, transformers, and LLMs more broadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2501.04969.pdf' target='_blank'>https://arxiv.org/pdf/2501.04969.pdf</a></span>   <span><a href='https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhu, Zhenyuan Dong, Kristi Topollai, Anna Choromanska
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04969">AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As opposed to human drivers, current autonomous driving systems still require vast amounts of labeled data to train. Recently, world models have been proposed to simultaneously enhance autonomous driving capabilities by improving the way these systems understand complex real-world environments and reduce their data demands via self-supervised pre-training. In this paper, we present AD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding Predictive Architecture), a novel self-supervised pre-training framework for autonomous driving with LiDAR data that, as opposed to existing methods, is neither generative nor contrastive. Our method learns spatial world models with a joint embedding predictive architecture. Instead of explicitly generating masked unknown regions, our self-supervised world models predict Bird's Eye View (BEV) embeddings to represent the diverse nature of autonomous driving scenes. Our approach furthermore eliminates the need to manually create positive and negative pairs, as is the case in contrastive learning. AD-L-JEPA leads to simpler implementation and enhanced learned representations. We qualitatively and quantitatively demonstrate high-quality of embeddings learned with AD-L-JEPA. We furthermore evaluate the accuracy and label efficiency of AD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and associated transfer learning. Our experimental evaluation demonstrates that AD-L-JEPA is a plausible approach for self-supervised pre-training in autonomous driving applications and is the best available approach outperforming SOTA, including most recently proposed Occupancy-MAE [1] and ALSO [2]. The source code of AD-L-JEPA is available at https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2501.03575.pdf' target='_blank'>https://arxiv.org/pdf/2501.03575.pdf</a></span>   <span><a href='https://github.com/nvidia-cosmos/cosmos-predict1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03575">Cosmos World Foundation Model Platform for Physical AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2412.19505.pdf' target='_blank'>https://arxiv.org/pdf/2412.19505.pdf</a></span>   <span><a href='https://github.com/YvanYin/DrivingWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19505">DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at https://github.com/YvanYin/DrivingWorld.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2412.15109.pdf' target='_blank'>https://arxiv.org/pdf/2412.15109.pdf</a></span>   <span><a href='https://github.com/OpenRobotLab/Seer/' target='_blank'>  GitHub</a></span> <span><a href='https://nimolty.github.io/Seer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15109">Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to realworld scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the synergy between vision and action, Seer significantly outperforms previous methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 21% on CALVIN ABC-D, and 43% in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D benchmark, achieving an average length of 4.28, and exhibits superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances on real-world scenarios. Code and models are publicly available at https://github.com/OpenRobotLab/Seer/.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2412.14957.pdf' target='_blank'>https://arxiv.org/pdf/2412.14957.pdf</a></span>   <span><a href='https://dreamtomanipulate.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14957">Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world robotics applications. To overcome those challenges, we propose to rethink robot world models as learnable digital twins. We introduce DreMa, a new approach for constructing digital twins automatically using learned explicit representations of the real world and its dynamics, bridging the gap between traditional digital twins and world models. DreMa replicates the observed world and its structure by integrating Gaussian Splatting and physics simulators, allowing robots to imagine novel configurations of objects and to predict the future consequences of robot actions thanks to its compositionality. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page can be found in: https://dreamtomanipulate.github.io/.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2412.14171.pdf' target='_blank'>https://arxiv.org/pdf/2412.14171.pdf</a></span>   <span><a href='https://vision-x-nyu.github.io/thinking-in-space.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, Saining Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14171">Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also ``think in space'' from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2412.10373.pdf' target='_blank'>https://arxiv.org/pdf/2412.10373.pdf</a></span>   <span><a href='https://github.com/zuosc19/GaussianWorld' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zuosc19/GaussianWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10373">GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D occupancy prediction is important for autonomous driving due to its comprehensive perception of the surroundings. To incorporate sequential inputs, most existing methods fuse representations from previous frames to infer the current 3D occupancy. However, they fail to consider the continuity of driving scenarios and ignore the strong prior provided by the evolution of 3D scenes (e.g., only dynamic objects move). In this paper, we propose a world-model-based framework to exploit the scene evolution for perception. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors: 1) ego motion alignment of static scenes; 2) local movements of dynamic objects; and 3) completion of newly-observed scenes. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit these priors and infer the scene evolution in the 3D Gaussian space considering the current RGB observation. We evaluate the effectiveness of our framework on the widely used nuScenes dataset. Our GaussianWorld improves the performance of the single-frame counterpart by over 2% in mIoU without introducing additional computations. Code: https://github.com/zuosc19/GaussianWorld.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2412.09627.pdf' target='_blank'>https://arxiv.org/pdf/2412.09627.pdf</a></span>   <span><a href='https://github.com/wzzheng/Doe' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/Doe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09627">Doe-1: Closed-Loop Autonomous Driving with Large World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving has received increasing attention due to its potential to learn from large amounts of data. However, most existing methods are still open-loop and suffer from weak scalability, lack of high-order interactions, and inefficient decision-making. In this paper, we explore a closed-loop framework for autonomous driving and propose a large Driving wOrld modEl (Doe-1) for unified perception, prediction, and planning. We formulate autonomous driving as a next-token generation problem and use multi-modal tokens to accomplish different tasks. Specifically, we use free-form texts (i.e., scene descriptions) for perception and generate future predictions directly in the RGB space with image tokens. For planning, we employ a position-aware tokenizer to effectively encode action into discrete tokens. We train a multi-modal transformer to autoregressively generate perception, prediction, and planning tokens in an end-to-end and unified manner. Experiments on the widely used nuScenes dataset demonstrate the effectiveness of Doe-1 in various tasks including visual question-answering, action-conditioned video generation, and motion planning. Code: https://github.com/wzzheng/Doe.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2412.09600.pdf' target='_blank'>https://arxiv.org/pdf/2412.09600.pdf</a></span>   <span><a href='https://github.com/huang-yh/Owl' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/huang-yh/Owl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09600">Owl-1: Omni World Model for Consistent Long Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models (VGMs) have received extensive attention recently and serve as promising candidates for general-purpose large vision models. While they can only generate short videos each time, existing methods achieve long video generation by iteratively calling the VGMs, using the last-frame output as the condition for the next-round generation. However, the last frame only contains short-term fine-grained information about the scene, resulting in inconsistency in the long horizon. To address this, we propose an Omni World modeL (Owl-1) to produce long-term coherent and comprehensive conditions for consistent long video generation. As videos are observations of the underlying evolving world, we propose to model the long-term developments in a latent space and use VGMs to film them into videos. Specifically, we represent the world with a latent state variable which can be decoded into explicit video observations. These observations serve as a basis for anticipating temporal dynamics which in turn update the state variable. The interaction between evolving dynamics and persistent state enhances the diversity and consistency of the long videos. Extensive experiments show that Owl-1 achieves comparable performance with SOTA methods on VBench-I2V and VBench-Long, validating its ability to generate high-quality video observations. Code: https://github.com/huang-yh/Owl.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2412.08410.pdf' target='_blank'>https://arxiv.org/pdf/2412.08410.pdf</a></span>   <span><a href='https://metadrivescape.github.io/papers_project/DrivePhysica/page.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08410">Physical Informed Driving World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving requires robust perception models trained on high-quality, large-scale multi-view driving videos for tasks like 3D object detection, segmentation and trajectory prediction. While world models provide a cost-effective solution for generating realistic driving videos, challenges remain in ensuring these videos adhere to fundamental physical principles, such as relative and absolute motion, spatial relationship like occlusion and spatial consistency, and temporal consistency. To address these, we propose DrivePhysica, an innovative model designed to generate realistic multi-view driving videos that accurately adhere to essential physical principles through three key advancements: (1) a Coordinate System Aligner module that integrates relative and absolute motion features to enhance motion interpretation, (2) an Instance Flow Guidance module that ensures precise temporal consistency via efficient 3D flow extraction, and (3) a Box Coordinate Guidance module that improves spatial relationship understanding and accurately resolves occlusion hierarchies. Grounded in physical principles, we achieve state-of-the-art performance in driving video generation quality (3.96 FID and 38.06 FVD on the Nuscenes dataset) and downstream perception tasks. Our project homepage: https://metadrivescape.github.io/papers_project/DrivePhysica/page.html
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2412.08261.pdf' target='_blank'>https://arxiv.org/pdf/2412.08261.pdf</a></span>   <span><a href='https://nus-lins-lab.github.io/flipweb/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, Lin Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08261">FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim to develop a model-based planning framework for world models that can be scaled with increasing model and data budgets for general-purpose manipulation tasks with only language and vision inputs. To this end, we present FLow-centric generative Planning (FLIP), a model-based planning algorithm on visual space that features three key modules: 1. a multi-modal flow generation model as the general-purpose action proposal module; 2. a flow-conditioned video generation model as the dynamics module; and 3. a vision-language representation learning model as the value module. Given an initial image and language instruction as the goal, FLIP can progressively search for long-horizon flow and video plans that maximize the discounted return to accomplish the task. FLIP is able to synthesize long-horizon plans across objects, robots, and tasks with image flows as the general action representation, and the dense flow information also provides rich guidance for long-horizon video generation. In addition, the synthesized flow and video plans can guide the training of low-level control policies for robot execution. Experiments on diverse benchmarks demonstrate that FLIP can improve both the success rates and quality of long-horizon video plan synthesis and has the interactive world model property, opening up wider applications for future works.Video demos are on our website: https://nus-lins-lab.github.io/flipweb/.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2412.07375.pdf' target='_blank'>https://arxiv.org/pdf/2412.07375.pdf</a></span>   <span><a href='https://github.com/Aria-Zhangjl/StoryWeaver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinlu Zhang, Jiji Tang, Rongsheng Zhang, Tangjie Lv, Xiaoshuai Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07375">StoryWeaver: A Unified World Model for Knowledge-Enhanced Story Character Customization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Story visualization has gained increasing attention in artificial intelligence. However, existing methods still struggle with maintaining a balance between character identity preservation and text-semantics alignment, largely due to a lack of detailed semantic modeling of the story scene. To tackle this challenge, we propose a novel knowledge graph, namely Character Graph (\textbf{CG}), which comprehensively represents various story-related knowledge, including the characters, the attributes related to characters, and the relationship between characters. We then introduce StoryWeaver, an image generator that achieve Customization via Character Graph (\textbf{C-CG}), capable of consistent story visualization with rich text semantics. To further improve the multi-character generation performance, we incorporate knowledge-enhanced spatial guidance (\textbf{KE-SG}) into StoryWeaver to precisely inject character semantics into generation. To validate the effectiveness of our proposed method, extensive experiments are conducted using a new benchmark called TBC-Bench. The experiments confirm that our StoryWeaver excels not only in creating vivid visual story plots but also in accurately conveying character identities across various scenarios with considerable storage efficiency, \emph{e.g.}, achieving an average increase of +9.03\% DINO-I and +13.44\% CLIP-T. Furthermore, ablation experiments are conducted to verify the superiority of the proposed module. Codes and datasets are released at https://github.com/Aria-Zhangjl/StoryWeaver.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2412.06162.pdf' target='_blank'>https://arxiv.org/pdf/2412.06162.pdf</a></span>   <span><a href='https://github.com/portal-cornell/llms-for-planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gonzalo Gonzalez-Pumariega, Wayne Chen, Kushal Kedia, Sanjiban Choudhury
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06162">Query-Efficient Planning with Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning in complex environments requires an agent to efficiently query a world model to find a feasible sequence of actions from start to goal. Recent work has shown that Large Language Models (LLMs), with their rich prior knowledge and reasoning capabilities, can potentially help with planning by searching over promising states and adapting to feedback from the world. In this paper, we propose and study two fundamentally competing frameworks that leverage LLMs for query-efficient planning. The first uses LLMs as a heuristic within a search-based planner to select promising nodes to expand and propose promising actions. The second uses LLMs as a generative planner to propose an entire sequence of actions from start to goal, query a world model, and adapt based on feedback. We show that while both approaches improve upon comparable baselines, using an LLM as a generative planner results in significantly fewer interactions. Our key finding is that the LLM as a planner can more rapidly adapt its planning strategies based on immediate feedback than LLM as a heuristic. We present evaluations and ablations on Robotouille and PDDL planning benchmarks and discuss connections to existing theory on query-efficient planning algorithms. Code is available at https://github.com/portal-cornell/llms-for-planning
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2412.05675.pdf' target='_blank'>https://arxiv.org/pdf/2412.05675.pdf</a></span>   <span><a href='https://github.com/wkh923/m3pc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehan Wen, Yutong Hu, Yao Mu, Lei Ke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05675">M$^3$PC: Test-time Model Predictive Control for Pretrained Masked Trajectory Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work in Offline Reinforcement Learning (RL) has shown that a unified Transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (e.g., states, actions, rewards) within given trajectory datasets. However, this information has not been fully exploited during the inference phase, where the agent needs to generate an optimal policy instead of just reconstructing masked components from unmasked ones. Given that a pretrained trajectory model can act as both a Policy Model and a World Model with appropriate mask patterns, we propose using Model Predictive Control (MPC) at test time to leverage the model's own predictive capability to guide its action selection. Empirical results on D4RL and RoboMimic show that our inference-phase MPC significantly improves the decision-making performance of a pretrained trajectory model without any additional parameter training. Furthermore, our framework can be adapted to Offline to Online (O2O) RL and Goal Reaching RL, resulting in more substantial performance gains when an additional online interaction budget is provided, and better generalization capabilities when different task targets are specified. Code is available: https://github.com/wkh923/m3pc.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2412.01522.pdf' target='_blank'>https://arxiv.org/pdf/2412.01522.pdf</a></span>   <span><a href='https://metadrivescape.github.io/papers_project/InfinityDrive/page.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Guo, Chenjing Ding, Haoxuan Dou, Xin Zhang, Weixuan Tang, Wei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01522">InfinityDrive: Breaking Time Limits in Driving World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving systems struggle with complex scenarios due to limited access to diverse, extensive, and out-of-distribution driving data which are critical for safe navigation. World models offer a promising solution to this challenge; however, current driving world models are constrained by short time windows and limited scenario diversity. To bridge this gap, we introduce InfinityDrive, the first driving world model with exceptional generalization capabilities, delivering state-of-the-art performance in high fidelity, consistency, and diversity with minute-scale video generation. InfinityDrive introduces an efficient spatio-temporal co-modeling module paired with an extended temporal training strategy, enabling high-resolution (576$\times$1024) video generation with consistent spatial and temporal coherence. By incorporating memory injection and retention mechanisms alongside an adaptive memory curve loss to minimize cumulative errors, achieving consistent video generation lasting over 1500 frames (more than 2 minutes). Comprehensive experiments in multiple datasets validate InfinityDrive's ability to generate complex and varied scenarios, highlighting its potential as a next-generation driving world model built for the evolving demands of autonomous driving. Our project homepage: https://metadrivescape.github.io/papers_project/InfinityDrive/page.html
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2412.00259.pdf' target='_blank'>https://arxiv.org/pdf/2412.00259.pdf</a></span>   <span><a href='https://tianyi20.github.io/rigid-world-model.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhu, Tianyi Xiang, Aaron Dollar, Zherong Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00259">One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2412.00154.pdf' target='_blank'>https://arxiv.org/pdf/2412.00154.pdf</a></span>   <span><a href='https://github.com/ADaM-BJTU/O1-CODER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00154">o1-Coder: an o1 Replication for Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode and then generate the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for world model construction. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models are disclosed at https://github.com/ADaM-BJTU/O1-CODER .
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2411.17027.pdf' target='_blank'>https://arxiv.org/pdf/2411.17027.pdf</a></span>   <span><a href='https://github.com/zhanghm1995/D2-World' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Zhang, Xu Yan, Ying Xue, Zixuan Guo, Shuguang Cui, Zhen Li, Bingbing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17027">D$^2$-World: An Efficient World Model through Decoupled Dynamic Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This technical report summarizes the second-place solution for the Predictive World Model Challenge held at the CVPR-2024 Workshop on Foundation Models for Autonomous Systems. We introduce D$^2$-World, a novel World model that effectively forecasts future point clouds through Decoupled Dynamic flow. Specifically, the past semantic occupancies are obtained via existing occupancy networks (e.g., BEVDet). Following this, the occupancy results serve as the input for a single-stage world model, generating future occupancy in a non-autoregressive manner. To further simplify the task, dynamic voxel decoupling is performed in the world model. The model generates future dynamic voxels by warping the existing observations through voxel flow, while remaining static voxels can be easily obtained through pose transformation. As a result, our approach achieves state-of-the-art performance on the OpenScene Predictive World Model benchmark, securing second place, and trains more than 300% faster than the baseline model. Code is available at https://github.com/zhanghm1995/D2-World.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2411.14499.pdf' target='_blank'>https://arxiv.org/pdf/2411.14499.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/World-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, Fengli Xu, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14499">Understanding World or Predicting Future? A Comprehensive Survey of World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2411.13550.pdf' target='_blank'>https://arxiv.org/pdf/2411.13550.pdf</a></span>   <span><a href='https://ziqi-ma.github.io/find3dsite/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Ma, Yisong Yue, Georgia Gkioxari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13550">Find Any Part in 3D</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Why don't we have foundation models in 3D yet? A key limitation is data scarcity. For 3D object part segmentation, existing datasets are small in size and lack diversity. We show that it is possible to break this data barrier by building a data engine powered by 2D foundation models. Our data engine automatically annotates any number of object parts: 1755x more unique part types than existing datasets combined. By training on our annotated data with a simple contrastive objective, we obtain an open-world model that generalizes to any part in any object based on any text query. Even when evaluated zero-shot, we outperform existing methods on the datasets they train on. We achieve 260% improvement in mIoU and boost speed by 6x to 300x. Our scaling analysis confirms that this generalization stems from the data scale, which underscores the impact of our data engine. Finally, to advance general-category open-world 3D part segmentation, we release a benchmark covering a wide range of objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2410.22689.pdf' target='_blank'>https://arxiv.org/pdf/2410.22689.pdf</a></span>   <span><a href='https://ut-austin-rpl.github.io/sirius-fleet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huihan Liu, Yu Zhang, Vaarij Betala, Evan Zhang, James Liu, Crystal Ding, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22689">Multi-Task Interactive Robot Fleet Learning with Visual World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a multi-task interactive robot fleet learning framework to address these challenges. Sirius-Fleet monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate Sirius-Fleet's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website: https://ut-austin-rpl.github.io/sirius-fleet
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2410.19923.pdf' target='_blank'>https://arxiv.org/pdf/2410.19923.pdf</a></span>   <span><a href='https://j0hngou.github.io/LLMCWM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>John Gkountouras, Matthias Lindemann, Phillip Lippe, Efstratios Gavves, Ivan Titov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19923">Language Agents Meet Causality -- Bridging LLMs and Causal World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2410.14081.pdf' target='_blank'>https://arxiv.org/pdf/2410.14081.pdf</a></span>   <span><a href='https://github.com/TobyLeelsz/iqmpc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangzhe Li, Zhiao Huang, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14081">Reward-free World Models for Online Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2410.11448.pdf' target='_blank'>https://arxiv.org/pdf/2410.11448.pdf</a></span>   <span><a href='https://github.com/NJU-RL/Meta-DT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Wang, Li Zhang, Wenhao Wu, Yuanheng Zhu, Dongbin Zhao, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11448">Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2410.11234.pdf' target='_blank'>https://arxiv.org/pdf/2410.11234.pdf</a></span>   <span><a href='https://github.com/LucasCJYSDL/Offline-RL-Kit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Wentse Chen, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11234">Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline RL is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based RL (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our ``RL + Search" framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three target tracking tasks in a challenging, stochastic tokamak control simulator. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2410.10429.pdf' target='_blank'>https://arxiv.org/pdf/2410.10429.pdf</a></span>   <span><a href='https://gusongen.github.io/DOME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songen Gu, Wei Yin, Bu Jin, Xiaoyang Guo, Junming Wang, Haodong Li, Qian Zhang, Xiaoxiao Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10429">DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DOME, a diffusion-based world model that predicts future occupancy frames based on past occupancy observations. The ability of this world model to capture the evolution of the environment is crucial for planning in autonomous driving. Compared to 2D video-based world models, the occupancy world model utilizes a native 3D representation, which features easily obtainable annotations and is modality-agnostic. This flexibility has the potential to facilitate the development of more advanced world models. Existing occupancy world models either suffer from detail loss due to discrete tokenization or rely on simplistic diffusion architectures, leading to inefficiencies and difficulties in predicting future occupancy with controllability. Our DOME exhibits two key features:(1) High-Fidelity and Long-Duration Generation. We adopt a spatial-temporal diffusion transformer to predict future occupancy frames based on historical context. This architecture efficiently captures spatial-temporal information, enabling high-fidelity details and the ability to generate predictions over long durations. (2)Fine-grained Controllability. We address the challenge of controllability in predictions by introducing a trajectory resampling method, which significantly enhances the model's ability to generate controlled predictions. Extensive experiments on the widely used nuScenes dataset demonstrate that our method surpasses existing baselines in both qualitative and quantitative evaluations, establishing a new state-of-the-art performance on nuScenes. Specifically, our approach surpasses the baseline by 10.5% in mIoU and 21.2% in IoU for occupancy reconstruction and by 36.0% in mIoU and 24.6% in IoU for 4D occupancy forecasting.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2410.08893.pdf' target='_blank'>https://arxiv.org/pdf/2410.08893.pdf</a></span>   <span><a href='https://github.com/realwenlongwang/Drama.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08893">Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often requires complex and deep architectures, which are computationally expensive and challenging to train. Within the world model, sequence models play a critical role in accurate predictions, and various architectures have been explored, each with its own challenges. Currently, recurrent neural network (RNN)-based world models struggle with vanishing gradients and capturing long-term dependencies. Transformers, on the other hand, suffer from the quadratic memory and computational complexity of self-attention mechanisms, scaling as $O(n^2)$, where $n$ is the sequence length.
  To address these challenges, we propose a state space model (SSM)-based world model, Drama, specifically leveraging Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and enabling efficient training with longer sequences. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early training stages. Combining these techniques, Drama achieves a normalised score on the Atari100k benchmark that is competitive with other state-of-the-art (SOTA) model-based RL algorithms, using only a 7 million-parameter world model. Drama is accessible and trainable on off-the-shelf hardware, such as a standard laptop. Our code is available at https://github.com/realwenlongwang/Drama.git.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2410.08822.pdf' target='_blank'>https://arxiv.org/pdf/2410.08822.pdf</a></span>   <span><a href='https://slot-latent-dynamics.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Mosbach, Jan Niklas Ewertz, Angel Villar-Corrales, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08822">SOLD: Slot Object-Centric Latent Dynamics Models for Relational Manipulation Learning from Pixels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning (RL) holds the potential to improve sample efficiency over model-free methods by learning from imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment's state. In contrast, humans reason about objects and their interactions, predicting how actions will affect specific parts of their surroundings. Inspired by this, we propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a novel model-based RL algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3 and TD-MPC2 - state-of-the-art model-based RL algorithms - across a range of benchmark robotic environments that require relational reasoning and manipulation capabilities. Videos are available at https://slot-latent-dynamics.github.io/.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2410.08751.pdf' target='_blank'>https://arxiv.org/pdf/2410.08751.pdf</a></span>   <span><a href='https://github.com/martius-lab/zilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Rupf, Marco Bagatella, Nico GÃ¼rtler, Jonas Frey, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08751">Zero-Shot Offline Imitation Learning via Optimal Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot imitation learning algorithms hold the promise of reproducing unseen behavior from as little as a single demonstration at test time. Existing practical approaches view the expert demonstration as a sequence of goals, enabling imitation with a high-level goal selector, and a low-level goal-conditioned policy. However, this framework can suffer from myopic behavior: the agent's immediate actions towards achieving individual goals may undermine long-term objectives. We introduce a novel method that mitigates this issue by directly optimizing the occupancy matching objective that is intrinsic to imitation learning. We propose to lift a goal-conditioned value function to a distance between occupancies, which are in turn approximated via a learned world model. The resulting method can learn from offline, suboptimal data, and is capable of non-myopic, zero-shot imitation, as we demonstrate in complex, continuous benchmarks. The code is available at https://github.com/martius-lab/zilot.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2410.07484.pdf' target='_blank'>https://arxiv.org/pdf/2410.07484.pdf</a></span>   <span><a href='https://github.com/elated-sawyer/WALL-E' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07484">WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such "world alignment" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent "WALL-E" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2410.05227.pdf' target='_blank'>https://arxiv.org/pdf/2410.05227.pdf</a></span>   <span><a href='https://ailab-cvc.github.io/VideoGen-Eval/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ailing Zeng, Yuhang Yang, Weidong Chen, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05227">The Dawn of Video Generation: Preliminary Explorations with SORA-like Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality video generation, encompassing text-to-video (T2V), image-to-video (I2V), and video-to-video (V2V) generation, holds considerable significance in content creation to benefit anyone express their inherent creativity in new ways and world simulation to modeling and understanding the world. Models like SORA have advanced generating videos with higher resolution, more natural motion, better vision-language alignment, and increased controllability, particularly for long video sequences. These improvements have been driven by the evolution of model architectures, shifting from UNet to more scalable and parameter-rich DiT models, along with large-scale data expansion and refined training strategies. However, despite the emergence of DiT-based closed-source and open-source models, a comprehensive investigation into their capabilities and limitations remains lacking. Furthermore, the rapid development has made it challenging for recent benchmarks to fully cover SORA-like models and recognize their significant advancements. Additionally, evaluation metrics often fail to align with human preferences.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2410.03618.pdf' target='_blank'>https://arxiv.org/pdf/2410.03618.pdf</a></span>   <span><a href='https://qiwang067.github.io/ls-imagine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03618">Open-World Reinforcement Learning over Long Short-Term Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be "short-sighted", as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2410.02253.pdf' target='_blank'>https://arxiv.org/pdf/2410.02253.pdf</a></span>   <span><a href='https://github.com/SCP-CN-001/ramble' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueyuan Li, Mingyang Jiang, Songan Zhang, Wei Yuan, Chunxiang Wang, Ming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02253">From Imitation to Exploration: End-to-end Autonomous Driving based on World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, end-to-end autonomous driving architectures have gained increasing attention due to their advantage in avoiding error accumulation. Most existing end-to-end autonomous driving methods are based on Imitation Learning (IL), which can quickly derive driving strategies by mimicking expert behaviors. However, IL often struggles to handle scenarios outside the training dataset, especially in high-dynamic and interaction-intensive traffic environments. In contrast, Reinforcement Learning (RL)-based driving models can optimize driving decisions through interaction with the environment, improving adaptability and robustness.
  To leverage the strengths of both IL and RL, we propose RAMBLE, an end-to-end world model-based RL method for driving decision-making. RAMBLE extracts environmental context information from RGB images and LiDAR data through an asymmetrical variational autoencoder. A transformer-based architecture is then used to capture the dynamic transitions of traffic participants. Next, an actor-critic structure reinforcement learning algorithm is applied to derive driving strategies based on the latent features of the current state and dynamics. To accelerate policy convergence and ensure stable training, we introduce a training scheme that initializes the policy network using IL, and employs KL loss and soft update mechanisms to smoothly transition the model from IL to RL.
  RAMBLE achieves state-of-the-art performance in route completion rate on the CARLA Leaderboard 1.0 and completes all 38 scenarios on the CARLA Leaderboard 2.0, demonstrating its effectiveness in handling complex and dynamic traffic scenarios. The model will be open-sourced upon paper acceptance at https://github.com/SCP-CN-001/ramble to support further research and development in autonomous driving.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2410.01440.pdf' target='_blank'>https://arxiv.org/pdf/2410.01440.pdf</a></span>   <span><a href='https://github.com/Singularity0104/equilibrium-planner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghan Li, Zhicheng Sun, Yadong Mu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01440">Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2410.00564.pdf' target='_blank'>https://arxiv.org/pdf/2410.00564.pdf</a></span>   <span><a href='https://github.com/CJReinforce/JOWA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Cheng, Ruixi Qiao, Yingwei Ma, Binhua Li, Gang Xiong, Qinghai Miao, Yongbin Li, Yisheng Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00564">Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization. We will release codes and model weights at https://github.com/CJReinforce/JOWA
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2409.16784.pdf' target='_blank'>https://arxiv.org/pdf/2409.16784.pdf</a></span>   <span><a href='https://wmp-loco.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Lai, Jiahang Cao, Jiafeng Xu, Hongtao Wu, Yunfeng Lin, Tao Kong, Yong Yu, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16784">World Model-based Perception for Visual Legged Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Legged locomotion over various terrains is challenging and requires precise perception of the robot and its surroundings from both proprioception and vision. However, learning directly from high-dimensional visual input is often data-inefficient and intricate. To address this issue, traditional methods attempt to learn a teacher policy with access to privileged information first and then learn a student policy to imitate the teacher's behavior with visual input. Despite some progress, this imitation framework prevents the student policy from achieving optimal performance due to the information gap between inputs. Furthermore, the learning process is unnatural since animals intuitively learn to traverse different terrains based on their understanding of the world without privileged knowledge. Inspired by this natural ability, we propose a simple yet effective method, World Model-based Perception (WMP), which builds a world model of the environment and learns a policy based on the world model. We illustrate that though completely trained in simulation, the world model can make accurate predictions of real-world trajectories, thus providing informative signals for the policy controller. Extensive simulated and real-world experiments demonstrate that WMP outperforms state-of-the-art baselines in traversability and robustness. Videos and Code are available at: https://wmp-loco.github.io/.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2409.15730.pdf' target='_blank'>https://arxiv.org/pdf/2409.15730.pdf</a></span>   <span><a href='https://github.com/Sephirex-X/LatentDriver' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyu Xiao, Jiang-Jiang Liu, Sen Yang, Xiaofan Li, Xiaoqing Ye, Wankou Yang, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15730">Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autoregressive world model exhibits robust generalization capabilities in vectorized scene understanding but encounters difficulties in deriving actions due to insufficient uncertainty modeling and self-delusion. In this paper, we explore the feasibility of deriving decisions from an autoregressive world model by addressing these challenges through the formulation of multiple probabilistic hypotheses. We propose LatentDriver, a framework models the environment's next states and the ego vehicle's possible actions as a mixture distribution, from which a deterministic control signal is then derived. By incorporating mixture modeling, the stochastic nature of decisionmaking is captured. Additionally, the self-delusion problem is mitigated by providing intermediate actions sampled from a distribution to the world model. Experimental results on the recently released close-loop benchmark Waymax demonstrate that LatentDriver surpasses state-of-the-art reinforcement learning and imitation learning methods, achieving expert-level performance. The code and models will be made available at https://github.com/Sephirex-X/LatentDriver.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2409.14216.pdf' target='_blank'>https://arxiv.org/pdf/2409.14216.pdf</a></span>   <span><a href='https://github.com/NACLab/robust-active-inference' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Viet Dung Nguyen, Zhizhuo Yang, Christopher L. Buckley, Alexander Ororbia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14216">R-AIF: Solving Sparse-Reward Robotic Tasks from Pixels with Active Inference and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although research has produced promising results demonstrating the utility of active inference (AIF) in Markov decision processes (MDPs), there is relatively less work that builds AIF models in the context of environments and problems that take the form of partially observable Markov decision processes (POMDPs). In POMDP scenarios, the agent must infer the unobserved environmental state from raw sensory observations, e.g., pixels in an image. Additionally, less work exists in examining the most difficult form of POMDP-centered control: continuous action space POMDPs under sparse reward signals. In this work, we address issues facing the AIF modeling paradigm by introducing novel prior preference learning techniques and self-revision schedules to help the agent excel in sparse-reward, continuous action, goal-based robotic control POMDP environments. Empirically, we show that our agents offer improved performance over state-of-the-art models in terms of cumulative rewards, relative stability, and success rate. The code in support of this work can be found at https://github.com/NACLab/robust-active-inference.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2409.08750.pdf' target='_blank'>https://arxiv.org/pdf/2409.08750.pdf</a></span>   <span><a href='https://jiangtaoran.github.io/dexsim2real2web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoran Jiang, Yixuan Guan, Liqian Ma, Jing Xu, Jiaojiao Meng, Weihang Chen, Zecui Zeng, Lusong Li, Dan Wu, Rui Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08750">DexSim2Real$^{2}$: Building Explicit World Model for Precise Articulated Object Dexterous Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Articulated objects are ubiquitous in daily life. In this paper, we present DexSim2Real$^{2}$, a novel framework for goal-conditioned articulated object manipulation. The core of our framework is constructing an explicit world model of unseen articulated objects through active interactions, which enables sampling-based model predictive control to plan trajectories achieving different goals without requiring demonstrations or RL. It first predicts an interaction using an affordance network trained on self-supervised interaction data or videos of human manipulation. After executing the interactions on the real robot to move the object parts, we propose a novel modeling pipeline based on 3D AIGC to build a digital twin of the object in simulation from multiple frames of observations. For dexterous hands, we utilize eigengrasp to reduce the action dimension, enabling more efficient trajectory searching. Experiments validate the framework's effectiveness for precise manipulation using a suction gripper, a two-finger gripper and two dexterous hand. The generalizability of the explicit world model also enables advanced manipulation strategies like manipulating with tools.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2409.06445.pdf' target='_blank'>https://arxiv.org/pdf/2409.06445.pdf</a></span>   <span><a href='https://github.com/insait-institute/GenieRedux' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naser Kazemi, Nedko Savov, Danda Paudel, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06445">Learning Generative Interactive Environments By Trained Agent Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are increasingly pivotal in interpreting and simulating the rules and actions of complex environments. Genie, a recent model, excels at learning from visually diverse environments but relies on costly human-collected data. We observe that their alternative method of using random agents is too limited to explore the environment. We propose to improve the model by employing reinforcement learning based agents for data generation. This approach produces diverse datasets that enhance the model's ability to adapt and perform well across various scenarios and realistic actions within the environment. In this paper, we first release the model GenieRedux - an implementation based on Genie. Additionally, we introduce GenieRedux-G, a variant that uses the agent's readily available actions to factor out action prediction uncertainty during validation. Our evaluation, including a replication of the Coinrun case study, shows that GenieRedux-G achieves superior visual fidelity and controllability using the trained agent exploration. The proposed approach is reproducable, scalable and adaptable to new types of environments. Our codebase is available at https://github.com/insait-institute/GenieRedux .
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2408.09807.pdf' target='_blank'>https://arxiv.org/pdf/2408.09807.pdf</a></span>   <span><a href='https://yangzhao-666.github.io/morefree' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Edward S. Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09807">Reset-free Reinforcement Learning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is an appealing paradigm for training intelligent agents, enabling policy acquisition from the agent's own autonomously acquired experience. However, the training process of RL is far from automatic, requiring extensive human effort to reset the agent and environments. To tackle the challenging reset-free setting, we first demonstrate the superiority of model-based (MB) RL methods in such setting, showing that a straightforward adaptation of MBRL can outperform all the prior state-of-the-art methods while requiring less supervision. We then identify limitations inherent to this direct extension and propose a solution called model-based reset-free (MoReFree) agent, which further enhances the performance. MoReFree adapts two key mechanisms, exploration and policy learning, to handle reset-free tasks by prioritizing task-relevant states. It exhibits superior data-efficiency across various reset-free tasks without access to environmental reward or demonstrations while significantly outperforming privileged baselines that require supervision. Our findings suggest model-based methods hold significant promise for reducing human effort in RL. Website: https://yangzhao-666.github.io/morefree
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2407.20806.pdf' target='_blank'>https://arxiv.org/pdf/2407.20806.pdf</a></span>   <span><a href='https://github.com/confeitoHS/arcle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hosung Lee, Sejin Kim, Seungpil Lee, Sanha Hwang, Jihwan Lee, Byung-Jun Lee, Sundong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20806">ARCLE: The Abstraction and Reasoning Corpus Learning Environment for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces ARCLE, an environment designed to facilitate reinforcement learning research on the Abstraction and Reasoning Corpus (ARC). Addressing this inductive reasoning benchmark with reinforcement learning presents these challenges: a vast action space, a hard-to-reach goal, and a variety of tasks. We demonstrate that an agent with proximal policy optimization can learn individual tasks through ARCLE. The adoption of non-factorial policies and auxiliary losses led to performance enhancements, effectively mitigating issues associated with action spaces and goal attainment. Based on these insights, we propose several research directions and motivations for using ARCLE, including MAML, GFlowNets, and World Models.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2407.15843.pdf' target='_blank'>https://arxiv.org/pdf/2407.15843.pdf</a></span>   <span><a href='https://kuis-ai.github.io/CarFormer/' target='_blank'>  GitHub</a></span> <span><a href='https://kuis-ai.github.io/CarFormer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shadi Hamdan, Fatma GÃ¼ney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15843">CarFormer: Self-Driving with Learned Object-Centric Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The choice of representation plays a key role in self-driving. Bird's eye view (BEV) representations have shown remarkable performance in recent years. In this paper, we propose to learn object-centric representations in BEV to distill a complex scene into more actionable information for self-driving. We first learn to place objects into slots with a slot attention model on BEV sequences. Based on these object-centric representations, we then train a transformer to learn to drive as well as reason about the future of other vehicles. We found that object-centric slot representations outperform both scene-level and object-level approaches that use the exact attributes of objects. Slot representations naturally incorporate information about objects from their spatial and temporal context such as position, heading, and speed without explicitly providing it. Our model with slots achieves an increased completion rate of the provided routes and, consequently, a higher driving score, with a lower variance across multiple runs, affirming slots as a reliable alternative in object-centric approaches. Additionally, we validate our model's performance as a world model through forecasting experiments, demonstrating its capability to predict future slot representations accurately. The code and the pre-trained models can be found at https://kuis-ai.github.io/CarFormer/.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2407.13163.pdf' target='_blank'>https://arxiv.org/pdf/2407.13163.pdf</a></span>   <span><a href='https://github.com/ArronDZhang/ROLeR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13163">ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) is an effective tool for real-world recommender systems with its capacity to model the dynamic interest of users and its interactive nature. Most existing offline RL recommender systems focus on model-based RL through learning a world model from offline data and building the recommendation policy by interacting with this model. Although these methods have made progress in the recommendation performance, the effectiveness of model-based offline RL methods is often constrained by the accuracy of the estimation of the reward model and the model uncertainties, primarily due to the extreme discrepancy between offline logged data and real-world data in user interactions with online platforms. To fill this gap, a more accurate reward model and uncertainty estimation are needed for the model-based RL methods. In this paper, a novel model-based Reward Shaping in Offline Reinforcement Learning for Recommender Systems, ROLeR, is proposed for reward and uncertainty estimation in recommendation systems. Specifically, a non-parametric reward shaping method is designed to refine the reward model. In addition, a flexible and more representative uncertainty penalty is designed to fit the needs of recommendation systems. Extensive experiments conducted on four benchmark datasets showcase that ROLeR achieves state-of-the-art performance compared with existing baselines. The source code can be downloaded at https://github.com/ArronDZhang/ROLeR.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2407.11965.pdf' target='_blank'>https://arxiv.org/pdf/2407.11965.pdf</a></span>   <span><a href='https://github.com/Urban-World/UrbanWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shang, Yuming Lin, Yu Zheng, Hangyu Fan, Jingtao Ding, Jie Feng, Jiansheng Chen, Li Tian, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11965">UrbanWorld: An Urban World Model for 3D City Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cities, as the essential environment of human life, encompass diverse physical elements such as buildings, roads and vegetation, which continuously interact with dynamic entities like people and vehicles. Crafting realistic, interactive 3D urban environments is essential for nurturing AGI systems and constructing AI agents capable of perceiving, decision-making, and acting like humans in real-world environments. However, creating high-fidelity 3D urban environments usually entails extensive manual labor from designers, involving intricate detailing and representation of complex urban elements. Therefore, accomplishing this automatically remains a longstanding challenge. Toward this problem, we propose UrbanWorld, the first generative urban world model that can automatically create a customized, realistic and interactive 3D urban world with flexible control conditions. UrbanWorld incorporates four key stages in the generation pipeline: flexible 3D layout generation from OSM data or urban layout with semantic and height maps, urban scene design with Urban MLLM, controllable urban asset rendering via progressive 3D diffusion, and MLLM-assisted scene refinement. We conduct extensive quantitative analysis on five visual metrics, demonstrating that UrbanWorld achieves SOTA generation realism. Next, we provide qualitative results about the controllable generation capabilities of UrbanWorld using both textual and image-based prompts. Lastly, we verify the interactive nature of these environments by showcasing the agent perception and navigation within the created environments. We contribute UrbanWorld as an open-source tool available at https://github.com/Urban-World/UrbanWorld.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2407.09533.pdf' target='_blank'>https://arxiv.org/pdf/2407.09533.pdf</a></span>   <span><a href='https://github.com/manantomar/video-occupancy-models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manan Tomar, Philippe Hansen-Estruch, Philip Bachman, Alex Lamb, John Langford, Matthew E. Taylor, Sergey Levine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09533">Video Occupancy Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a new family of video prediction models designed to support downstream control tasks. We call these models Video Occupancy models (VOCs). VOCs operate in a compact latent space, thus avoiding the need to make predictions about individual pixels. Unlike prior latent-space world models, VOCs directly predict the discounted distribution of future states in a single step, thus avoiding the need for multistep roll-outs. We show that both properties are beneficial when building predictive models of video for use in downstream control. Code is available at \href{https://github.com/manantomar/video-occupancy-models}{\texttt{github.com/manantomar/video-occupancy-models}}.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2407.06886.pdf' target='_blank'>https://arxiv.org/pdf/2407.06886.pdf</a></span>   <span><a href='https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06886">Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2407.04363.pdf' target='_blank'>https://arxiv.org/pdf/2407.04363.pdf</a></span>   <span><a href='https://github.com/AIRI-Institute/AriGraph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, Evgeny Burnaev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04363">AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2406.19320.pdf' target='_blank'>https://arxiv.org/pdf/2406.19320.pdf</a></span>   <span><a href='https://github.com/vmicheli/delta-iris' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Micheli, Eloi Alonso, FranÃ§ois Fleuret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19320">Efficient World Models with Context-Aware Tokenization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling up deep Reinforcement Learning (RL) methods presents a significant challenge. Following developments in generative modelling, model-based RL positions itself as a strong contender. Recent advances in sequence modelling have led to effective transformer-based world models, albeit at the price of heavy computations due to the long sequences of tokens required to accurately simulate environments. In this work, we propose $Î$-IRIS, a new agent with a world model architecture composed of a discrete autoencoder that encodes stochastic deltas between time steps and an autoregressive transformer that predicts future deltas by summarizing the current state of the world with continuous tokens. In the Crafter benchmark, $Î$-IRIS sets a new state of the art at multiple frame budgets, while being an order of magnitude faster to train than previous attention-based approaches. We release our code and models at https://github.com/vmicheli/delta-iris.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2406.18043.pdf' target='_blank'>https://arxiv.org/pdf/2406.18043.pdf</a></span>   <span><a href='https://mazpie.github.io/genrl/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, Sai Rajeswar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18043">GenRL: Multimodal-foundation world models for generalization in embodied agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain's dynamics, and learn the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2406.13948.pdf' target='_blank'>https://arxiv.org/pdf/2406.13948.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/CityGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Feng, Tianhui Liu, Yuwei Du, Siqi Guo, Yuming Lin, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13948">CityGPT: Empowering Urban Spatial Cognition of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit{CityInstruction} by \textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit{CityEval}.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2406.11911.pdf' target='_blank'>https://arxiv.org/pdf/2406.11911.pdf</a></span>   <span><a href='https://flecart.github.io/complexity-tom-dwm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>X. Angelo Huang, Emanuele La Malfa, Samuele Marro, Andrea Asperti, Anthony Cohn, Michael Wooldridge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11911">A Notion of Complexity for Theory of Mind via Discrete World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Theory of Mind (ToM) can be used to assess the capabilities of Large Language Models (LLMs) in complex scenarios where social reasoning is required. While the research community has proposed many ToM benchmarks, their hardness varies greatly, and their complexity is not well defined. This work proposes a framework inspired by cognitive load theory to measure the complexity of ToM tasks. We quantify a problem's complexity as the number of states necessary to solve it correctly. Our complexity measure also accounts for spurious states of a ToM problem designed to make it apparently harder. We use our method to assess the complexity of five widely adopted ToM benchmarks. On top of this framework, we design a prompting technique that augments the information available to a model with a description of how the environment changes with the agents' interactions. We name this technique Discrete World Models (DWM) and show how it elicits superior performance on ToM tasks.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2406.10788.pdf' target='_blank'>https://arxiv.org/pdf/2406.10788.pdf</a></span>   <span><a href='https://embodied-gaussians.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jad Abou-Chakra, Krishan Rana, Feras Dayoub, Niko SÃ¼nderhauf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10788">Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For robots to robustly understand and interact with the physical world, it is highly beneficial to have a comprehensive representation - modelling geometry, physics, and visual observations - that informs perception, planning, and control algorithms. We propose a novel dual Gaussian-Particle representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates visual forces that correct the particle positions while respecting known physical constraints. By integrating predictive physical modelling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality. Our system runs in realtime at 30Hz using only 3 cameras. We validate our approach on 2D and 3D tracking tasks as well as photometric reconstruction quality. Videos are found at https://embodied-gaussians.github.io/.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2406.10714.pdf' target='_blank'>https://arxiv.org/pdf/2406.10714.pdf</a></span>   <span><a href='https://arunbalajeev.github.io/world_models_planning/world_model_paper.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arun Balajee Vasudevan, Neehar Peri, Jeff Schneider, Deva Ramanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10714">Planning with Adaptive World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion planning is crucial for safe navigation in complex urban environments. Historically, motion planners (MPs) have been evaluated with procedurally-generated simulators like CARLA. However, such synthetic benchmarks do not capture real-world multi-agent interactions. nuPlan, a recently released MP benchmark, addresses this limitation by augmenting real-world driving logs with closed-loop simulation logic, effectively turning the fixed dataset into a reactive simulator. We analyze the characteristics of nuPlan's recorded logs and find that each city has its own unique driving behaviors, suggesting that robust planners must adapt to different environments. We learn to model such unique behaviors with BehaviorNet, a graph convolutional neural network (GCNN) that predicts reactive agent behaviors using features derived from recently-observed agent histories; intuitively, some aggressive agents may tailgate lead vehicles, while others may not. To model such phenomena, BehaviorNet predicts the parameters of an agent's motion controller rather than directly predicting its spacetime trajectory (as most forecasters do). Finally, we present AdaptiveDriver, a model-predictive control (MPC) based planner that unrolls different world models conditioned on BehaviorNet's predictions. Our extensive experiments demonstrate that AdaptiveDriver achieves state-of-the-art results on the nuPlan closed-loop planning benchmark, improving over prior work by 2% on Test-14 Hard R-CLS, and generalizes even when evaluated on never-before-seen cities.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2406.10667.pdf' target='_blank'>https://arxiv.org/pdf/2406.10667.pdf</a></span>   <span><a href='https://github.com/opendilab/LightZero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Pu, Yazhe Niu, Zhenjie Yang, Jiyuan Ren, Hongsheng Li, Yu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10667">UniZero: Generalized and Efficient Planning with Scalable Latent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning predictive world models is crucial for enhancing the planning capabilities of reinforcement learning (RL) agents. Recently, MuZero-style algorithms, leveraging the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains. However, these methods struggle to scale in heterogeneous scenarios with diverse dependencies and task variability. To overcome these limitations, we introduce UniZero, a novel approach that employs a modular transformer-based world model to effectively learn a shared latent space. By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in the latent space. We show that UniZero significantly outperforms existing baselines in benchmarks that require long-term memory. Additionally, UniZero demonstrates superior scalability in multitask learning experiments conducted on Atari benchmarks. In standard single-task RL settings, such as Atari and DMControl, UniZero matches or even surpasses the performance of current state-of-the-art methods. Finally, extensive ablation studies and visual analyses validate the effectiveness and scalability of UniZero's design choices. Our code is available at \textcolor{magenta}{https://github.com/opendilab/LightZero}.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2406.08481.pdf' target='_blank'>https://arxiv.org/pdf/2406.08481.pdf</a></span>   <span><a href='https://github.com/BraveGroup/LAW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, Tieniu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08481">Enhancing End-to-End Autonomous Driving with Latent World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, end-to-end planners directly utilize raw sensor data, enabling them to extract richer scene features and reduce information loss compared to traditional planners. This raises a crucial research question: how can we develop better scene feature representations to fully leverage sensor data in end-to-end driving? Self-supervised learning methods show great success in learning rich feature representations in NLP and computer vision. Inspired by this, we propose a novel self-supervised learning approach using the LAtent World model (LAW) for end-to-end driving. LAW predicts future scene features based on current features and ego trajectories. This self-supervised task can be seamlessly integrated into perception-free and perception-based frameworks, improving scene feature learning and optimizing trajectory prediction. LAW achieves state-of-the-art performance across multiple benchmarks, including real-world open-loop benchmark nuScenes, NAVSIM, and simulator-based closed-loop benchmark CARLA. The code is released at https://github.com/BraveGroup/LAW.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2406.01361.pdf' target='_blank'>https://arxiv.org/pdf/2406.01361.pdf</a></span>   <span><a href='https://pranaval.github.io/DART/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Agarwal, Sheldon Andrews, Samira Ebrahimi Kahou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01361">Learning to Play Atari in a World of Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2405.20337.pdf' target='_blank'>https://arxiv.org/pdf/2405.20337.pdf</a></span>   <span><a href='https://github.com/wzzheng/OccSora' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/OccSora' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20337">OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the evolution of 3D scenes is important for effective autonomous driving. While conventional methods mode scene development with the motion of individual instances, world models emerge as a generative framework to describe the general scene dynamics. However, most existing methods adopt an autoregressive framework to perform next-token prediction, which suffer from inefficiency in modeling long-term temporal evolutions. To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving. We employ a 4D scene tokenizer to obtain compact discrete spatial-temporal representations for 4D occupancy input and achieve high-quality reconstruction for long-sequence occupancy videos. We then learn a diffusion transformer on the spatial-temporal representations and generate 4D occupancy conditioned on a trajectory prompt. We conduct extensive experiments on the widely used nuScenes dataset with Occ3D occupancy annotations. OccSora can generate 16s-videos with authentic 3D layout and temporal consistency, demonstrating its ability to understand the spatial and temporal distributions of driving scenes. With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving. Code is available at: https://github.com/wzzheng/OccSora.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2405.19334.pdf' target='_blank'>https://arxiv.org/pdf/2405.19334.pdf</a></span>   <span><a href='https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19334">LLMs Meet Multimodal Generation and Editing: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2405.17940.pdf' target='_blank'>https://arxiv.org/pdf/2405.17940.pdf</a></span>   <span><a href='https://linhongbin.github.io/gas/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbin Lin, Bin Li, Chun Wai Wong, Juan Rojas, Xiangyu Chu, Kwok Wai Samuel Au
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17940">World Models for General Surgical Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent vision control systems for surgical robots should adapt to unknown and diverse objects while being robust to system disturbances. Previous methods did not meet these requirements due to mainly relying on pose estimation and feature tracking. We propose a world-model-based deep reinforcement learning framework "Grasp Anything for Surgery" (GAS), that learns a pixel-level visuomotor policy for surgical grasping, enhancing both generality and robustness. In particular, a novel method is proposed to estimate the values and uncertainties of depth pixels for a rigid-link object's inaccurate region based on the empirical prior of the object's size; both depth and mask images of task objects are encoded to a single compact 3-channel image (size: 64x64x3) by dynamically zooming in the mask regions, minimizing the information loss. The learned controller's effectiveness is extensively evaluated in simulation and in a real robot. Our learned visuomotor policy handles: i) unseen objects, including 5 types of target grasping objects and a robot gripper, in unstructured real-world surgery environments, and ii) disturbances in perception and control. Note that we are the first work to achieve a unified surgical control system that grasps diverse surgical objects using different robot grippers on real robots in complex surgery scenes (average success rate: 69%). Our system also demonstrates significant robustness across 6 conditions including background variation, target disturbance, camera pose variation, kinematic control error, image noise, and re-grasping after the gripped target object drops from the gripper. Videos and codes can be found on our project page: https://linhongbin.github.io/gas/.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2405.17398.pdf' target='_blank'>https://arxiv.org/pdf/2405.17398.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/Vista,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17398">Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2405.15223.pdf' target='_blank'>https://arxiv.org/pdf/2405.15223.pdf</a></span>   <span><a href='https://thuml.github.io/iVideoGPT' target='_blank'>  GitHub</a></span> <span><a href='https://thuml.github.io/iVideoGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15223">iVideoGPT: Interactive VideoGPTs are Scalable World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications. Code and pre-trained models are available at https://thuml.github.io/iVideoGPT.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2405.14853.pdf' target='_blank'>https://arxiv.org/pdf/2405.14853.pdf</a></span>   <span><a href='https://penn-pal-lab.github.io/scaffolder/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward S. Hu, James Springer, Oleh Rybkin, Dinesh Jayaraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14853">Privileged Sensing Scaffolds Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon "sensory scaffolding": observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose "Scaffolder", a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new "S3" suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2405.13570.pdf' target='_blank'>https://arxiv.org/pdf/2405.13570.pdf</a></span>   <span><a href='https://jiupinjia.github.io/metaearth/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiping Yu, Chenyang Liu, Liqin Liu, Zhenwei Shi, Zhengxia Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13570">MetaEarth: A Generative Foundation Model for Global-Scale Remote Sensing Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancement of generative foundational models has ushered in a new era of image generation in the realm of natural images, revolutionizing art design, entertainment, environment simulation, and beyond. Despite producing high-quality samples, existing methods are constrained to generating images of scenes at a limited scale. In this paper, we present MetaEarth, a generative foundation model that breaks the barrier by scaling image generation to a global level, exploring the creation of worldwide, multi-resolution, unbounded, and virtually limitless remote sensing images. In MetaEarth, we propose a resolution-guided self-cascading generative framework, which enables the generating of images at any region with a wide range of geographical resolutions. To achieve unbounded and arbitrary-sized image generation, we design a novel noise sampling strategy for denoising diffusion models by analyzing the generation conditions and initial noise. To train MetaEarth, we construct a large dataset comprising multi-resolution optical remote sensing images with geographical information. Experiments have demonstrated the powerful capabilities of our method in generating global-scale images. Additionally, the MetaEarth serves as a data engine that can provide high-quality and rich training data for downstream tasks. Our model opens up new possibilities for constructing generative world models by simulating Earth visuals from an innovative overhead perspective.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2405.09111.pdf' target='_blank'>https://arxiv.org/pdf/2405.09111.pdf</a></span>   <span><a href='https://github.com/ucd-dare/CarDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dechen Gao, Shuangyu Cai, Hanchu Zhou, Hang Wang, Iman Soltani, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09111">CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To safely navigate intricate real-world scenarios, autonomous vehicles must be able to adapt to diverse road conditions and anticipate future events. World model (WM) based reinforcement learning (RL) has emerged as a promising approach by learning and predicting the complex dynamics of various environments. Nevertheless, to the best of our knowledge, there does not exist an accessible platform for training and testing such algorithms in sophisticated driving environments. To fill this void, we introduce CarDreamer, the first open-source learning platform designed specifically for developing WM based autonomous driving algorithms. It comprises three key components: 1) World model backbone: CarDreamer has integrated some state-of-the-art WMs, which simplifies the reproduction of RL algorithms. The backbone is decoupled from the rest and communicates using the standard Gym interface, so that users can easily integrate and test their own algorithms. 2) Built-in tasks: CarDreamer offers a comprehensive set of highly configurable driving tasks which are compatible with Gym interfaces and are equipped with empirically optimized reward functions. 3) Task development suite: This suite streamlines the creation of driving tasks, enabling easy definition of traffic flows and vehicle routes, along with automatic collection of multi-modal observation data. A visualization server allows users to trace real-time agent driving videos and performance metrics through a browser. Furthermore, we conduct extensive experiments using built-in tasks to evaluate the performance and potential of WMs in autonomous driving. Thanks to the richness and flexibility of CarDreamer, we also systematically study the impact of observation modality, observability, and sharing of vehicle intentions on AV safety and efficiency. All code and documents are accessible on https://github.com/ucd-dare/CarDreamer.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2405.06263.pdf' target='_blank'>https://arxiv.org/pdf/2405.06263.pdf</a></span>   <span><a href='https://github.com/bit1029public/HRSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Sun, Hongyu Zang, Xin Li, Riashat Islam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06263">Learning Latent Dynamic Robust Representations for World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Model-Based Reinforcement Learning (MBRL) promises to encapsulate agent's knowledge about the underlying dynamics of the environment, enabling learning a world model as a useful planner. However, top MBRL agents such as Dreamer often struggle with visual pixel-based inputs in the presence of exogenous or irrelevant noise in the observation space, due to failure to capture task-specific features while filtering out irrelevant spatio-temporal details. To tackle this problem, we apply a spatio-temporal masking strategy, a bisimulation principle, combined with latent reconstruction, to capture endogenous task-specific aspects of the environment for world models, effectively eliminating non-essential information. Joint training of representations, dynamics, and policy often leads to instabilities. To further address this issue, we develop a Hybrid Recurrent State-Space Model (HRSSM) structure, enhancing state representation robustness for effective policy learning. Our empirical evaluation demonstrates significant performance improvements over existing methods in a range of visually complex control tasks such as Maniskill \cite{gu2023maniskill2} with exogenous distractors from the Matterport environment. Our code is avaliable at https://github.com/bit1029public/HRSSM.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2405.05956.pdf' target='_blank'>https://arxiv.org/pdf/2405.05956.pdf</a></span>   <span><a href='https://github.com/sreeramsa/DriveSim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05956">Probing Multimodal LLMs as World Models for Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We provide a sober look at the application of Multimodal Large Language Models (MLLMs) in autonomous driving, challenging common assumptions about their ability to interpret dynamic driving scenarios. Despite advances in models like GPT-4o, their performance in complex driving environments remains largely unexplored. Our experimental study assesses various MLLMs as world models using in-car camera perspectives and reveals that while these models excel at interpreting individual images, they struggle to synthesize coherent narratives across frames, leading to considerable inaccuracies in understanding (i) ego vehicle dynamics, (ii) interactions with other road actors, (iii) trajectory planning, and (iv) open-set scene reasoning. We introduce the Eval-LLM-Drive dataset and DriveSim simulator to enhance our evaluation, highlighting gaps in current MLLM capabilities and the need for improved models in dynamic real-world environments.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2405.03520.pdf' target='_blank'>https://arxiv.org/pdf/2405.03520.pdf</a></span>   <span><a href='https://github.com/GigaAI-research/General-World-Models-Survey' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GigaAI-research/General-World-Models-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, Yang You, Zhaoxiang Zhang, Dawei Zhao, Liang Xiao, Jian Zhao, Jiwen Lu, Guan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03520">Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2404.18926.pdf' target='_blank'>https://arxiv.org/pdf/2404.18926.pdf</a></span>   <span><a href='https://pvskand.github.io/projects/PCWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Skand Peri, Iain Lee, Chanho Kim, Li Fuxin, Tucker Hermans, Stefan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18926">Point Cloud Models Improve Visual Robustness in Robotic Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Project Webpage: https://pvskand.github.io/projects/PCWM
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2404.18896.pdf' target='_blank'>https://arxiv.org/pdf/2404.18896.pdf</a></span>   <span><a href='https://github.com/IcarusWizard/AIME-NoB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18896">Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretraining and finetuning models has become increasingly popular in decision-making. But there are still serious impediments in Imitation Learning from Observation (ILfO) with pretrained models. This study identifies two primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB). The EKB emerges due to the pretrained models' limitations in handling novel observations, which leads to inaccurate action inference. Conversely, the DKB stems from the reliance on limited demonstration datasets, restricting the model's adaptability across diverse scenarios. We propose separate solutions to overcome each barrier and apply them to Action Inference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new algorithm, AIME-NoB, integrates online interactions and a data-driven regulariser to mitigate the EKB. Additionally, it uses a surrogate reward function to broaden the policy's supported states, addressing the DKB. Our experiments on vision-based control tasks from the DeepMind Control Suite and MetaWorld benchmarks show that AIME-NoB significantly improves sample efficiency and converged performance, presenting a robust framework for overcoming the challenges in ILfO with pretrained models. Code available at https://github.com/IcarusWizard/AIME-NoB.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2404.18202.pdf' target='_blank'>https://arxiv.org/pdf/2404.18202.pdf</a></span>   <span><a href='https://github.com/DCDmllm/WorldGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqi Ge, Hongzhe Huang, Mingze Zhou, Juncheng Li, Guoming Wang, Siliang Tang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18202">WorldGPT: Empowering LLM as Multimodal World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are progressively being employed across diverse fields, extending from basic environment simulation to complex scenario construction. However, existing models are mainly trained on domain-specific states and actions, and confined to single-modality state representations. In this paper, We introduce WorldGPT, a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPT's capability in specialized scenarios and long-term tasks, we have integrated it with a novel cognitive architecture that combines memory offloading, knowledge retrieval, and context reflection. As for evaluation, we build WorldNet, a multimodal state transition prediction benchmark encompassing varied real-life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPT's capability to accurately model state transition patterns, affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPT's emerging potential in serving as a world simulator, helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes. The project is available on \url{https://github.com/DCDmllm/WorldGPT}.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2404.16077.pdf' target='_blank'>https://arxiv.org/pdf/2404.16077.pdf</a></span>   <span><a href='https://github.com/thuml/CompilerDream' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Deng, Jialong Wu, Ningya Feng, Jianmin Wang, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16077">CompilerDream: Learning a Compiler World Model for General Code Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective code optimization in compilers is crucial for computer and software engineering. The success of these optimizations primarily depends on the selection and ordering of the optimization passes applied to the code. While most compilers rely on a fixed sequence of optimization passes, current methods to find the optimal sequence either employ impractically slow search algorithms or learning methods that struggle to generalize to code unseen during training. We introduce CompilerDream, a model-based reinforcement learning approach to general code optimization. CompilerDream comprises a compiler world model that accurately simulates the intrinsic properties of optimization passes and an agent trained on this model to produce effective optimization strategies. By training on a large-scale program dataset, CompilerDream is equipped to serve as a general code optimizer across various application scenarios and source-code languages. Our extensive experiments first highlight CompilerDream's strong optimization capabilities for autotuning, where it leads the CompilerGym leaderboard. More importantly, the zero-shot generalization ability of large-scale trained compiler world model and agent, excels across diverse datasets, surpassing LLVM's built-in optimizations and other state-of-the-art methods in both settings of value prediction and end-to-end code optimization.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2404.10775.pdf' target='_blank'>https://arxiv.org/pdf/2404.10775.pdf</a></span>   <span><a href='https://umass-embodied-agi.github.io/COMBO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10775">COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at https://umass-embodied-agi.github.io/COMBO/.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2404.08828.pdf' target='_blank'>https://arxiv.org/pdf/2404.08828.pdf</a></span>   <span><a href='https://github.com/apple/ml-rlhf-hindsight-prior' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mudit Verma, Katherine Metcalf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08828">Hindsight PRIORs for Reward Learning from Human Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning a reward from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference, which result in data intensive approaches and subpar reward functions. We address such limitations by introducing a credit assignment strategy (Hindsight PRIOR) that uses a world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, Hindsight PRIOR recovers on average significantly (p<0.05) more reward on MetaWorld (20%) and DMC (15%). The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision. Code repository can be found at https://github.com/apple/ml-rlhf-hindsight-prior.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2403.19652.pdf' target='_blank'>https://arxiv.org/pdf/2403.19652.pdf</a></span>   <span><a href='https://sirui-xu.github.io/InterDreamer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19652">InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2403.10967.pdf' target='_blank'>https://arxiv.org/pdf/2403.10967.pdf</a></span>   <span><a href='https://github.com/sai-prasanna/dreaming_of_many_worlds' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Prasanna, Karim Farid, Raghu Rajan, AndrÃ© Biedenkapp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10967">Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our approach is evaluated on two tasks from the CARL benchmark suite, which is tailored to study contextual RL. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the "dreams" of the world model. We further find qualitatively that our approach allows Dreamer to disentangle the latent state from context, allowing it to extrapolate its dreams to the many worlds of unseen contexts. The code for all our experiments is available at https://github.com/sai-prasanna/dreaming_of_many_worlds.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2403.08321.pdf' target='_blank'>https://arxiv.org/pdf/2403.08321.pdf</a></span>   <span><a href='https://guanxinglu.github.io/ManiGaussian/' target='_blank'>  GitHub</a></span> <span><a href='https://guanxinglu.github.io/ManiGaussian/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08321">ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\% in average success rate. Project page: https://guanxinglu.github.io/ManiGaussian/.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2403.07376.pdf' target='_blank'>https://arxiv.org/pdf/2403.07376.pdf</a></span>   <span><a href='https://github.com/expectorlin/NavCoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07376">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2403.06833.pdf' target='_blank'>https://arxiv.org/pdf/2403.06833.pdf</a></span>   <span><a href='https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06833">Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model's outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2402.11871.pdf' target='_blank'>https://arxiv.org/pdf/2402.11871.pdf</a></span>   <span><a href='https://aair-lab.github.io/r2l-lamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naman Shah, Jayesh Nagpal, Siddharth Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11871">From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots still lag behind humans in their ability to generalize from limited experience, particularly when transferring learned behaviors to long-horizon tasks in unseen environments. We present the first method that enables robots to autonomously invent symbolic, relational concepts directly from a small number of raw, unsegmented, and unannotated demonstrations. From these, the robot learns logic-based world models that support zero-shot generalization to tasks of far greater complexity than those in training. Our framework achieves performance on par with hand-engineered symbolic models, while scaling to execution horizons far beyond training and handling up to 18$\times$ more objects than seen during learning. The results demonstrate a framework for autonomously acquiring transferable symbolic abstractions from raw robot experience, contributing toward the development of interpretable, scalable, and generalizable robot planning systems. Project website and code: https://aair-lab.github.io/r2l-lamp.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2402.11871.pdf' target='_blank'>https://arxiv.org/pdf/2402.11871.pdf</a></span>   <span><a href='https://aair-lab.github.io/r2l-lamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naman Shah, Jayesh Nagpal, Siddharth Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11871">From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots still lag behind humans in their ability to generalize from limited experience, particularly when transferring learned behaviors to long-horizon tasks in unseen environments. We present the first method that enables robots to autonomously invent symbolic, relational concepts directly from a small number of raw, unsegmented, and unannotated demonstrations. From these, the robot learns logic-based world models that support zero-shot generalization to tasks of far greater complexity than those in training. Our framework achieves performance on par with hand-engineered symbolic models, while scaling to execution horizons far beyond training and handling up to 18$\times$ more objects than seen during learning. The results demonstrate a framework for autonomously acquiring transferable symbolic abstractions from raw robot experience, contributing toward the development of interpretable, scalable, and generalizable robot planning systems. Project website and code: https://aair-lab.github.io/r2l-lamp.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2402.05643.pdf' target='_blank'>https://arxiv.org/pdf/2402.05643.pdf</a></span>   <span><a href='https://github.com/leor-c/REM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05643">Improving Token-Based World Models with Parallel Observation Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at \url{https://github.com/leor-c/REM}.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2402.03326.pdf' target='_blank'>https://arxiv.org/pdf/2402.03326.pdf</a></span>   <span><a href='https://github.com/JonathanCollu/Slot-Structured-World-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Collu, Riccardo Majellaro, Aske Plaat, Thomas M. Moerland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03326">Slot Structured World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to perceive and reason about individual objects and their interactions is a goal to be achieved for building intelligent artificial systems. State-of-the-art approaches use a feedforward encoder to extract object embeddings and a latent graph neural network to model the interaction between these object embeddings. However, the feedforward encoder can not extract {\it object-centric} representations, nor can it disentangle multiple objects with similar appearance. To solve these issues, we introduce {\it Slot Structured World Models} (SSWM), a class of world models that combines an {\it object-centric} encoder (based on Slot Attention) with a latent graph-based dynamics model. We evaluate our method in the Spriteworld benchmark with simple rules of physical interaction, where Slot Structured World Models consistently outperform baselines on a range of (multi-step) prediction tasks with action-conditional object interactions. All code to reproduce paper experiments is available from \url{https://github.com/JonathanCollu/Slot-Structured-World-Models}.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2402.02968.pdf' target='_blank'>https://arxiv.org/pdf/2402.02968.pdf</a></span>   <span><a href='https://github.com/rolsheng/MM-VUFM4DS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao, Qun Li, Guobin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02968">Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2401.08045.pdf' target='_blank'>https://arxiv.org/pdf/2401.08045.pdf</a></span>   <span><a href='https://github.com/zhanghm1995/Forge_VFM4AD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/zhanghm1995/Forge_VFM4AD,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08045">Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2312.10812.pdf' target='_blank'>https://arxiv.org/pdf/2312.10812.pdf</a></span>   <span><a href='http://github.com/schmidtdominik/LAPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dominik Schmidt, Minqi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10812">Learning to Act without Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information, and thereby latent-action policies, world models, and inverse dynamics models, purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled dataset, or online with rewards. LAPO takes a first step towards pre-training powerful, generalist policies and world models on the vast amounts of videos readily available on the web.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2312.04316.pdf' target='_blank'>https://arxiv.org/pdf/2312.04316.pdf</a></span>   <span><a href='https://github.com/PJLab-ADG/awesome-knowledge-driven-AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Yeqi Bai, Pinlong Cai, Licheng Wen, Daocheng Fu, Bo Zhang, Xuemeng Yang, Xinyu Cai, Tao Ma, Jianfei Guo, Xing Gao, Min Dou, Yikang Li, Botian Shi, Yong Liu, Liang He, Yu Qiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04316">Towards Knowledge-driven Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the emerging knowledge-driven autonomous driving technologies. Our investigation highlights the limitations of current autonomous driving systems, in particular their sensitivity to data bias, difficulty in handling long-tail scenarios, and lack of interpretability. Conversely, knowledge-driven methods with the abilities of cognition, generalization and life-long learning emerge as a promising way to overcome these challenges. This paper delves into the essence of knowledge-driven autonomous driving and examines its core components: dataset \& benchmark, environment, and driver agent. By leveraging large language models, world models, neural rendering, and other advanced artificial intelligence techniques, these components collectively contribute to a more holistic, adaptive, and intelligent autonomous driving system. The paper systematically organizes and reviews previous research efforts in this area, and provides insights and guidance for future research and practical applications of autonomous driving. We will continually share the latest updates on cutting-edge developments in knowledge-driven autonomous driving along with the relevant valuable open-source resources at: \url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2312.02019.pdf' target='_blank'>https://arxiv.org/pdf/2312.02019.pdf</a></span>   <span><a href='https://github.com/argmax-ai/aime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02019">Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike most reinforcement learning agents which require an unrealistic amount of environment interactions to learn a new behaviour, humans excel at learning quickly by merely observing and imitating others. This ability highly depends on the fact that humans have a model of their own embodiment that allows them to infer the most likely actions that led to the observed behaviour. In this paper, we propose Action Inference by Maximising Evidence (AIME) to replicate this behaviour using world models. AIME consists of two distinct phases. In the first phase, the agent learns a world model from its past experience to understand its own body by maximising the ELBO. While in the second phase, the agent is given some observation-only demonstrations of an expert performing a novel task and tries to imitate the expert's behaviour. AIME achieves this by defining a policy as an inference model and maximising the evidence of the demonstration under the policy and world model. Our method is "zero-shot" in the sense that it does not require further training for the world model or online interactions with the environment after given the demonstration. We empirically validate the zero-shot imitation performance of our method on the Walker and Cheetah embodiment of the DeepMind Control Suite and find it outperforms the state-of-the-art baselines. Code is available at: https://github.com/argmax-ai/aime.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2311.17918.pdf' target='_blank'>https://arxiv.org/pdf/2311.17918.pdf</a></span>   <span><a href='https://github.com/BraveGroup/Drive-WM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17918">Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In autonomous driving, predicting future events in advance and evaluating the foreseeable risks empowers autonomous vehicles to better plan their actions, enhancing safety and efficiency on the road. To this end, we propose Drive-WM, the first driving world model compatible with existing end-to-end planning models. Through a joint spatial-temporal modeling facilitated by view factorization, our model generates high-fidelity multiview videos in driving scenes. Building on its powerful generation ability, we showcase the potential of applying the world model for safe driving planning for the first time. Particularly, our Drive-WM enables driving into multiple futures based on distinct driving maneuvers, and determines the optimal trajectory according to the image-based rewards. Evaluation on real-world driving datasets verifies that our method could generate high-quality, consistent, and controllable multiview videos, opening up possibilities for real-world simulations and safe planning.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2311.16038.pdf' target='_blank'>https://arxiv.org/pdf/2311.16038.pdf</a></span>   <span><a href='https://github.com/wzzheng/OccWorld' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/OccWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16038">OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding how the 3D scene evolves is vital for making decisions in autonomous driving. Most existing methods achieve this by predicting the movements of object boxes, which cannot capture more fine-grained scene information. In this paper, we explore a new framework of learning a world model, OccWorld, in the 3D Occupancy space to simultaneously predict the movement of the ego car and the evolution of the surrounding scenes. We propose to learn a world model based on 3D occupancy rather than 3D bounding boxes and segmentation maps for three reasons: 1) expressiveness. 3D occupancy can describe the more fine-grained 3D structure of the scene; 2) efficiency. 3D occupancy is more economical to obtain (e.g., from sparse LiDAR points). 3) versatility. 3D occupancy can adapt to both vision and LiDAR. To facilitate the modeling of the world evolution, we learn a reconstruction-based scene tokenizer on the 3D occupancy to obtain discrete scene tokens to describe the surrounding scenes. We then adopt a GPT-like spatial-temporal generative transformer to generate subsequent scene and ego tokens to decode the future occupancy and ego trajectory. Extensive experiments on the widely used nuScenes benchmark demonstrate the ability of OccWorld to effectively model the evolution of the driving scenes. OccWorld also produces competitive planning results without using instance and map supervision. Code: https://github.com/wzzheng/OccWorld.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2311.09353.pdf' target='_blank'>https://arxiv.org/pdf/2311.09353.pdf</a></span>   <span><a href='https://github.com/RVMI/skiros2' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/matthias-mayr/SkiREIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Mayr, Faseeh Ahmad, Volker Krueger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09353">Flexible and Adaptive Manufacturing by Complementing Knowledge Representation, Reasoning and Planning with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes a novel approach to adaptive manufacturing in the context of small batch production and customization. It focuses on integrating task-level planning and reasoning with reinforcement learning (RL) in the SkiROS2 skill-based robot control platform. This integration enhances the efficiency and adaptability of robotic systems in manufacturing, enabling them to adjust to task variations and learn from interaction data. The paper highlights the architecture of SkiROS2, particularly its world model, skill libraries, and task management. It demonstrates how combining RL with robotic manipulators can learn and improve the execution of industrial tasks. It advocates a multi-objective learning model that eases the learning problem design. The approach can incorporate user priors or previous experiences to accelerate learning and increase safety.
  Spotlight video: https://youtu.be/H5PmZl2rRbs?si=8wmZ-gbwuSJRxe3S&t=1422
  SkiROS2 code: https://github.com/RVMI/skiros2
  SkiROS2 talk at ROSCon: https://vimeo.com/879001825/2a0e9d5412
  SkiREIL code: https://github.com/matthias-mayr/SkiREIL
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2310.18534.pdf' target='_blank'>https://arxiv.org/pdf/2310.18534.pdf</a></span>   <span><a href='https://github.com/ALRhub/MTS3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaisakh Shaj, Saleh Gholam Zadeh, Ozan Demir, Luiz Ricardo Douat, Gerhard Neumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18534">Multi Time Scale World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent agents use internal world models to reason and make predictions about different courses of their actions at many scales. Devising learning paradigms and architectures that allow machines to learn world models that operate at multiple levels of temporal abstractions while dealing with complex uncertainty predictions is a major technical hurdle. In this work, we propose a probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale State Space (MTS3) model. Our model uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future. Our experiments, which focus on action conditional long horizon future predictions, show that MTS3 outperforms recent methods on several system identification benchmarks including complex simulated and real-world dynamical systems. Code is available at this repository: https://github.com/ALRhub/MTS3.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2310.12690.pdf' target='_blank'>https://arxiv.org/pdf/2310.12690.pdf</a></span>   <span><a href='https://trishullab.github.io/cosmos-web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atharva Sehgal, Arya Grayeli, Jennifer J. Sun, Swarat Chaudhuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12690">Neurosymbolic Grounding for Compositional World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CompGen), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CompGen on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CompGen in world modeling. Artifacts are available at: https://trishullab.github.io/cosmos-web/
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2310.10237.pdf' target='_blank'>https://arxiv.org/pdf/2310.10237.pdf</a></span>   <span><a href='https://github.com/TommyDzh/SGOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Ding, Jieming Shi, Shiqi Shen, Xuequn Shang, Jiannong Cao, Zhipeng Wang, Zhi Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10237">SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph-level representation learning is important in a wide range of applications. Existing graph-level models are generally built on i.i.d. assumption for both training and testing graphs. However, in an open world, models can encounter out-of-distribution (OOD) testing graphs that are from different distributions unknown during training. A trustworthy model should be able to detect OOD graphs to avoid unreliable predictions, while producing accurate in-distribution (ID) predictions. To achieve this, we present SGOOD, a novel graph-level OOD detection framework. We find that substructure differences commonly exist between ID and OOD graphs, and design SGOOD with a series of techniques to encode task-agnostic substructures for effective OOD detection. Specifically, we build a super graph of substructures for every graph, and develop a two-level graph encoding pipeline that works on both original graphs and super graphs to obtain substructure-enhanced graph representations. We then devise substructure-preserving graph augmentation techniques to further capture more substructure semantics of ID graphs. Extensive experiments against 11 competitors on numerous graph datasets demonstrate the superiority of SGOOD, often surpassing existing methods by a significant margin. The code is available at https://github.com/TommyDzh/SGOOD.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2310.05167.pdf' target='_blank'>https://arxiv.org/pdf/2310.05167.pdf</a></span>   <span><a href='https://github.com/Snagnar/Hieros' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Mattes, Rainer Schlosser, Ralf Herbrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05167">Hieros: Hierarchical Imagination on Structured State Space Sequence World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.
  We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2310.00344.pdf' target='_blank'>https://arxiv.org/pdf/2310.00344.pdf</a></span>   <span><a href='https://github.com/thuml/HarmonyDream' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/thuml/HarmonyDream' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li, Jianye Hao, Jianmin Wang, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00344">HarmonyDream: Task Harmonization Inside World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a simple yet effective approach, HarmonyDream, which automatically adjusts loss coefficients to maintain task harmonization, i.e. a dynamic equilibrium between the two tasks in world model learning. Our experiments show that the base MBRL method equipped with HarmonyDream gains 10%-69% absolute performance boosts on visual robotic tasks and sets a new state-of-the-art result on the Atari 100K benchmark. Code is available at https://github.com/thuml/HarmonyDream.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2309.00616.pdf' target='_blank'>https://arxiv.org/pdf/2309.00616.pdf</a></span>   <span><a href='https://zheninghuang.github.io/OpenIns3D/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00616">OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D open-vocabulary scene understanding. The OpenIns3D framework employs a "Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic mask proposals in 3D point clouds, the "Snap" module generates synthetic scene-level images at multiple scales and leverages 2D vision-language models to extract interesting objects, and the "Lookup" module searches through the outcomes of "Snap" to assign category names to the proposed masks. This approach, yet simple, achieves state-of-the-art performance across a wide range of 3D open-vocabulary tasks, including recognition, object detection, and instance segmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D facilitates effortless switching between different 2D detectors without requiring retraining. When integrated with powerful 2D open-world models, it achieves excellent results in scene understanding tasks. Furthermore, when combined with LLM-powered 2D models, OpenIns3D exhibits an impressive capability to comprehend and process highly complex text queries that demand intricate reasoning and real-world knowledge. Project page: https://zheninghuang.github.io/OpenIns3D/
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2308.07498.pdf' target='_blank'>https://arxiv.org/pdf/2308.07498.pdf</a></span>   <span><a href='https://github.com/hanqingwangai/Dreamwalker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07498">DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of ``mental experiments.'' Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2308.07234.pdf' target='_blank'>https://arxiv.org/pdf/2308.07234.pdf</a></span>   <span><a href='https://github.com/chaytonmin/UniWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07234">UniWorld: Autonomous Driving Pre-training via World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we draw inspiration from Alberto Elfes' pioneering work in 1989, where he introduced the concept of the occupancy grid as World Models for robots. We imbue the robot with a spatial-temporal world model, termed UniWorld, to perceive its surroundings and predict the future behavior of other participants. UniWorld involves initially predicting 4D geometric occupancy as the World Models for foundational stage and subsequently fine-tuning on downstream tasks. UniWorld can estimate missing information concerning the world state and predict plausible future states of the world. Besides, UniWorld's pre-training process is label-free, enabling the utilization of massive amounts of image-LiDAR pairs to build a Foundational Model.The proposed unified pre-training framework demonstrates promising results in key tasks such as motion prediction, multi-camera 3D object detection, and surrounding semantic scene completion. When compared to monocular pre-training methods on the nuScenes dataset, UniWorld shows a significant improvement of about 1.5% in IoU for motion prediction, 2.0% in mAP and 2.0% in NDS for multi-camera 3D object detection, as well as a 3% increase in mIoU for surrounding semantic scene completion. By adopting our unified pre-training method, a 25% reduction in 3D training annotation costs can be achieved, offering significant practical value for the implementation of real-world autonomous driving. Codes are publicly available at https://github.com/chaytonmin/UniWorld.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2307.10710.pdf' target='_blank'>https://arxiv.org/pdf/2307.10710.pdf</a></span>   <span><a href='https://haosulab.github.io/RPG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10710">Reparameterized Policy Learning for Multimodal Trajectory Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2307.07176.pdf' target='_blank'>https://arxiv.org/pdf/2307.07176.pdf</a></span>   <span><a href='https://github.com/PKU-Alignment/SafeDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07176">SafeDreamer: Safe Reinforcement Learning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks. Further details can be found in the code repository: \url{https://github.com/PKU-Alignment/SafeDreamer}.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2307.02064.pdf' target='_blank'>https://arxiv.org/pdf/2307.02064.pdf</a></span>   <span><a href='https://fdeng18.github.io/s4wm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Deng, Junyeong Park, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02064">Facing Off World Model Backbones: RNNs, Transformers, and S4</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are a fundamental component in model-based reinforcement learning (MBRL). To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first world model compatible with parallelizable SSMs including S4 and its variants. By incorporating latent variable modeling, S4WM can efficiently generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2306.16927.pdf' target='_blank'>https://arxiv.org/pdf/2306.16927.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16927">End-to-end Autonomous Driving: Challenges and Frontiers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2306.15934.pdf' target='_blank'>https://arxiv.org/pdf/2306.15934.pdf</a></span>   <span><a href='https://github.com/AutonomousAgentsLab/curiousreplay' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Kauvar, Chris Doyle, Linqi Zhou, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15934">Curious Replay for Model-based Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite. Code for Curious Replay is available at https://github.com/AutonomousAgentsLab/curiousreplay
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2305.18499.pdf' target='_blank'>https://arxiv.org/pdf/2305.18499.pdf</a></span>   <span><a href='https://github.com/thuml/ContextWM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/thuml/ContextWM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Wu, Haoyu Ma, Chaoyi Deng, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18499">Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly separate context and dynamics modeling to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is elaborately realized by incorporating a context encoder to retain contextual information and empower the image decoder, which encourages the latent dynamics model to concentrate on essential temporal variations. Our experiments show that in-the-wild video pre-training equipped with ContextWM can significantly improve the sample efficiency of MBRL in various domains, including robotic manipulation, locomotion, and autonomous driving. Code is available at this repository: https://github.com/thuml/ContextWM.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2305.15260.pdf' target='_blank'>https://arxiv.org/pdf/2305.15260.pdf</a></span>   <span><a href='https://qiwang067.github.io/coworld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15260">Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training offline RL models using visual inputs poses two significant challenges, i.e., the overfitting problem in representation learning and the overestimation bias for expected future rewards. Recent work has attempted to alleviate the overestimation bias by encouraging conservative behaviors. This paper, in contrast, tries to build more flexible constraints for value estimation without impeding the exploration of potential advantages. The key idea is to leverage off-the-shelf RL simulators, which can be easily interacted with in an online manner, as the "test bed" for offline policies. To enable effective online-to-offline knowledge transfer, we introduce CoWorld, a model-based RL approach that mitigates cross-domain discrepancies in state and reward spaces. Experimental results demonstrate the effectiveness of CoWorld, outperforming existing RL approaches by large margins.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2305.14992.pdf' target='_blank'>https://arxiv.org/pdf/2305.14992.pdf</a></span>   <span><a href='https://github.com/Ber666/llm-reasoners' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14992">Reasoning with Language Model is Planning with World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2305.14909.pdf' target='_blank'>https://arxiv.org/pdf/2305.14909.pdf</a></span>   <span><a href='https://guansuns.github.io/pages/llm-dm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14909">Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2303.13515.pdf' target='_blank'>https://arxiv.org/pdf/2303.13515.pdf</a></span>   <span><a href='https://chail.github.io/persistent-nature/' target='_blank'>  GitHub</a></span> <span><a href='https://chail.github.io/persistent-nature/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, Noah Snavely
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13515">Persistent Nature: A Generative Model of Unbounded 3D Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency--for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in contrast to auto-regressive 3D prediction models. Our project page: https://chail.github.io/persistent-nature/.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2303.13002.pdf' target='_blank'>https://arxiv.org/pdf/2303.13002.pdf</a></span>   <span><a href='https://penn-pal-lab.github.io/peg/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward S. Hu, Richard Chang, Oleh Rybkin, Dinesh Jayaraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13002">Planning Goals for Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to "plan goal commands". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://penn-pal-lab.github.io/peg/
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2303.07109.pdf' target='_blank'>https://arxiv.org/pdf/2303.07109.pdf</a></span>   <span><a href='https://github.com/jrobine/twm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Robine, Marc HÃ¶ftmann, Tobias Uelwer, Stefan Harmeling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07109">Transformer-based World Models Are Happy With 100k Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2303.04116.pdf' target='_blank'>https://arxiv.org/pdf/2303.04116.pdf</a></span>   <span><a href='https://github.com/zhejz/TrafficBots' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04116">TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven simulation has become a favorable way to train and test autonomous driving algorithms. The idea of replacing the actual environment with a learned simulator has also been explored in model-based reinforcement learning in the context of world models. In this work, we show data-driven traffic simulation can be formulated as a world model. We present TrafficBots, a multi-agent policy built upon motion prediction and end-to-end driving, and based on TrafficBots we obtain a world model tailored for the planning module of autonomous vehicles. Existing data-driven traffic simulators are lacking configurability and scalability. To generate configurable behaviors, for each agent we introduce a destination as navigational information, and a time-invariant latent personality that specifies the behavioral style. To improve the scalability, we present a new scheme of positional encoding for angles, allowing all agents to share the same vectorized context and the use of an architecture based on dot-product attention. As a result, we can simulate all traffic participants seen in dense urban scenarios. Experiments on the Waymo open motion dataset show TrafficBots can simulate realistic multi-agent behaviors and achieve good performance on the motion prediction task.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2301.04783.pdf' target='_blank'>https://arxiv.org/pdf/2301.04783.pdf</a></span>   <span><a href='https://github.com/robin-karlsson0/predictive-world-models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Karlsson, Alexander Carballo, Keisuke Fujii, Kento Ohtani, Kazuya Takeda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04783">Predictive World Models from Real-World Partial Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive scientists believe adaptable intelligent agents like humans perform reasoning through learned causal mental simulations of agents and environments. The problem of learning such simulations is called predictive world modeling. Recently, reinforcement learning (RL) agents leveraging world models have achieved SOTA performance in game environments. However, understanding how to apply the world modeling approach in complex real-world environments relevant to mobile robots remains an open question. In this paper, we present a framework for learning a probabilistic predictive world model for real-world road environments. We implement the model using a hierarchical VAE (HVAE) capable of predicting a diverse set of fully observed plausible worlds from accumulated sensor observations. While prior HVAE methods require complete states as ground truth for learning, we present a novel sequential training method to allow HVAEs to learn to predict complete states from partially observed states only. We experimentally demonstrate accurate spatial structure prediction of deterministic regions achieving 96.21 IoU, and close the gap to perfect prediction by 62% for stochastic regions using the best prediction. By extending HVAEs to cases where complete ground truth states do not exist, we facilitate continual learning of spatial prediction as a step towards realizing explainable and comprehensive predictive world models for real-world mobile robotics applications. Code is available at https://github.com/robin-karlsson0/predictive-world-models.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2210.10763.pdf' target='_blank'>https://arxiv.org/pdf/2210.10763.pdf</a></span>   <span><a href='https://nicklashansen.github.io/xtra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, Zhuowen Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.10763">On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) algorithms can solve challenging control problems directly from image observations, but they often require millions of environment interactions to do so. Recently, model-based RL algorithms have greatly improved sample-efficiency by concurrently learning an internal model of the world, and supplementing real environment interactions with imagined rollouts for policy improvement. However, learning an effective model of the world from scratch is challenging, and in stark contrast to humans that rely heavily on world understanding and visual cues for learning new skills. In this work, we investigate whether internal models learned by modern model-based RL algorithms can be leveraged to solve new, distinctly different tasks faster. We propose Model-Based Cross-Task Transfer (XTRA), a framework for sample-efficient online RL with scalable pretraining and finetuning of learned world models. By offline multi-task pretraining and online cross-task finetuning, we achieve substantial improvements over a baseline trained from scratch; we improve mean performance of model-based algorithm EfficientZero by 23%, and by as much as 71% in some instances.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2210.00999.pdf' target='_blank'>https://arxiv.org/pdf/2210.00999.pdf</a></span>   <span><a href='https://github.com/zdhNarsil/Stochastic-Marginal-Actor-Critic' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dinghuai Zhang, Aaron Courville, Yoshua Bengio, Qinqing Zheng, Amy Zhang, Ricky T. Q. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00999">Latent State Marginalization as a Low-cost Approach for Improving Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the maximum entropy (MaxEnt) reinforcement learning (RL) framework -- often touted for its exploration and robustness capabilities -- is usually motivated from a probabilistic perspective, the use of deep probabilistic models has not gained much traction in practice due to their inherent complexity. In this work, we propose the adoption of latent variable policies within the MaxEnt framework, which we show can provably approximate any policy distribution, and additionally, naturally emerges under the use of world models with a latent belief state. We discuss why latent variable policies are difficult to train, how naive approaches can fail, then subsequently introduce a series of improvements centered around low-cost marginalization of the latent state, allowing us to make full use of the latent state at minimal additional cost. We instantiate our method under the actor-critic framework, marginalizing both the actor and critic. The resulting algorithm, referred to as Stochastic Marginal Actor-Critic (SMAC), is simple yet effective. We experimentally validate our method on continuous control tasks, showing that effective marginalization can lead to better exploration and more robust training. Our implementation is open sourced at https://github.com/zdhNarsil/Stochastic-Marginal-Actor-Critic.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2209.00588.pdf' target='_blank'>https://arxiv.org/pdf/2209.00588.pdf</a></span>   <span><a href='https://github.com/eloialonso/iris' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Micheli, Eloi Alonso, FranÃ§ois Fleuret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00588">Transformers are Sample-Efficient World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2206.15477.pdf' target='_blank'>https://arxiv.org/pdf/2206.15477.pdf</a></span>   <span><a href='https://github.com/facebookresearch/denoised_mdp' target='_blank'>  GitHub</a></span> <span><a href='https://ssnl.github.io/denoised_mdp/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongzhou Wang, Simon S. Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.15477">Denoised MDPs: Learning World Models Better Than the World Itself</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors.How can artificial agents do the same? What kind of information can agents safely discard as noises?
  In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy optimization control tasks as well as the non-control task of joint position regression.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2206.04384.pdf' target='_blank'>https://arxiv.org/pdf/2206.04384.pdf</a></span>   <span><a href='https://github.com/TsuTikgiau/ValueMemoryGraph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyao Zhu, Li Erran Li, Mohamed Elhoseiny
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.04384">Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline RL dataset. Together with an action translator that converts the abstract graph actions in VMG to real actions in the original environment, VMG controls agents to maximize episode returns. Our experiments on the D4RL benchmark show that VMG can outperform state-of-the-art offline RL methods in several goal-oriented tasks, especially when environments have sparse rewards and long temporal horizons. Code is available at https://github.com/TsuTikgiau/ValueMemoryGraph
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2112.02817.pdf' target='_blank'>https://arxiv.org/pdf/2112.02817.pdf</a></span>   <span><a href='https://github.com/ED2-source-code/ED2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianye Hao, Yifu Yuan, Cong Wang, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.02817">ED2: Environment Dynamics Decomposition World Models for Continuous Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) achieves significant sample efficiency in practice in comparison to model-free RL, but its performance is often limited by the existence of model prediction error. To reduce the model error, standard MBRL approaches train a single well-designed network to fit the entire environment dynamics, but this wastes rich information on multiple sub-dynamics which can be modeled separately, allowing us to construct the world model more accurately. In this paper, we propose the Environment Dynamics Decomposition (ED2), a novel world model construction framework that models the environment in a decomposing manner. ED2 contains two key components: sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an environment automatically and then D2P constructs the decomposed world model following the sub-dynamics. ED2 can be easily combined with existing MBRL algorithms and empirical results show that ED2 significantly reduces the model error, increases the sample efficiency, and achieves higher asymptotic performance when combined with the state-of-the-art MBRL algorithms on various continuous control tasks. Our code is open source and available at https://github.com/ED2-source-code/ED2.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2508.09561.pdf' target='_blank'>https://arxiv.org/pdf/2508.09561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changyuan Zhao, Guangyuan Liu, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Zan Li, Xuemin, Shen, Zhu Han, Sumei Sun, Chau Yuen, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09561">Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2508.14704.pdf' target='_blank'>https://arxiv.org/pdf/2508.14704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14704">MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2506.00417.pdf' target='_blank'>https://arxiv.org/pdf/2506.00417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Gaosheng Zhao, Dusit Niyato, Geng Sun, Shiwen Mao, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00417">World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are emerging as a transformative paradigm in artificial intelligence, enabling agents to construct internal representations of their environments for predictive reasoning, planning, and decision-making. By learning latent dynamics, world models provide a sample-efficient framework that is especially valuable in data-constrained or safety-critical scenarios. In this paper, we present a comprehensive overview of world models, highlighting their architecture, training paradigms, and applications across prediction, generation, planning, and causal reasoning. We compare and distinguish world models from related concepts such as digital twins, the metaverse, and foundation models, clarifying their unique role as embedded cognitive engines for autonomous agents. We further propose Wireless Dreamer, a novel world model-based reinforcement learning framework tailored for wireless edge intelligence optimization, particularly in low-altitude wireless networks (LAWNs). Through a weather-aware UAV trajectory planning case study, we demonstrate the effectiveness of our framework in improving learning efficiency and decision quality.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2502.09923.pdf' target='_blank'>https://arxiv.org/pdf/2502.09923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinning Zhou, Chengyang Ying, Yao Feng, Hang Su, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09923">Self-Consistent Model-based Adaptation for Visual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy's representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2303.05092.pdf' target='_blank'>https://arxiv.org/pdf/2303.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyang Ying, Xinning Zhou, Zhongkai Hao, Hang Su, Songming Liu, Dong Yan, Jun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05092">Task Aware Dreamer for Task Generalization in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. The ability to generalize across tasks is important as it determines an agent's adaptability to real-world scenarios where reward mechanisms might vary. In this work, we first show that training a general world model can utilize similar structures in these tasks and help train more generalizable agents. Extending world models into the task generalization setting, we introduce a novel method named Task Aware Dreamer (TAD), which integrates reward-informed features to identify consistent latent characteristics across tasks. Within TAD, we compute the variational lower bound of sample data log-likelihood, which introduces a new term designed to differentiate tasks using their states, as the optimization objective of our reward-informed world models. To demonstrate the advantages of the reward-informed policy in TAD, we introduce a new metric called Task Distribution Relevance (TDR) which quantitatively measures the relevance of different tasks. For tasks exhibiting a high TDR, i.e., the tasks differ significantly, we illustrate that Markovian policies struggle to distinguish them, thus it is necessary to utilize reward-informed policies in TAD. Extensive experiments in both image-based and state-based tasks show that TAD can significantly improve the performance of handling different tasks simultaneously, especially for those with high TDR, and display a strong generalization ability to unseen tasks.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2505.22246.pdf' target='_blank'>https://arxiv.org/pdf/2505.22246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nedko Savov, Naser Kazemi, Deheng Zhang, Danda Pani Paudel, Xi Wang, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22246">StateSpaceDiffuser: Bringing Long Context to Diffusion World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have recently become promising tools for predicting realistic visuals based on actions in complex environments. However, their reliance on only a few recent observations leads them to lose track of the long-term context. Consequently, in just a few steps the generated scenes drift from what was previously observed, undermining the temporal coherence of the sequence. This limitation of the state-of-the-art world models, most of which rely on diffusion, comes from their lack of a lasting environment state. To address this problem, we introduce StateSpaceDiffuser, where a diffusion model is enabled to perform long-context tasks by integrating features from a state-space model, representing the entire interaction history. This design restores long-term memory while preserving the high-fidelity synthesis of diffusion models. To rigorously measure temporal consistency, we develop an evaluation protocol that probes a model's ability to reinstantiate seen content in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser significantly outperforms a strong diffusion-only baseline, maintaining a coherent visual context for an order of magnitude more steps. It delivers consistent views in both a 2D maze navigation and a complex 3D environment. These results establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2502.10012.pdf' target='_blank'>https://arxiv.org/pdf/2502.10012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asen Nachkov, Danda Pani Paudel, Jan-Nico Zaech, Davide Scaramuzza, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10012">Dream to Drive: Model-Based Vehicle Control Using Analytic World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2507.08885.pdf' target='_blank'>https://arxiv.org/pdf/2507.08885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baining Zhao, Rongze Tang, Mingyuan Jia, Ziyou Wang, Fanghang Man, Xin Zhang, Yu Shang, Weichen Zhang, Chen Gao, Wei Wu, Xin Wang, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08885">AirScape: An Aerial Generative World Model with Motion Controllability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to enable robots to predict the outcomes of their own motion intentions in three-dimensional space has been a fundamental problem in embodied intelligence. To explore more general spatial imagination capabilities, here we present AirScape, the first world model designed for six-degree-of-freedom aerial agents. AirScape predicts future observation sequences based on current visual inputs and motion intentions. Specifically, we construct an dataset for aerial world model training and testing, which consists of 11k video-intention pairs. This dataset includes first-person-view videos capturing diverse drone actions across a wide range of scenarios, with over 1,000 hours spent annotating the corresponding motion intentions. Then we develop a two-phase training schedule to train a foundation model -- initially devoid of embodied spatial knowledge -- into a world model that is controllable by motion intentions and adheres to physical spatio-temporal constraints.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2503.24388.pdf' target='_blank'>https://arxiv.org/pdf/2503.24388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghan Zhao, Wenwei Zhang, Haian Huang, Kuikun Liu, Jianfei Gao, Gaoang Wang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24388">RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than $17\times$ sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2410.23277.pdf' target='_blank'>https://arxiv.org/pdf/2410.23277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, Kai-Wei Chang, Linjie Li, Kevin Lin, Chung-Ching Lin, Jianfeng Wang, Zhengyuan Yang, Yingnian Wu, Lijuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23277">SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2406.08407.pdf' target='_blank'>https://arxiv.org/pdf/2406.08407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08407">MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of "world models" -- interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2505.20425.pdf' target='_blank'>https://arxiv.org/pdf/2505.20425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raktim Gautam Goswami, Prashanth Krishnamurthy, Yann LeCun, Farshad Khorrami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20425">OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual imitation learning enables robotic agents to acquire skills by observing expert demonstration videos. In the one-shot setting, the agent generates a policy after observing a single expert demonstration without additional fine-tuning. Existing approaches typically train and evaluate on the same set of tasks, varying only object configurations, and struggle to generalize to unseen tasks with different semantic or structural requirements. While some recent methods attempt to address this, they exhibit low success rates on hard test tasks that, despite being visually similar to some training tasks, differ in context and require distinct responses. Additionally, most existing methods lack an explicit model of environment dynamics, limiting their ability to reason about future states. To address these limitations, we propose a novel framework for one-shot visual imitation learning via world-model-guided trajectory generation. Given an expert demonstration video and the agent's initial observation, our method leverages a learned world model to predict a sequence of latent states and actions. This latent trajectory is then decoded into physical waypoints that guide the agent's execution. Our method is evaluated on two simulated benchmarks and three real-world robotic platforms, where it consistently outperforms prior approaches, with over 30% improvement in some cases.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2507.07978.pdf' target='_blank'>https://arxiv.org/pdf/2507.07978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Li, Zhiwen Fan, Wenyan Cong, Xinhang Liu, Yuyang Yin, Matt Foutter, Panwang Pan, Chenyu You, Yue Wang, Zhangyang Wang, Yao Zhao, Marco Pavone, Yunchao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07978">Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2406.19756.pdf' target='_blank'>https://arxiv.org/pdf/2406.19756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojun Jiang, Meng Li, Zhenguo Sun, Ning Jia, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19756">Structure-aware World Model for Probe Guidance via Large-scale Self-supervised Pre-train</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complex structure of the heart leads to significant challenges in echocardiography, especially in acquisition cardiac ultrasound images. Successful echocardiography requires a thorough understanding of the structures on the two-dimensional plane and the spatial relationships between planes in three-dimensional space. In this paper, we innovatively propose a large-scale self-supervised pre-training method to acquire a cardiac structure-aware world model. The core innovation lies in constructing a self-supervised task that requires structural inference by predicting masked structures on a 2D plane and imagining another plane based on pose transformation in 3D space. To support large-scale pre-training, we collected over 1.36 million echocardiograms from ten standard views, along with their 3D spatial poses. In the downstream probe guidance task, we demonstrate that our pre-trained model consistently reduces guidance errors across the ten most common standard views on the test set with 0.29 million samples from 74 routine clinical scans, indicating that structure-aware pre-training benefits the scanning.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2406.13165.pdf' target='_blank'>https://arxiv.org/pdf/2406.13165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojun Jiang, Zhenguo Sun, Ning Jia, Meng Li, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13165">Cardiac Copilot: Automatic Probe Guidance for Echocardiography with World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Echocardiography is the only technique capable of real-time imaging of the heart and is vital for diagnosing the majority of cardiac diseases. However, there is a severe shortage of experienced cardiac sonographers, due to the heart's complex structure and significant operational challenges. To mitigate this situation, we present a Cardiac Copilot system capable of providing real-time probe movement guidance to assist less experienced sonographers in conducting freehand echocardiography. This system can enable non-experts, especially in primary departments and medically underserved areas, to perform cardiac ultrasound examinations, potentially improving global healthcare delivery. The core innovation lies in proposing a data-driven world model, named Cardiac Dreamer, for representing cardiac spatial structures. This world model can provide structure features of any cardiac planes around the current probe position in the latent space, serving as an precise navigation map for autonomous plane localization. We train our model with real-world ultrasound data and corresponding probe motion from 110 routine clinical scans with 151K sample pairs by three certified sonographers. Evaluations on three standard planes with 37K sample pairs demonstrate that the world model can reduce navigation errors by up to 33\% and exhibit more stable performance.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2509.22642.pdf' target='_blank'>https://arxiv.org/pdf/2509.22642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22642">WoW: Towards a World omniscient World model Through Embodied Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2509.22642.pdf' target='_blank'>https://arxiv.org/pdf/2509.22642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22642">WoW: Towards a World omniscient World model Through Embodied Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2506.18897.pdf' target='_blank'>https://arxiv.org/pdf/2506.18897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia, Zichen He, Yuzhen Liu, Tingguang Li, Lei Han, Sirui Han, Shanghang Zhang, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18897">MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2505.16422.pdf' target='_blank'>https://arxiv.org/pdf/2505.16422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoran Yin, Xu Luo, Hao Wu, Lianli Gao, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16422">Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automatic control of mobile devices is essential for efficiently performing complex tasks that involve multiple sequential steps. However, these tasks pose significant challenges due to the limited environmental information available at each step, primarily through visual observations. As a result, current approaches, which typically rely on reactive policies, focus solely on immediate observations and often lead to suboptimal decision-making. To address this problem, we propose \textbf{Foresighted Planning with World Model-Driven Code Execution (FPWC)},a framework that prioritizes natural language understanding and structured reasoning to enhance the agent's global understanding of the environment by developing a task-oriented, refinable \emph{world model} at the outset of the task. Foresighted actions are subsequently generated through iterative planning within this world model, executed in the form of executable code. Extensive experiments conducted in simulated environments and on real mobile devices demonstrate that our method outperforms previous approaches, particularly achieving a 44.4\% relative improvement in task success rate compared to the state-of-the-art in the simulated environment. Code and demo are provided in the supplementary material.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2505.16422.pdf' target='_blank'>https://arxiv.org/pdf/2505.16422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoran Yin, Xu Luo, Hao Wu, Lianli Gao, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16422">Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automatic control of mobile devices is essential for efficiently performing complex tasks that involve multiple sequential steps. However, these tasks pose significant challenges due to the limited environmental information available at each step, primarily through visual observations. As a result, current approaches, which typically rely on reactive policies, focus solely on immediate observations and often lead to suboptimal decision-making. To address this problem, we propose \textbf{Foresighted Planning with World Model-Driven Code Execution (FPWC)},a framework that prioritizes natural language understanding and structured reasoning to enhance the agent's global understanding of the environment by developing a task-oriented, refinable \emph{world model} at the outset of the task. Foresighted actions are subsequently generated through iterative planning within this world model, executed in the form of executable code. Extensive experiments conducted in simulated environments and on real mobile devices demonstrate that our method outperforms previous approaches, particularly achieving a 44.4\% relative improvement in task success rate compared to the state-of-the-art in the simulated environment. Code and demo are provided in the supplementary material.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2406.15836.pdf' target='_blank'>https://arxiv.org/pdf/2406.15836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Chenjia Bai, Bin Zhao, Junchi Yan, Xiu Li, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15836">Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue in a centralized architecture arising from a large number of agents, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Results on Starcraft Multi-Agent Challenge (SMAC) show that it outperforms strong model-free approaches and existing model-based methods in both sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2406.15836.pdf' target='_blank'>https://arxiv.org/pdf/2406.15836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Chenjia Bai, Bin Zhao, Junchi Yan, Xiu Li, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15836">Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue in a centralized architecture arising from a large number of agents, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Results on Starcraft Multi-Agent Challenge (SMAC) show that it outperforms strong model-free approaches and existing model-based methods in both sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2410.15461.pdf' target='_blank'>https://arxiv.org/pdf/2410.15461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15461">EVA: An Embodied World Model for Future Video Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have made significant progress in simulating future states, showcasing their potential as world simulators in embodied scenarios. However, existing models often lack robust understanding, limiting their ability to perform multi-step predictions or handle Out-of-Distribution (OOD) scenarios. To address this challenge, we propose the Reflection of Generation (RoG), a set of intermediate reasoning strategies designed to enhance video prediction. It leverages the complementary strengths of pre-trained vision-language and video generation models, enabling them to function as a world model in embodied scenarios. To support RoG, we introduce Embodied Video Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates embodied world models across diverse tasks and scenarios, utilizing both in-domain and OOD datasets. Building on this foundation, we devise a world model, Embodied Video Anticipator (EVA), that follows a multistage training paradigm to generate high-fidelity video frames and apply an autoregressive strategy to enable adaptive generalization for longer video sequences. Extensive experiments demonstrate the efficacy of EVA in various downstream tasks like video generation and robotics, thereby paving the way for large-scale pre-trained models in real-world video prediction applications. The video demos are available at \hyperlink{https://sites.google.com/view/icml-eva}{https://sites.google.com/view/icml-eva}.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2510.07974.pdf' target='_blank'>https://arxiv.org/pdf/2510.07974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07974">Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2510.07974.pdf' target='_blank'>https://arxiv.org/pdf/2510.07974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07974">Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2510.07974.pdf' target='_blank'>https://arxiv.org/pdf/2510.07974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialu Du, Guiyang Hou, Yihui Fu, Chen Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07974">Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2508.13154.pdf' target='_blank'>https://arxiv.org/pdf/2508.13154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13154">4DNeX: Feed-Forward 4D Generative Modeling Made Easy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2502.20694.pdf' target='_blank'>https://arxiv.org/pdf/2502.20694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20694">WorldModelBench: Judging Video Generation Models As World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video generation models have rapidly progressed, positioning themselves as video world models capable of supporting decision-making applications like robotics and autonomous driving. However, current benchmarks fail to rigorously evaluate these claims, focusing only on general video quality, ignoring important factors to world models such as physics adherence. To bridge this gap, we propose WorldModelBench, a benchmark designed to evaluate the world modeling capabilities of video generation models in application-driven domains. WorldModelBench offers two key advantages: (1) Against to nuanced world modeling violations: By incorporating instruction-following and physics-adherence dimensions, WorldModelBench detects subtle violations, such as irregular changes in object size that breach the mass conservation law - issues overlooked by prior benchmarks. (2) Aligned with large-scale human preferences: We crowd-source 67K human labels to accurately measure 14 frontier models. Using our high-quality human labels, we further fine-tune an accurate judger to automate the evaluation procedure, achieving 8.6% higher average accuracy in predicting world modeling violations than GPT-4o with 2B parameters. In addition, we demonstrate that training to align human annotations by maximizing the rewards from the judger noticeably improve the world modeling capability. The website is available at https://worldmodelbench-team.github.io.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2407.01455.pdf' target='_blank'>https://arxiv.org/pdf/2407.01455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guiyang Hou, Wenqi Zhang, Yongliang Shen, Linjuan Wu, Weiming Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01455">TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Theory of Mind (ToM)-the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character's higher-order beliefs into another character's first-order beliefs under belief communication period. Experimental results indicate that TimeToM can dramatically improve the reasoning performance of LLMs on ToM questions while taking a big step towards coherent and robust ToM reasoning.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2509.20021.pdf' target='_blank'>https://arxiv.org/pdf/2509.20021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20021">Embodied AI: From LLMs to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2509.20021.pdf' target='_blank'>https://arxiv.org/pdf/2509.20021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20021">Embodied AI: From LLMs to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2502.05907.pdf' target='_blank'>https://arxiv.org/pdf/2502.05907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Zekai Zhou, Ren Wang, Yuwei Zhan, Guangyao Li, Qing Li, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05907">EvoAgent: Agent Autonomous Evolution with Continual World Model for Long-Horizon Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet difficult problem for embodied agents. Existing approaches suffer from two key challenges: (1) they heavily rely on experiences obtained from human-created data or curricula, lacking the ability to continuously update multimodal experiences, and (2) they may encounter catastrophic forgetting issues when faced with new tasks, lacking the ability to continuously update world knowledge. To solve these challenges, this paper presents EvoAgent, an autonomous-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules, i.e., i) the memory-driven planner which uses an LLM along with the WM and interaction memory, to convert LH tasks into executable sub-tasks; ii) the WM-guided action controller which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences; iii) the experience-inspired reflector which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we develop a continual World Model for EvoAgent, which can continuously update the multimodal experience pool and world knowledge through closed-loop dynamics. We conducted extensive experiments on Minecraft, compared with existing methods, EvoAgent can achieve an average success rate improvement of 105% and reduce ineffective actions by more than 6x.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2502.05907.pdf' target='_blank'>https://arxiv.org/pdf/2502.05907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Zekai Zhou, Ren Wang, Yuwei Zhan, Guangyao Li, Qing Li, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05907">EvoAgent: Self-evolving Agent with Continual World Model for Long-Horizon Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet difficult problem for embodied agents. Existing approaches suffer from two key challenges: (1) they heavily rely on experiences obtained from human-created data or curricula, failing to autonomously update and select multimodal experiences, and (2) they may encounter catastrophic forgetting issues when faced with new tasks, failing to autonomously update world knowledge. To solve these challenges, this paper presents {\it EvoAgent}, a self-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules, i.e., i) the memory-driven planner which uses an LLM along with the WM and interaction memory, to convert LH tasks into executable sub-tasks; ii) the WM-guided action controller which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences; iii) the experience-inspired reflector which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we develop a continual World Model for EvoAgent, which can autonomously update the multimodal experience pool and world knowledge through closed-loop dynamics. We conducted extensive experiments on Minecraft and Atair, compared with existing methods, EvoAgent can achieve an average success rate improvement of 105% and reduce ineffective actions by more than 6x.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2502.05907.pdf' target='_blank'>https://arxiv.org/pdf/2502.05907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Zekai Zhou, Ren Wang, Yuwei Zhan, Guangyao Li, Qing Li, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05907">EvoAgent: Self-evolving Agent with Continual World Model for Long-Horizon Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet difficult problem for embodied agents. Existing approaches suffer from two key challenges: (1) they heavily rely on experiences obtained from human-created data or curricula, failing to autonomously update and select multimodal experiences, and (2) they may encounter catastrophic forgetting issues when faced with new tasks, failing to autonomously update world knowledge. To solve these challenges, this paper presents {\it EvoAgent}, a self-evolving agent with a continual World Model (WM), which can autonomously complete various LH tasks across environments through self-planning, self-control, and self-reflection, without human intervention. Our proposed EvoAgent contains three modules, i.e., i) the memory-driven planner which uses an LLM along with the WM and interaction memory, to convert LH tasks into executable sub-tasks; ii) the WM-guided action controller which leverages WM to generate low-level actions and incorporates a self-verification mechanism to update multimodal experiences; iii) the experience-inspired reflector which implements a two-stage curriculum learning algorithm to select experiences for task-adaptive WM updates. Moreover, we develop a continual World Model for EvoAgent, which can autonomously update the multimodal experience pool and world knowledge through closed-loop dynamics. We conducted extensive experiments on Minecraft and Atair, compared with existing methods, EvoAgent can achieve an average success rate improvement of 105% and reduce ineffective actions by more than 6x.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2505.03556.pdf' target='_blank'>https://arxiv.org/pdf/2505.03556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Merouane Debbah, Dusit Niyato, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03556">A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 6G wireless communications aim to establish an intelligent world of ubiquitous connectivity, providing an unprecedented communication experience. Large artificial intelligence models (LAMs) are characterized by significantly larger scales (e.g., billions or trillions of parameters) compared to typical artificial intelligence (AI) models. LAMs exhibit outstanding cognitive abilities, including strong generalization capabilities for fine-tuning to downstream tasks, and emergent capabilities to handle tasks unseen during training. Therefore, LAMs efficiently provide AI services for diverse communication applications, making them crucial tools for addressing complex challenges in future wireless communication systems. This study provides a comprehensive review of the foundations, applications, and challenges of LAMs in communication. First, we introduce the current state of AI-based communication systems, emphasizing the motivation behind integrating LAMs into communications and summarizing the key contributions. We then present an overview of the essential concepts of LAMs in communication. This includes an introduction to the main architectures of LAMs, such as transformer, diffusion models, and mamba. We also explore the classification of LAMs, including large language models (LLMs), large vision models (LVMs), large multimodal models (LMMs), and world models, and examine their potential applications in communication. Additionally, we cover the training methods and evaluation techniques for LAMs in communication systems. Lastly, we introduce optimization strategies such as chain of thought (CoT), retrieval augmented generation (RAG), and agentic systems. Following this, we discuss the research advancements of LAMs across various communication scenarios. Finally, we analyze the challenges in the current research and provide insights into potential future research directions.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2411.09153.pdf' target='_blank'>https://arxiv.org/pdf/2411.09153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09153">VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose VidMan (Video Diffusion for Robot Manipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7% relative improvement, and demonstrates over 9% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2410.10394.pdf' target='_blank'>https://arxiv.org/pdf/2410.10394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaidong Zhang, Pengzhen Ren, Bingqian Lin, Junfan Lin, Shikui Ma, Hang Xu, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10394">PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-guided robotic manipulation is a challenging task that requires an embodied agent to follow abstract user instructions to accomplish various complex manipulation tasks. Previous work trivially fitting the data without revealing the relation between instruction and low-level executable actions, these models are prone to memorizing the surficial pattern of the data instead of acquiring the transferable knowledge, and thus are fragile to dynamic environment changes. To address this issue, we propose a PrIrmitive-driVen waypOinT-aware world model for Robotic manipulation (PIVOT-R) that focuses solely on the prediction of task-relevant waypoints. Specifically, PIVOT-R consists of a Waypoint-aware World Model (WAWM) and a lightweight action prediction module. The former performs primitive action parsing and primitive-driven waypoint prediction, while the latter focuses on decoding low-level actions. Additionally, we also design an asynchronous hierarchical executor (AHE), which can use different execution frequencies for different modules of the model, thereby helping the model reduce computational redundancy and improve model execution efficiency. Our PIVOT-R outperforms state-of-the-art (SoTA) open-source models on the SeaWave benchmark, achieving an average relative improvement of 19.45% across four levels of instruction tasks. Moreover, compared to the synchronously executed PIVOT-R, the execution efficiency of PIVOT-R with AHE is increased by 28-fold, with only a 2.9% drop in performance. These results provide compelling evidence that our PIVOT-R can significantly improve both the performance and efficiency of robotic manipulation.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2507.09177.pdf' target='_blank'>https://arxiv.org/pdf/2507.09177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Liu, Guoji Fu, Chao Du, Wee Sun Lee, Min Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09177">Continual Reinforcement Learning by Planning with Online World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2401.13034.pdf' target='_blank'>https://arxiv.org/pdf/2401.13034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichen Liu, Chao Du, Wee Sun Lee, Min Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13034">Locality Sensitive Sparse Encoding for Learning World Models Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a single pass of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2505.05512.pdf' target='_blank'>https://arxiv.org/pdf/2505.05512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Zhang, Qiang Zhang, Wei Cui, Shuai Shi, Yijie Guo, Gang Han, Wen Zhao, Jingkai Sun, Jiahang Cao, Jiaxu Wang, Hao Cheng, Xiaozhu Ju, Zhengping Che, Renjing Xu, Jian Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05512">Occupancy World Model for Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2410.03904.pdf' target='_blank'>https://arxiv.org/pdf/2410.03904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ksheeraja Raghavan, Samiran Gode, Ankit Shah, Surabhi Raghavan, Wolfram Burgard, Bhiksha Raj, Rita Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03904">Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel, general-purpose audio generation framework specifically designed for anomaly detection and localization. Unlike existing datasets that predominantly focus on industrial and machine-related sounds, our framework focuses a broader range of environments, particularly useful in real-world scenarios where only audio data are available, such as in video-derived or telephonic audio. To generate such data, we propose a new method inspired by the LLM-Modulo framework, which leverages large language models(LLMs) as world models to simulate such real-world scenarios. This tool is modular allowing a plug-and-play approach. It operates by first using LLMs to predict plausible real-world scenarios. An LLM further extracts the constituent sounds, the order and the way in which these should be merged to create coherent wholes. Much like the LLM-Modulo framework, we include rigorous verification of each output stage, ensuring the reliability of the generated data. The data produced using the framework serves as a benchmark for anomaly detection applications, potentially enhancing the performance of models trained on audio data, particularly in handling out-of-distribution cases. Our contributions thus fill a critical void in audio anomaly detection resources and provide a scalable tool for generating diverse, realistic audio data.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2507.23325.pdf' target='_blank'>https://arxiv.org/pdf/2507.23325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Yang, Hongbin Lin, Yueru Luo, Suzhong Fu, Chao Zheng, Xinrui Yan, Shuqi Mei, Kun Tang, Shuguang Cui, Zhen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23325">FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane segment topology reasoning provides comprehensive bird's-eye view (BEV) road scene understanding, which can serve as a key perception module in planning-oriented end-to-end autonomous driving systems. Existing lane topology reasoning methods often fall short in effectively leveraging temporal information to enhance detection and reasoning performance. Recently, stream-based temporal propagation method has demonstrated promising results by incorporating temporal cues at both the query and BEV levels. However, it remains limited by over-reliance on historical queries, vulnerability to pose estimation failures, and insufficient temporal propagation. To overcome these limitations, we propose FASTopoWM, a novel fast-slow lane segment topology reasoning framework augmented with latent world models. To reduce the impact of pose estimation failures, this unified framework enables parallel supervision of both historical and newly initialized queries, facilitating mutual reinforcement between the fast and slow systems. Furthermore, we introduce latent query and BEV world models conditioned on the action latent to propagate the state representations from past observations to the current timestep. This design substantially improves the performance of temporal perception within the slow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate that FASTopoWM outperforms state-of-the-art methods in both lane segment detection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5% on OLS).
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2507.03034.pdf' target='_blank'>https://arxiv.org/pdf/2507.03034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Shuo Shao, Yu He, Junfeng Guo, Tianwei Zhang, Zhan Qin, Pin-Yu Chen, Michael Backes, Philip Torr, Dacheng Tao, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03034">Rethinking Data Protection in the (Generative) Artificial Intelligence Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2507.03034.pdf' target='_blank'>https://arxiv.org/pdf/2507.03034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Shuo Shao, Yu He, Junfeng Guo, Tianwei Zhang, Zhan Qin, Pin-Yu Chen, Michael Backes, Philip Torr, Dacheng Tao, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03034">Rethinking Data Protection in the (Generative) Artificial Intelligence Era</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2503.08751.pdf' target='_blank'>https://arxiv.org/pdf/2503.08751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Wang, Zhipeng Zhang, Baao Xie, Xin Jin, Yunbo Wang, Shiyu Wang, Liaomo Zheng, Xiaokang Yang, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08751">Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2412.13772.pdf' target='_blank'>https://arxiv.org/pdf/2412.13772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Zhang, Ying Xue, Xu Yan, Jiacheng Zhang, Weichao Qiu, Dongfeng Bai, Bingbing Liu, Shuguang Cui, Zhen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13772">An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of autonomous driving is experiencing a surge of interest in world models, which aim to predict potential future scenarios based on historical observations. In this paper, we introduce DFIT-OccWorld, an efficient 3D occupancy world model that leverages decoupled dynamic flow and image-assisted training strategy, substantially improving 4D scene forecasting performance. To simplify the training process, we discard the previous two-stage training strategy and innovatively reformulate the occupancy forecasting problem as a decoupled voxels warping process. Our model forecasts future dynamic voxels by warping existing observations using voxel flow, whereas static voxels are easily obtained through pose transformation. Moreover, our method incorporates an image-assisted training paradigm to enhance prediction reliability. Specifically, differentiable volume rendering is adopted to generate rendered depth maps through predicted future volumes, which are adopted in render-based photometric consistency. Experiments demonstrate the effectiveness of our approach, showcasing its state-of-the-art performance on the nuScenes and OpenScene benchmarks for 4D occupancy forecasting, end-to-end motion planning and point cloud forecasting. Concretely, it achieves state-of-the-art performances compared to existing 3D world models while incurring substantially lower computational costs.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2412.01407.pdf' target='_blank'>https://arxiv.org/pdf/2412.01407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehuan Wu, Jingcheng Ni, Xiaodong Wang, Yuxin Guo, Rui Chen, Lewei Lu, Jifeng Dai, Yuwen Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01407">HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have significantly improved the generation and prediction quality on either camera images or LiDAR point clouds for autonomous driving. However, a real-world autonomous driving system uses multiple kinds of input modality, usually cameras and LiDARs, where they contain complementary information for generation, while existing generation methods ignore this crucial feature, resulting in the generated results only covering separate 2D or 3D information. In order to fill the gap in 2D-3D multi-modal joint generation for autonomous driving, in this paper, we propose our framework, \emph{HoloDrive}, to jointly generate the camera images and LiDAR point clouds. We employ BEV-to-Camera and Camera-to-BEV transform modules between heterogeneous generative models, and introduce a depth prediction branch in the 2D generative model to disambiguate the un-projecting from image space to BEV space, then extend the method to predict the future by adding temporal structure and carefully designed progressive training. Further, we conduct experiments on single frame generation and world model benchmarks, and demonstrate our method leads to significant performance gains over SOTA methods in terms of generation metrics.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2510.10670.pdf' target='_blank'>https://arxiv.org/pdf/2510.10670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Li, Menghan Xia, Gongye Liu, Jianhong Bai, Xintao Wang, Conglang Zhang, Yuxuan Lin, Ruihang Chu, Pengfei Wan, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10670">AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2411.06559.pdf' target='_blank'>https://arxiv.org/pdf/2411.06559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06559">Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language agents based on large language models (LLMs) have demonstrated great promise in automating web-based tasks. Recent work has shown that incorporating advanced planning algorithms, e.g., tree search, is advantageous over reactive planning for web agents. However, unlike simulated sandbox environments, real-world environments such as the web are rife with irreversible actions. This undermines the feasibility of backtracking, a cornerstone of (tree) search. Overly relying on test-time search also hurts efficiency. We advocate model-based planning for web agents that employs a world model to simulate and deliberate over the outcome of each candidate action before committing to one. We systematically explore this paradigm by (1) Proposing a model-based planning framework, WebDreamer, which employs LLMs to serve as both world models and value functions; (2) Training specialized LLMs as world models with a scalable data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves substantial performance improvements over reactive baselines. It is competitive, while being 4-5 times more efficient, with tree search in sandbox environments (VisualWebArena) and also works effectively on real-world websites (Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model, Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of specialized world models for efficient and effective planning in complex web environments.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2307.12573.pdf' target='_blank'>https://arxiv.org/pdf/2307.12573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanzhi Liang, Linchao Zhu, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12573">Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2504.10157.pdf' target='_blank'>https://arxiv.org/pdf/2504.10157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinnong Zhang, Jiayu Lin, Xinyi Mou, Shiyue Yang, Xiawei Liu, Libo Sun, Hanjia Lyu, Yihang Yang, Weihong Qi, Yue Chen, Guanying Li, Ling Yan, Yao Hu, Siming Chen, Yu Wang, Xuanjing Huang, Jiebo Luo, Shiping Tang, Libo Wu, Baohua Zhou, Zhongyu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10157">SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2502.04728.pdf' target='_blank'>https://arxiv.org/pdf/2502.04728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04728">Generating Symbolic World Models via Test-time Scaling of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domains, achieving over 50\% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2508.17600.pdf' target='_blank'>https://arxiv.org/pdf/2508.17600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxing Lu, Baoxiong Jia, Puhao Li, Yixin Chen, Ziwei Wang, Yansong Tang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17600">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2508.17600.pdf' target='_blank'>https://arxiv.org/pdf/2508.17600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxing Lu, Baoxiong Jia, Puhao Li, Yixin Chen, Ziwei Wang, Yansong Tang, Siyuan Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17600">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2507.06710.pdf' target='_blank'>https://arxiv.org/pdf/2507.06710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyang Liu, Yikai Wang, Kuanning Wang, Longfei Liang, Xiangyang Xue, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06710">Spatial-Temporal Aware Visuomotor Diffusion Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual imitation learning is effective for robots to learn versatile tasks. However, many existing methods rely on behavior cloning with supervised historical trajectories, limiting their 3D spatial and 4D spatiotemporal awareness. Consequently, these methods struggle to capture the 3D structures and 4D spatiotemporal relationships necessary for real-world deployment. In this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation learning method that incorporates spatiotemporal awareness into diffusion-based policies. Unlike traditional approaches that rely on trajectory cloning, DP4 leverages a dynamic Gaussian world model to guide the learning of 3D spatial and 4D spatiotemporal perceptions from interactive environments. Our method constructs the current 3D scene from a single-view RGB-D observation and predicts the future 3D scene, optimizing trajectory generation by explicitly modeling both spatial and temporal dependencies. Extensive experiments across 17 simulation tasks with 173 variants and 3 real-world robotic tasks demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods, improving the average simulation task success rate by 16.4% (Adroit), 14% (DexArt), and 6.45% (RLBench), and the average real-world robotic task success rate by 8.6%.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2504.18904.pdf' target='_blank'>https://arxiv.org/pdf/2504.18904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, Yutong Liang, Dylan Goetting, Chaoyi Xu, Haozhe Chen, Yuxi Qian, Yiran Geng, Jiageng Mao, Weikang Wan, Mingtong Zhang, Jiangran Lyu, Siheng Zhao, Jiazhao Zhang, Jialiang Zhang, Chengyang Zhao, Haoran Lu, Yufei Ding, Ran Gong, Yuran Wang, Yuxuan Kuang, Ruihai Wu, Baoxiong Jia, Carlo Sferrazza, Hao Dong, Siyuan Huang, Yue Wang, Jitendra Malik, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18904">RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2404.05522.pdf' target='_blank'>https://arxiv.org/pdf/2404.05522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Zhou, Weidong Yang, Ben Fei, Jingyi Xu, Rui Zhang, Keyi Liu, Yeqi Luo, Ying He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05522">3DMambaIPF: A State Space Model for Iterative Point Cloud Filtering via Differentiable Rendering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Noise is an inevitable aspect of point cloud acquisition, necessitating filtering as a fundamental task within the realm of 3D vision. Existing learning-based filtering methods have shown promising capabilities on small-scale synthetic or real-world datasets. Nonetheless, the effectiveness of these methods is constrained when dealing with a substantial quantity of point clouds. This limitation primarily stems from their limited denoising capabilities for large-scale point clouds and their inclination to generate noisy outliers after denoising. The recent introduction of State Space Models (SSMs) for long sequence modeling in Natural Language Processing (NLP) presents a promising solution for handling large-scale data. Encouraged by iterative point cloud filtering methods, we introduce 3DMambaIPF, firstly incorporating Mamba (Selective SSM) architecture to sequentially handle extensive point clouds from large scenes, capitalizing on its strengths in selective input processing and long sequence modeling capabilities. Additionally, we integrate a robust and fast differentiable rendering loss to constrain the noisy points around the surface. In contrast to previous methodologies, this differentiable rendering loss enhances the visual realism of denoised geometric structures and aligns point cloud boundaries more closely with those observed in real-world objects. Extensive evaluation on datasets comprising small-scale synthetic and real-world models (typically with up to 50K points) demonstrate that our method achieves state-of-the-art results. Moreover, we showcase the superior scalability and efficiency of our method on large-scale models with about 500K points, where the majority of the existing learning-based denoising methods are unable to handle.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2402.15819.pdf' target='_blank'>https://arxiv.org/pdf/2402.15819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Li, Ruichu Cai, Haiqin Huang, Sili Zhang, Yuguang Yan, Zhifeng Hao, Zhenghua Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15819">Debiased Model-based Interactive Recommendation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing model-based interactive recommendation systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying recommendation generation process with identification guarantees; for the second drawback, we devise a debiased contrastive policy, which coincides with the debiased contrastive learning and avoids sampling bias. Moreover, we demonstrate that the proposed method not only outperforms several latest interactive recommendation algorithms but also enjoys diverse recommendation performance.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2507.09462.pdf' target='_blank'>https://arxiv.org/pdf/2507.09462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoye Chai, Yuan Yuan, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09462">MobiWorld: World Models for Mobile Wireless Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate modeling and simulation of mobile networks are essential for enabling intelligent and cost-effective network optimization. In this paper, we propose MobiWorld, a generative world model designed to support high-fidelity and flexible environment simulation for mobile network planning and optimization. Unlike traditional predictive models constrained by limited generalization capabilities, MobiWorld exhibits strong universality by integrating heterogeneous data sources, including sensors, mobile devices, and base stations, as well as multimodal data types such as sequences and images. It is capable of generating both network element-level observations (e.g., traffic load, user distribution) and system-level performance indicators (e.g., throughput, energy consumption) to support a wide range of planning and optimization tasks. Built upon advanced diffusion models, MobiWorld offers powerful controllable generation capabilities by modeling the joint distribution between mobile network data and diverse conditional factors including spatio temporal contexts, user behaviors, and optimization policies. This enables accurate simulation of dynamic network states under varying policy configurations, providing optimization agents with precise environmental feedback and facilitating effective decision-making without relying on costly real-network interactions. We demonstrate the effectiveness of MobiWorld in a collaborative energy-saving scenario, where an agent uses observations and rewards generated by MobiWorld to optimize base station sleep and user offloading policies. Experimental results show that MobiWorld exhibits strong controllable generation performance and outperforms traditional methods in energy optimization.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2506.13138.pdf' target='_blank'>https://arxiv.org/pdf/2506.13138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamin Wang, Yichen Yao, Xiang Feng, Hang Wu, Yaming Wang, Qingqiu Huang, Yuexin Ma, Xinge Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13138">STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis. To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation. Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2506.13138.pdf' target='_blank'>https://arxiv.org/pdf/2506.13138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamin Wang, Yichen Yao, Xiang Feng, Hang Wu, Yaming Wang, Qingqiu Huang, Yuexin Ma, Xinge Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13138">STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis. To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation. Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2506.00320.pdf' target='_blank'>https://arxiv.org/pdf/2506.00320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00320">Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2506.00320.pdf' target='_blank'>https://arxiv.org/pdf/2506.00320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00320">Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2506.00320.pdf' target='_blank'>https://arxiv.org/pdf/2506.00320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00320">Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2506.00320.pdf' target='_blank'>https://arxiv.org/pdf/2506.00320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman Nath, Jianfeng Gao, Zhou Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00320">Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2504.16464.pdf' target='_blank'>https://arxiv.org/pdf/2504.16464.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Li, Xiaobao Wei, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16464">ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent advancements in robotic manipulation video synthesis have shown promise, significant challenges persist in ensuring effective instruction-following and achieving high visual quality. Recent methods, like RoboDreamer, utilize linguistic decomposition to divide instructions into separate lower-level primitives, conditioning the world model on these primitives to achieve compositional instruction-following. However, these separate primitives do not consider the relationships that exist between them. Furthermore, recent methods neglect valuable visual guidance, including depth and semantic guidance, both crucial for enhancing visual quality. This paper introduces ManipDreamer, an advanced world model based on the action tree and visual guidance. To better learn the relationships between instruction primitives, we represent the instruction as the action tree and assign embeddings to tree nodes, each instruction can acquire its embeddings by navigating through the action tree. The instruction embeddings can be used to guide the world model. To enhance visual quality, we combine depth and semantic guidance by introducing a visual guidance adapter compatible with the world model. This visual adapter enhances both the temporal and physical consistency of video generation. Based on the action tree and visual guidance, ManipDreamer significantly boosts the instruction-following ability and visual quality. Comprehensive evaluations on robotic manipulation benchmarks reveal that ManipDreamer achieves large improvements in video quality metrics in both seen and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from 0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks, compared to the recent RoboDreamer model. Additionally, our method increases the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on average.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2410.13232.pdf' target='_blank'>https://arxiv.org/pdf/2410.13232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13232">Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2407.05679.pdf' target='_blank'>https://arxiv.org/pdf/2407.05679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiaofan Li, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05679">BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2505.01729.pdf' target='_blank'>https://arxiv.org/pdf/2505.01729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bu Jin, Weize Li, Baihan Yang, Zhenxin Zhu, Junpeng Jiang, Huan-ang Gao, Haiyang Sun, Kun Zhan, Hengtong Hu, Xueyang Zhang, Peng Jia, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01729">PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2503.19913.pdf' target='_blank'>https://arxiv.org/pdf/2503.19913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingju Gao, Yike Pan, Huan-ang Gao, Zongzheng Zhang, Wenyi Li, Hao Dong, Hao Tang, Li Yi, Hao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19913">PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the model's understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2303.01212.pdf' target='_blank'>https://arxiv.org/pdf/2303.01212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Shi, Kun Jiang, Jiusi Li, Zelin Qian, Junze Wen, Mengmeng Yang, Ke Wang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01212">Grid-Centric Traffic Scenario Perception for Autonomous Driving: A Comprehensive Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grid-centric perception is a crucial field for mobile robot perception and navigation. Nonetheless, grid-centric perception is less prevalent than object-centric perception as autonomous vehicles need to accurately perceive highly dynamic, large-scale traffic scenarios and the complexity and computational costs of grid-centric perception are high. In recent years, the rapid development of deep learning techniques and hardware provides fresh insights into the evolution of grid-centric perception. The fundamental difference between grid-centric and object-centric pipeline lies in that grid-centric perception follows a geometry-first paradigm which is more robust to the open-world driving scenarios with endless long-tailed semantically-unknown obstacles. Recent researches demonstrate the great advantages of grid-centric perception, such as comprehensive fine-grained environmental representation, greater robustness to occlusion and irregular shaped objects, better ground estimation, and safer planning policies. There is also a growing trend that the capacity of occupancy networks are greatly expanded to 4D scene perception and prediction and latest techniques are highly related to new research topics such as 4D occupancy forecasting, generative AI and world models in the field of autonomous driving. Given the lack of current surveys for this rapidly expanding field, we present a hierarchically-structured review of grid-centric perception for autonomous vehicles. We organize previous and current knowledge of occupancy grid techniques along the main vein from 2D BEV grids to 3D occupancy to 4D occupancy forecasting. We additionally summarize label-efficient occupancy learning and the role of grid-centric perception in driving systems. Lastly, we present a summary of the current research trend and provide future outlooks.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2507.12821.pdf' target='_blank'>https://arxiv.org/pdf/2507.12821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lance Ying, Katherine M. Collins, Prafull Sharma, Cedric Colas, Kaiya Ivy Zhao, Adrian Weller, Zenna Tavares, Phillip Isola, Samuel J. Gershman, Jacob D. Andreas, Thomas L. Griffiths, Francois Chollet, Kelsey R. Allen, Joshua B. Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12821">Assessing Adaptive World Models in Machines with Novel Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on massive corpora of data, instead of the efficiency and efficacy in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this class of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2507.12547.pdf' target='_blank'>https://arxiv.org/pdf/2507.12547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lionel Wong, Katherine M. Collins, Lance Ying, Cedegao E. Zhang, Adrian Weller, Tobias Gerstenberg, Timothy O'Donnell, Alexander K. Lew, Jacob D. Andreas, Joshua B. Tenenbaum, Tyler Brooke-Wilson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12547">Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2509.00559.pdf' target='_blank'>https://arxiv.org/pdf/2509.00559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhui Zhou, Jiarui Liu, Akhila Yerukola, Hyunwoo Kim, Maarten Sap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00559">Social World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2509.00559.pdf' target='_blank'>https://arxiv.org/pdf/2509.00559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhui Zhou, Jiarui Liu, Akhila Yerukola, Hyunwoo Kim, Maarten Sap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00559">Social World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans intuitively navigate social interactions by simulating unspoken dynamics and reasoning about others' perspectives, even with limited information. In contrast, AI systems struggle to automatically structure and reason about these implicit social contexts. In this paper, we introduce a novel structured social world representation formalism (S3AP), designed to help AI systems reason more effectively about social dynamics. Following a POMDP-driven design, S3AP represents social interactions as structured tuples, such as state, observation, agent actions, and mental states, which can be automatically induced from free-form narratives or other inputs. We first show S3AP can help LLMs better understand social narratives across 5 social reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then induce social world models from these structured representations, demonstrating their ability to predict future social dynamics and improve agent decision-making, yielding up to +18% improvement on the SOTOPIA social interaction benchmark. Our findings highlight the promise of S3AP as a powerful, general-purpose representation for social world states, enabling the development of more socially-aware systems that better navigate social interactions.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2506.05284.pdf' target='_blank'>https://arxiv.org/pdf/2506.05284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, Gordon Wetzstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05284">Video World Models with Long-term Spatial Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2506.23068.pdf' target='_blank'>https://arxiv.org/pdf/2506.23068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Zhao, Haoxuan Li, Haifeng Zhang, Jun Wang, Francesco Faccio, JÃ¼rgen Schmidhuber, Mengyue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23068">Curious Causality-Seeking Agents Learn Meta Causal World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2506.16565.pdf' target='_blank'>https://arxiv.org/pdf/2506.16565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Chen, Jianglan Wei, Chenfeng Xu, Boyi Li, Masayoshi Tomizuka, Andrea Bajcsy, Ran Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16565">Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models enable robots to "imagine" future observations given current observations and planned actions, and have been increasingly adopted as generalized dynamics models to facilitate robot learning. Despite their promise, these models remain brittle when encountering novel visual distractors such as objects and background elements rarely seen during training. Specifically, novel distractors can corrupt action outcome predictions, causing downstream failures when robots rely on the world model imaginations for planning or action verification. In this work, we propose Reimagination with Observation Intervention (ReOI), a simple yet effective test-time strategy that enables world models to predict more reliable action outcomes in open-world scenarios where novel and unanticipated visual distractors are inevitable. Given the current robot observation, ReOI first detects visual distractors by identifying which elements of the scene degrade in physically implausible ways during world model prediction. Then, it modifies the current observation to remove these distractors and bring the observation closer to the training distribution. Finally, ReOI "reimagines" future outcomes with the modified observation and reintroduces the distractors post-hoc to preserve visual consistency for downstream planning and verification. We validate our approach on a suite of robotic manipulation tasks in the context of action verification, where the verifier needs to select desired action plans based on predictions from a world model. Our results show that ReOI is robust to both in-distribution and out-of-distribution visual distractors. Notably, it improves task success rates by up to 3x in the presence of novel distractors, significantly outperforming action verification that relies on world model predictions without imagination interventions.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2505.16394.pdf' target='_blank'>https://arxiv.org/pdf/2505.16394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16394">Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2503.00713.pdf' target='_blank'>https://arxiv.org/pdf/2503.00713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinqian Sun, Feifei Zhao, Mingyang Lv, Yi Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00713">Spiking World Model with Multi-Compartment Neurons for Model-based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-inspired spiking neural networks (SNNs) have garnered significant research attention in algorithm design and perception applications. However, their potential in the decision-making domain, particularly in model-based reinforcement learning, remains underexplored. The difficulty lies in the need for spiking neurons with long-term temporal memory capabilities, as well as network optimization that can integrate and learn information for accurate predictions. The dynamic dendritic information integration mechanism of biological neurons brings us valuable insights for addressing these challenges. In this study, we propose a multi-compartment neuron model capable of nonlinearly integrating information from multiple dendritic sources to dynamically process long sequential inputs. Based on this model, we construct a Spiking World Model (Spiking-WM), to enable model-based deep reinforcement learning (DRL) with SNNs. We evaluated our model using the DeepMind Control Suite, demonstrating that Spiking-WM outperforms existing SNN-based models and achieves performance comparable to artificial neural network (ANN)-based world models employing Gated Recurrent Units (GRUs). Furthermore, we assess the long-term memory capabilities of the proposed model in speech datasets, including SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment neuron model surpasses other SNN-based architectures in processing long sequences. Our findings underscore the critical role of dendritic information integration in shaping neuronal function, emphasizing the importance of cooperative dendritic processing in enhancing neural computation.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2510.04020.pdf' target='_blank'>https://arxiv.org/pdf/2510.04020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04020">Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2510.04020.pdf' target='_blank'>https://arxiv.org/pdf/2510.04020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04020">Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2510.04020.pdf' target='_blank'>https://arxiv.org/pdf/2510.04020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04020">Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2509.21592.pdf' target='_blank'>https://arxiv.org/pdf/2509.21592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabrijel Boduljak, Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21592">What Happens Next? Anticipating Future Motion by Generating Point Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. We extensively evaluate our method on simulated data, demonstrate its effectiveness on downstream applications such as robotics, and show promising accuracy on real-world intuitive physics datasets. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2509.21592.pdf' target='_blank'>https://arxiv.org/pdf/2509.21592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabrijel Boduljak, Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21592">What Happens Next? Anticipating Future Motion by Generating Point Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. We extensively evaluate our method on simulated data, demonstrate its effectiveness on downstream applications such as robotics, and show promising accuracy on real-world intuitive physics datasets. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2506.09981.pdf' target='_blank'>https://arxiv.org/pdf/2506.09981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazhi Yang, Kashyap Chitta, Shenyuan Gao, Long Chen, Yuqian Shao, Xiaosong Jia, Hongyang Li, Andreas Geiger, Xiangyu Yue, Li Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09981">ReSim: Reliable World Simulation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring a diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates a reward from ReSim's simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2505.23561.pdf' target='_blank'>https://arxiv.org/pdf/2505.23561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghui Yuan, Yangming Xu, Jiawen Shi, Pan Zhou, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23561">Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model merging for Large Language Models (LLMs) directly fuses the parameters of different models finetuned on various tasks, creating a unified model for multi-domain tasks. However, due to potential vulnerabilities in models available on open-source platforms, model merging is susceptible to backdoor attacks. In this paper, we propose Merge Hijacking, the first backdoor attack targeting model merging in LLMs. The attacker constructs a malicious upload model and releases it. Once a victim user merges it with any other models, the resulting merged model inherits the backdoor while maintaining utility across tasks. Merge Hijacking defines two main objectives-effectiveness and utility-and achieves them through four steps. Extensive experiments demonstrate the effectiveness of our attack across different models, merging algorithms, and tasks. Additionally, we show that the attack remains effective even when merging real-world models. Moreover, our attack demonstrates robustness against two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time defense (Fine-pruning).
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2411.19548.pdf' target='_blank'>https://arxiv.org/pdf/2411.19548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, Wenjun Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19548">ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Closed-loop simulation is crucial for end-to-end autonomous driving. Existing sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes based on conditions that closely mirror training data distributions. However, these methods struggle with rendering novel trajectories, such as lane changes. Recent works have demonstrated that integrating world model knowledge alleviates these issues. Despite their efficiency, these approaches still encounter difficulties in the accurate representation of more complex maneuvers, with multi-lane shifts being a notable example. Therefore, we introduce ReconDreamer, which enhances driving scene reconstruction through incremental integration of world model knowledge. Specifically, DriveRestorer is proposed to mitigate artifacts via online restoration. This is complemented by a progressive data update strategy designed to ensure high-quality rendering for more complex maneuvers. To the best of our knowledge, ReconDreamer is the first method to effectively render in large maneuvers. Experimental results demonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU, NTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%. Furthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large maneuver rendering, as verified by a relative improvement of 195.87% in the NTA-IoU metric and a comprehensive user study.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2410.13571.pdf' target='_blank'>https://arxiv.org/pdf/2410.13571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13571">DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which are largely confined to forward-driving scenarios. Consequently, these methods face limitations when rendering complex maneuvers (e.g., lane change, acceleration, deceleration). Recent advancements in autonomous-driving world models have demonstrated the potential to generate diverse driving videos. However, these approaches remain constrained to 2D video generation, inherently lacking the spatiotemporal coherence required to capture intricacies of dynamic driving environments. In this paper, we introduce DriveDreamer4D, which enhances 4D driving scene representation leveraging world model priors. Specifically, we utilize the world model as a data machine to synthesize novel trajectory videos, where structured conditions are explicitly leveraged to control the spatial-temporal consistency of traffic elements. Besides, the cousin data training strategy is proposed to facilitate merging real and synthetic data for optimizing 4DGS. To our knowledge, DriveDreamer4D is the first to utilize video generation models for improving 4D reconstruction in driving scenarios. Experimental results reveal that DriveDreamer4D significantly enhances generation quality under novel trajectory views, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3% compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D markedly enhances the spatiotemporal coherence of driving agents, which is verified by a comprehensive user study and the relative increases of 22.6%, 43.5%, and 15.6% in the NTA-IoU metric.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2405.03272.pdf' target='_blank'>https://arxiv.org/pdf/2405.03272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03272">WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal information, together with our knowledge, help us to understand the complex and dynamic world. Large language models (LLM) and large multimodal models (LMM), however, still struggle to emulate this capability. In this paper, we present WorldQA, a video understanding dataset designed to push the boundaries of multimodal world models with three appealing properties: (1) Multimodal Inputs: The dataset comprises 1007 question-answer pairs and 303 videos, necessitating the analysis of both auditory and visual data for successful interpretation. (2) World Knowledge: We identify five essential types of world knowledge for question formulation. This approach challenges models to extend their capabilities beyond mere perception. (3) Long-Chain Reasoning: Our dataset introduces an average reasoning step of 4.45, notably surpassing other videoQA datasets. Furthermore, we introduce WorldRetriever, an agent designed to synthesize expert knowledge into a coherent reasoning chain, thereby facilitating accurate responses to WorldQA queries. Extensive evaluations of 13 prominent LLMs and LMMs reveal that WorldRetriever, although being the most effective model, achieved only 70% of humanlevel performance in multiple-choice questions. This finding highlights the necessity for further advancement in the reasoning and comprehension abilities of models. Our experiments also yield several key insights. For instance, while humans tend to perform better with increased frames, current LMMs, including WorldRetriever, show diminished performance under similar conditions. We hope that WorldQA,our methodology, and these insights could contribute to the future development of multimodal world models.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2403.06845.pdf' target='_blank'>https://arxiv.org/pdf/2403.06845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06845">DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos. Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2401.14159.pdf' target='_blank'>https://arxiv.org/pdf/2401.14159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14159">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2401.09985.pdf' target='_blank'>https://arxiv.org/pdf/2401.09985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09985">WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2310.09615.pdf' target='_blank'>https://arxiv.org/pdf/2310.09615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09615">STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2309.09777.pdf' target='_blank'>https://arxiv.org/pdf/2309.09777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09777">DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models, especially in autonomous driving, are trending and drawing extensive attention due to their capacity for comprehending driving environments. The established world model holds immense potential for the generation of high-quality driving videos, and driving policies for safe maneuvering. However, a critical limitation in relevant research lies in its predominant focus on gaming environments or simulated settings, thereby lacking the representation of real-world driving scenarios. Therefore, we introduce DriveDreamer, a pioneering world model entirely derived from real-world driving scenarios. Regarding that modeling the world in intricate driving scenes entails an overwhelming search space, we propose harnessing the powerful diffusion model to construct a comprehensive representation of the complex environment. Furthermore, we introduce a two-stage training pipeline. In the initial phase, DriveDreamer acquires a deep understanding of structured traffic constraints, while the subsequent stage equips it with the ability to anticipate future states. The proposed DriveDreamer is the first world model established from real-world driving scenarios. We instantiate DriveDreamer on the challenging nuScenes benchmark, and extensive experiments verify that DriveDreamer empowers precise, controllable video generation that faithfully captures the structural constraints of real-world traffic scenarios. Additionally, DriveDreamer enables the generation of realistic and reasonable driving policies, opening avenues for interaction and practical applications.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2505.16723.pdf' target='_blank'>https://arxiv.org/pdf/2505.16723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Robin Staab, Nikola JovanoviÄ, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16723">Robust LLM Fingerprinting via Domain-Specific Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As open-source language models (OSMs) grow more capable and are widely shared and finetuned, ensuring model provenance, i.e., identifying the origin of a given model instance, has become an increasingly important issue. At the same time, existing backdoor-based model fingerprinting techniques often fall short of achieving key requirements of real-world model ownership detection. In this work, we build on the observation that while current open-source model watermarks fail to achieve reliable content traceability, they can be effectively adapted to address the challenge of model provenance. To this end, we introduce the concept of domain-specific watermarking for model fingerprinting. Rather than watermarking all generated content, we train the model to embed watermarks only within specified subdomains (e.g., particular languages or topics). This targeted approach ensures detection reliability, while improving watermark durability and quality under a range of real-world deployment settings. Our evaluations show that domain-specific watermarking enables model fingerprinting with strong statistical guarantees, controllable false positive rates, high detection power, and preserved generation quality. Moreover, we find that our fingerprints are inherently stealthy and naturally robust to real-world variability across deployment scenarios.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2501.00226.pdf' target='_blank'>https://arxiv.org/pdf/2501.00226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura, Masahiro Suzuki, Akira Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00226">Generative Emergent Communication: Large Language Model is a Collective World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making. To formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations. This perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2509.13903.pdf' target='_blank'>https://arxiv.org/pdf/2509.13903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13903">PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PhysicalAgent, an agentic framework for robotic manipulation that integrates iterative reasoning, diffusion-based video generation, and closed-loop execution. Given a textual instruction, our method generates short video demonstrations of candidate trajectories, executes them on the robot, and iteratively re-plans in response to failures. This approach enables robust recovery from execution errors. We evaluate PhysicalAgent across multiple perceptual modalities (egocentric, third-person, and simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing against state-of-the-art task-specific baselines. Experiments demonstrate that our method consistently outperforms prior approaches, achieving up to 83% success on human-familiar tasks. Physical trials reveal that first-attempt success is limited (20-30%), yet iterative correction increases overall success to 80% across platforms. These results highlight the potential of video-based generative reasoning for general-purpose robotic manipulation and underscore the importance of iterative execution for recovering from initial failures. Our framework paves the way for scalable, adaptable, and robust robot control.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2509.13903.pdf' target='_blank'>https://arxiv.org/pdf/2509.13903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13903">PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PhysicalAgent, an agentic framework for robotic manipulation that integrates iterative reasoning, diffusion-based video generation, and closed-loop execution. Given a textual instruction, our method generates short video demonstrations of candidate trajectories, executes them on the robot, and iteratively re-plans in response to failures. This approach enables robust recovery from execution errors. We evaluate PhysicalAgent across multiple perceptual modalities (egocentric, third-person, and simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing against state-of-the-art task-specific baselines. Experiments demonstrate that our method consistently outperforms prior approaches, achieving up to 83% success on human-familiar tasks. Physical trials reveal that first-attempt success is limited (20-30%), yet iterative correction increases overall success to 80% across platforms. These results highlight the potential of video-based generative reasoning for general-purpose robotic manipulation and underscore the importance of iterative execution for recovering from initial failures. Our framework paves the way for scalable, adaptable, and robust robot control.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2509.11959.pdf' target='_blank'>https://arxiv.org/pdf/2509.11959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11959">Learning to Generate 4D LiDAR Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.11959.pdf' target='_blank'>https://arxiv.org/pdf/2509.11959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11959">Learning to Generate 4D LiDAR Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2508.03692.pdf' target='_blank'>https://arxiv.org/pdf/2508.03692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03692">LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2508.03692.pdf' target='_blank'>https://arxiv.org/pdf/2508.03692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03692">LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2503.10480.pdf' target='_blank'>https://arxiv.org/pdf/2503.10480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10480">World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D$^2$PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D$^2$PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2501.06605.pdf' target='_blank'>https://arxiv.org/pdf/2501.06605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Chen, Jing Huo, Yangtao Chen, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06605">RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient control in long-horizon robotic manipulation is challenging due to complex representation and policy learning requirements. Model-based visual reinforcement learning (RL) has shown great potential in addressing these challenges but still faces notable limitations, particularly in handling sparse rewards and complex visual features in long-horizon environments. To address these limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for long-horizon tasks and further introduce RoboHorizon, an LLM-assisted multi-view world model tailored for long-horizon robotic manipulation. In RoboHorizon, pre-trained LLMs generate dense reward structures for multi-stage sub-tasks based on task language instructions, enabling robots to better recognize long-horizon tasks. Keyframe discovery is then integrated into the multi-view masked autoencoder (MAE) architecture to enhance the robot's ability to sense critical task sequences, strengthening its multi-stage perception of long-horizon processes. Leveraging these dense rewards and multi-view representations, a robotic world model is constructed to efficiently plan long-horizon tasks, enabling the robot to reliably act through RL algorithms. Experiments on two representative benchmarks, RLBench and FurnitureBench, show that RoboHorizon outperforms state-of-the-art visual model-based RL methods, achieving a 23.35% improvement in task success rates on RLBench's 4 short-horizon tasks and a 29.23% improvement on 6 long-horizon tasks from RLBench and 3 furniture assembly tasks from FurnitureBench.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2306.06561.pdf' target='_blank'>https://arxiv.org/pdf/2306.06561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Ren Liu, Biwei Huang, Zhengmao Zhu, Honglong Tian, Mingming Gong, Yang Yu, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06561">Learning World Models with Identifiable Factorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments -- how to effectively extract and disentangle these information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundants but retaining minimal and sufficient information for policy optimization. Experiments in synthetic worlds demonstrate that our method accurately identifies the ground-truth latent variables, substantiating our theoretical findings. Moreover, experiments in variants of the DeepMind Control Suite and RoboDesk showcase the superior performance of our approach over baselines.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2510.00855.pdf' target='_blank'>https://arxiv.org/pdf/2510.00855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00855">Can World Models Benefit VLMs for World Dynamics?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2510.00855.pdf' target='_blank'>https://arxiv.org/pdf/2510.00855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Zhang, Kuangzhi Ge, Xiaowei Chi, Renrui Zhang, Shaojun Shi, Zhen Dong, Sirui Han, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00855">Can World Models Benefit VLMs for World Dynamics?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2504.13643.pdf' target='_blank'>https://arxiv.org/pdf/2504.13643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao He, Lizi Liao, Ming Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13643">Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in dialogue policy planning have emphasized optimizing system agent policies to achieve predefined goals, focusing on strategy design, trajectory acquisition, and efficient training paradigms. However, these approaches often overlook the critical role of user characteristics, which are essential in real-world scenarios like conversational search and recommendation, where interactions must adapt to individual user traits such as personality, preferences, and goals. To address this gap, we first conduct a comprehensive study utilizing task-specific user personas to systematically assess dialogue policy planning under diverse user behaviors. By leveraging realistic user profiles for different tasks, our study reveals significant limitations in existing approaches, highlighting the need for user-tailored dialogue policy planning. Building on this foundation, we present the User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an Intrinsic User World Model to model user traits and feedback. UDP operates in three stages: (1) User Persona Portraying, using a diffusion model to dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a Brownian Bridge-inspired anticipator to predict user reactions; and (3) User-Tailored Policy Planning, integrating these insights to optimize response strategies. To ensure robust performance, we further propose an active learning approach that prioritizes challenging user personas during training. Comprehensive experiments on benchmarks, including collaborative and non-collaborative settings, demonstrate the effectiveness of UDP in learning user-specific dialogue strategies. Results validate the protocol's utility and highlight UDP's robustness, adaptability, and potential to advance user-centric dialogue systems.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2501.00296.pdf' target='_blank'>https://arxiv.org/pdf/2501.00296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashay Athalye, Nishanth Kumar, Tom Silver, Yichao Liang, Jiuguang Wang, TomÃ¡s Lozano-PÃ©rez, Leslie Pack Kaelbling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00296">From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our aim is to learn to solve long-horizon decision-making problems in complex robotics domains given low-level skills and a handful of short-horizon demonstrations containing sequences of images. To this end, we focus on learning abstract symbolic world models that facilitate zero-shot generalization to novel goals via planning. A critical component of such models is the set of symbolic predicates that define properties of and relationships between objects. In this work, we leverage pretrained vision language models (VLMs) to propose a large set of visual predicates potentially relevant for decision-making, and to evaluate those predicates directly from camera images. At training time, we pass the proposed predicates and demonstrations into an optimization-based model-learning algorithm to obtain an abstract symbolic world model that is defined in terms of a compact subset of the proposed predicates. At test time, given a novel goal in a novel setting, we use the VLM to construct a symbolic description of the current world state, and then use a search-based planning algorithm to find a sequence of low-level skills that achieves the goal. We demonstrate empirically across experiments in both simulation and the real world that our method can generalize aggressively, applying its learned world model to solve problems with a wide variety of object types, arrangements, numbers of objects, and visual backgrounds, as well as novel goals and much longer horizons than those seen at training time.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2509.15536.pdf' target='_blank'>https://arxiv.org/pdf/2509.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15536">SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2509.15536.pdf' target='_blank'>https://arxiv.org/pdf/2509.15536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, Jiayi Li, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Hua Gang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15536">SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2508.03645.pdf' target='_blank'>https://arxiv.org/pdf/2508.03645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshay L Chandra, Iman Nematollahi, Chenguang Huang, Tim Welschehold, Wolfram Burgard, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03645">DiWA: Diffusion Policy Adaptation with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Moreover, standard RL methods require millions of real-world interactions, posing a major bottleneck for practical fine-tuning. Although prior work frames the denoising process in diffusion policies as a Markov Decision Process to enable RL-based updates, its strong dependence on environment interaction remains highly inefficient. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at https://diwa.cs.uni-freiburg.de.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2507.12762.pdf' target='_blank'>https://arxiv.org/pdf/2507.12762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanchen Guan, Haicheng Liao, Chengyue Wang, Xingcheng Liu, Jiaxun Zhang, Zhenning Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12762">World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable anticipation of traffic accidents is essential for advancing autonomous driving systems. However, this objective is limited by two fundamental challenges: the scarcity of diverse, high-quality training data and the frequent absence of crucial object-level cues due to environmental disruptions or sensor deficiencies. To tackle these issues, we propose a comprehensive framework combining generative scene augmentation with adaptive temporal reasoning. Specifically, we develop a video generation pipeline that utilizes a world model guided by domain-informed prompts to create high-resolution, statistically consistent driving scenarios, particularly enriching the coverage of edge cases and complex interactions. In parallel, we construct a dynamic prediction model that encodes spatio-temporal relationships through strengthened graph convolutions and dilated temporal operators, effectively addressing data incompleteness and transient visual noise. Furthermore, we release a new benchmark dataset designed to better capture diverse real-world driving risks. Extensive experiments on public and newly released datasets confirm that our framework enhances both the accuracy and lead time of accident anticipation, offering a robust solution to current data and modeling limitations in safety-critical autonomous driving applications.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2506.02327.pdf' target='_blank'>https://arxiv.org/pdf/2506.02327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02327">Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2411.08794.pdf' target='_blank'>https://arxiv.org/pdf/2411.08794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Yang, Xinrun Wang, Junzhe Jiang, Qinggang Zhang, Xiao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08794">Evaluating World Models with LLM for Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World model emerges as a key module in decision making, where MuZero and Dreamer achieve remarkable successes in complex tasks. Recent work leverages Large Language Models (LLMs) as general world simulators to simulate the dynamics of the world due to their generalizability. LLMs also serve as the world model for deliberative reasoning in Reasoning via Planning (RAP) and Tree of Thought (ToT). However, the world models are either evaluated as a general world simulator, or as a functional module of the agent, i.e., predicting the transitions to assist the planning. In this work, we propose a comprehensive evaluation of the world models with LLMs from the decision making perspective. Specifically, we leverage the 31 diverse environments from (Wang et al., 2023;2024) and curate the rule-based policy of each environment for the diverse evaluation. Then, we design three main tasks, i.e., policy verification, action proposal, and policy planning, where the world models can be used for decision making solely. Finally, we conduct the comprehensive evaluation of the advanced LLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main tasks under various settings. The key observations include: i) GPT-4o significantly outperforms GPT-4o-mini on the three main tasks, especially for the tasks which require the domain knowledge, ii) the performance of the world model with LLM will be decreased for long-term decision-making tasks, and iii) the combination of different functionalities of the world model will brings additional unstabilities of the performance.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2411.02385.pdf' target='_blank'>https://arxiv.org/pdf/2411.02385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, Jiashi Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02385">How Far is Video Generation from World Model: A Physical Law Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2403.02622.pdf' target='_blank'>https://arxiv.org/pdf/2403.02622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanchen Guan, Haicheng Liao, Zhenning Li, Jia Hu, Runze Yuan, Yunjian Li, Guohui Zhang, Chengzhong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02622">World Models for Autonomous Driving: An Initial Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and comprehension of this burgeoning field, and inspiring continued innovation and exploration.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2210.04017.pdf' target='_blank'>https://arxiv.org/pdf/2210.04017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Gao, Yao Mu, Chen Chen, Jingliang Duan, Shengbo Eben Li, Ping Luo, Yanfeng Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.04017">Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving provides a feasible way to automatically maximize overall driving system performance by directly mapping the raw pixels from a front-facing camera to control signals. Recent advanced methods construct a latent world model to map the high dimensional observations into compact latent space. However, the latent states embedded by the world model proposed in previous works may contain a large amount of task-irrelevant information, resulting in low sampling efficiency and poor robustness to input perturbations. Meanwhile, the training data distribution is usually unbalanced, and the learned policy is challenging to cope with the corner cases during the driving process. To solve the above challenges, we present a SEMantic Masked recurrent world model (SEM2), which introduces a semantic filter to extract key driving-relevant features and make decisions via the filtered features, and is trained with a multi-source data sampler, which aggregates common data and multiple corner case data in a single batch, to balance the data distribution. Extensive experiments on CARLA show our method outperforms the state-of-the-art approaches in terms of sample efficiency and robustness to input permutations.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2506.01442.pdf' target='_blank'>https://arxiv.org/pdf/2506.01442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xidong Yang, Wenhao Li, Junjie Sheng, Chuyun Shen, Yun Hua, Xiangfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01442">Agentic Episodic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data efficiency and poor generalizability. Recent advances suggest that large language models, with their rich world knowledge and reasoning capabilities, could complement RL by enabling semantic state modeling and task-agnostic planning. In this work, we propose the Agentic Episodic Control (AEC), a novel architecture that integrates RL with LLMs to enhance decision-making. The AEC can leverage a large language model (LLM) to map the observations into language-grounded embeddings, which further can be stored in an episodic memory for rapid retrieval of high-value experiences. Simultaneously, a World-Graph working memory module is utilized to capture structured environmental dynamics in order to enhance relational reasoning. Furthermore, a lightweight critical state detector dynamically arbitrates between the episodic memory recall and the world-model-guided exploration. On the whole, by combining the trial-and-error learning scheme with LLM-derived semantic priors, the proposed AEC can improve both data efficiency and generalizability in reinforcement learning. In experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial improvements over existing baselines, especially on complex and generalization tasks like FindObj, where it outperforms the best baseline by up to 76%. The proposed AEC framework bridges the strengths of numeric reinforcement learning and symbolic reasoning, which provides a pathway toward more adaptable and sample-efficient agents.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2503.05696.pdf' target='_blank'>https://arxiv.org/pdf/2503.05696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjie Liu, Cyrus Neary, Kushagra Gupta, Christian Ellis, Ufuk Topcu, David Fridovich-Keil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05696">Multi-Fidelity Policy Gradient Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many reinforcement learning (RL) algorithms require large amounts of data, prohibiting their use in applications where frequent interactions with operational systems are infeasible, or high-fidelity simulations are expensive or unavailable. Meanwhile, low-fidelity simulators--such as reduced-order models, heuristic reward functions, or generative world models--can cheaply provide useful data for RL training, even if they are too coarse for direct sim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL framework that mixes a small amount of data from the target environment with a large volume of low-fidelity simulation data to form unbiased, reduced-variance estimators (control variates) for on-policy policy gradients. We instantiate the framework by developing multi-fidelity variants of two policy gradient algorithms: REINFORCE and proximal policy optimization. Experimental results across a suite of simulated robotics benchmark problems demonstrate that when target-environment samples are limited, MFPG achieves up to 3.9x higher reward and improves training stability when compared to baselines that only use high-fidelity data. Moreover, even when the baselines are given more high-fidelity samples--up to 10x as many interactions with the target environment--MFPG continues to match or outperform them. Finally, we observe that MFPG is capable of training effective policies even when the low-fidelity environment is drastically different from the target environment. MFPG thus not only offers a novel paradigm for efficient sim-to-real transfer but also provides a principled approach to managing the trade-off between policy performance and data collection costs.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2503.05696.pdf' target='_blank'>https://arxiv.org/pdf/2503.05696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjie Liu, Cyrus Neary, Kushagra Gupta, Wesley A. Suttle, Christian Ellis, Ufuk Topcu, David Fridovich-Keil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05696">A Multi-Fidelity Control Variate Approach for Policy Gradient Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many reinforcement learning (RL) algorithms are impractical for deployment in operational systems or for training with computationally expensive high-fidelity simulations, as they require large amounts of data. Meanwhile, low-fidelity simulators -- such as reduced-order models, heuristic rewards, or generative world models -- can cheaply provide useful data for RL training, even if they are too coarse for zero-shot transfer. We propose multi-fidelity policy gradients (MFPGs), an RL framework that mixes a small amount of data from the target environment with a control variate formed from a large volume of low-fidelity simulation data to construct an unbiased, variance-reduced estimator for on-policy policy gradients. We instantiate the framework with a multi-fidelity variant of the classical REINFORCE algorithm. We show that under standard assumptions, the MFPG estimator guarantees asymptotic convergence of REINFORCE to locally optimal policies in the target environment, and achieves faster finite-sample convergence rates compared to training with high-fidelity data alone. Empirically, we evaluate the MFPG algorithm across a suite of simulated robotics benchmark tasks with limited high-fidelity data but abundant off-dynamics, low-fidelity data. With mild-moderate dynamics gaps, MFPG reliably improves the median performance over a high-fidelity-only baseline, matching the performance of leading multi-fidelity baselines despite its simplicity and minimal tuning overhead. Under large dynamics gaps, MFPG demonstrates the strongest robustness among the evaluated multi-fidelity approaches. An additional experiment shows that MFPG can remain effective even under low-fidelity reward misspecification. Thus, MFPG not only offers a novel paradigm for efficient sim-to-real transfer but also provides a principled approach to managing the trade-off between policy performance and data collection costs.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2503.05696.pdf' target='_blank'>https://arxiv.org/pdf/2503.05696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinjie Liu, Cyrus Neary, Kushagra Gupta, Wesley A. Suttle, Christian Ellis, Ufuk Topcu, David Fridovich-Keil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05696">A Multi-Fidelity Control Variate Approach for Policy Gradient Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many reinforcement learning (RL) algorithms are impractical for deployment in operational systems or for training with computationally expensive high-fidelity simulations, as they require large amounts of data. Meanwhile, low-fidelity simulators -- such as reduced-order models, heuristic rewards, or generative world models -- can cheaply provide useful data for RL training, even if they are too coarse for zero-shot transfer. We propose multi-fidelity policy gradients (MFPGs), an RL framework that mixes a small amount of data from the target environment with a control variate formed from a large volume of low-fidelity simulation data to construct an unbiased, variance-reduced estimator for on-policy policy gradients. We instantiate the framework with a multi-fidelity variant of the classical REINFORCE algorithm. We show that under standard assumptions, the MFPG estimator guarantees asymptotic convergence of REINFORCE to locally optimal policies in the target environment, and achieves faster finite-sample convergence rates compared to training with high-fidelity data alone. Empirically, we evaluate the MFPG algorithm across a suite of simulated robotics benchmark tasks with limited high-fidelity data but abundant off-dynamics, low-fidelity data. With mild-moderate dynamics gaps, MFPG reliably improves the median performance over a high-fidelity-only baseline, matching the performance of leading multi-fidelity baselines despite its simplicity and minimal tuning overhead. Under large dynamics gaps, MFPG demonstrates the strongest robustness among the evaluated multi-fidelity approaches. An additional experiment shows that MFPG can remain effective even under low-fidelity reward misspecification. Thus, MFPG not only offers a novel paradigm for efficient sim-to-real transfer but also provides a principled approach to managing the trade-off between policy performance and data collection costs.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2412.18607.pdf' target='_blank'>https://arxiv.org/pdf/2412.18607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Chen, Yuqi Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.18607">DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World model-based searching and planning are widely recognized as a promising path toward human-level physical intelligence. However, current driving world models primarily rely on video diffusion models, which specialize in visual generation but lack the flexibility to incorporate other modalities like action. In contrast, autoregressive transformers have demonstrated exceptional capability in modeling multimodal data. Our work aims to unify both driving model simulation and trajectory planning into a single sequence modeling problem. We introduce a multimodal driving language based on interleaved image and action tokens, and develop DrivingGPT to learn joint world modeling and planning through standard next-token prediction. Our DrivingGPT demonstrates strong performance in both action-conditioned video generation and end-to-end planning, outperforming strong baselines on large-scale nuPlan and NAVSIM benchmarks.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2410.10738.pdf' target='_blank'>https://arxiv.org/pdf/2410.10738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Wang, Ke Cheng, Jiawei He, Qitai Wang, Hengchen Dai, Yuntao Chen, Fei Xia, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10738">DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2405.17209.pdf' target='_blank'>https://arxiv.org/pdf/2405.17209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhash Kantamneni, Ziming Liu, Max Tegmark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17209">How Do Transformers "Do" Physics? Investigating the Simple Harmonic Oscillator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do transformers model physics? Do transformers model systems with interpretable analytical solutions, or do they create "alien physics" that are difficult for humans to decipher? We take a step in demystifying this larger puzzle by investigating the simple harmonic oscillator (SHO), $\ddot{x}+2Î³\dot{x}+Ï_0^2x=0$, one of the most fundamental systems in physics. Our goal is to identify the methods transformers use to model the SHO, and to do so we hypothesize and evaluate possible methods by analyzing the encoding of these methods' intermediates. We develop four criteria for the use of a method within the simple testbed of linear regression, where our method is $y = wx$ and our intermediate is $w$: (1) Can the intermediate be predicted from hidden states? (2) Is the intermediate's encoding quality correlated with model performance? (3) Can the majority of variance in hidden states be explained by the intermediate? (4) Can we intervene on hidden states to produce predictable outcomes? Armed with these two correlational (1,2), weak causal (3) and strong causal (4) criteria, we determine that transformers use known numerical methods to model trajectories of the simple harmonic oscillator, specifically the matrix exponential method. Our analysis framework can conveniently extend to high-dimensional linear systems and nonlinear systems, which we hope will help reveal the "world model" hidden in transformers.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2311.13549.pdf' target='_blank'>https://arxiv.org/pdf/2311.13549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, Tiancai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13549">ADriver-I: A General World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Typically, autonomous driving adopts a modular design, which divides the full stack into perception, prediction, planning and control parts. Though interpretable, such modular design tends to introduce a substantial amount of redundancy. Recently, multimodal large language models (MLLM) and diffusion techniques have demonstrated their superior performance on comprehension and generation ability. In this paper, we first introduce the concept of interleaved vision-action pair, which unifies the format of visual features and control signals. Based on the vision-action pairs, we construct a general world model based on MLLM and diffusion model for autonomous driving, termed ADriver-I. It takes the vision-action pairs as inputs and autoregressively predicts the control signal of the current frame. The generated control signals together with the historical vision-action pairs are further conditioned to predict the future frames. With the predicted next frame, ADriver-I performs further control signal prediction. Such a process can be repeated infinite times, ADriver-I achieves autonomous driving in the world created by itself. Extensive experiments are conducted on nuScenes and our large-scale private datasets. ADriver-I shows impressive performance compared to several constructed baselines. We hope our ADriver-I can provide some new insights for future autonomous driving and embodied intelligence.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2508.11428.pdf' target='_blank'>https://arxiv.org/pdf/2508.11428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Li, Bozhou Zhang, Xin Jin, Jiankang Deng, Xiatian Zhu, Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11428">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving requires rich contextual comprehension and precise predictive reasoning to navigate dynamic and complex environments safely. Vision-Language Models (VLMs) and Driving World Models (DWMs) have independently emerged as powerful recipes addressing different aspects of this challenge. VLMs provide interpretability and robust action prediction through their ability to understand multi-modal context, while DWMs excel in generating detailed and plausible future driving scenarios essential for proactive planning. Integrating VLMs with DWMs is an intuitive, promising, yet understudied strategy to exploit the complementary strengths of accurate behavioral prediction and realistic scene generation. Nevertheless, this integration presents notable challenges, particularly in effectively connecting action-level decisions with high-fidelity pixel-level predictions and maintaining computational efficiency. In this paper, we propose ImagiDrive, a novel end-to-end autonomous driving framework that integrates a VLM-based driving agent with a DWM-based scene imaginer to form a unified imagination-and-planning loop. The driving agent predicts initial driving trajectories based on multi-modal inputs, guiding the scene imaginer to generate corresponding future scenarios. These imagined scenarios are subsequently utilized to iteratively refine the driving agent's planning decisions. To address efficiency and predictive accuracy challenges inherent in this integration, we introduce an early stopping mechanism and a trajectory selection strategy. Extensive experimental validation on the nuScenes and NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over previous alternatives under both open-loop and closed-loop conditions.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2503.09215.pdf' target='_blank'>https://arxiv.org/pdf/2503.09215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Zhu, Zhengyu Jia, Tian Gao, Jiaxin Deng, Shidi Li, Lang Zhang, Fu Liu, Peng Jia, Xianpeng Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09215">Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In this paper, we propose a driving World Model named EOT-WM, unifying Ego-Other vehicle Trajectories in videos for driving simulation. Specifically, it remains a challenge to match multiple trajectories in the BEV space with each vehicle in the video to control the video generation. We first project ego-other vehicle trajectories in the BEV space into the image coordinate for vehicle-trajectory match via pixel positions. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30% in FID and 55% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2408.14197.pdf' target='_blank'>https://arxiv.org/pdf/2408.14197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing Chen, Yijie Qian, Yuxiang Feng, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14197">Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models envision potential future states based on various ego actions. They embed extensive knowledge about the driving environment, facilitating safe and scalable autonomous driving. Most existing methods primarily focus on either data generation or the pretraining paradigms of world models. Unlike the aforementioned prior works, we propose Drive-OccWorld, which adapts a vision-centric 4D forecasting world model to end-to-end planning for autonomous driving. Specifically, we first introduce a semantic and motion-conditional normalization in the memory module, which accumulates semantic and dynamic information from historical BEV embeddings. These BEV features are then conveyed to the world decoder for future occupancy and flow forecasting, considering both geometry and spatiotemporal modeling. Additionally, we propose injecting flexible action conditions, such as velocity, steering angle, trajectory, and commands, into the world model to enable controllable generation and facilitate a broader range of downstream applications. Furthermore, we explore integrating the generative capabilities of the 4D world model with end-to-end planning, enabling continuous forecasting of future states and the selection of optimal trajectories using an occupancy-based cost function. Comprehensive experiments conducted on the nuScenes, nuScenes-Occupancy, and Lyft-Level5 datasets illustrate that our method can generate plausible and controllable 4D occupancy, paving the way for advancements in driving world generation and end-to-end planning. Project page: https://drive-occworld.github.io/
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2405.13193.pdf' target='_blank'>https://arxiv.org/pdf/2405.13193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Kolev, Rafael Rafailov, Kyle Hatch, Jiajun Wu, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13193">Efficient Imitation Learning with Conservative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of policy learning from expert demonstrations without a reward function. A central challenge in this space is that these policies fail upon deployment due to issues of distributional shift, environment stochasticity, or compounding errors. Adversarial imitation learning alleviates this issue but requires additional on-policy training samples for stability, which presents a challenge in realistic domains due to inefficient learning and high sample complexity. One approach to this issue is to learn a world model of the environment, and use synthetic data for policy training. While successful in prior works, we argue that this is sub-optimal due to additional distribution shifts between the learned model and the real environment. Instead, we re-frame imitation learning as a fine-tuning problem, rather than a pure reinforcement learning one. Drawing theoretical connections to offline RL and fine-tuning algorithms, we argue that standard online world model algorithms are not well suited to the imitation learning problem. We derive a principled conservative optimization bound and demonstrate empirically that it leads to improved performance on two very challenging manipulation environments from high-dimensional raw pixel observations. We set a new state-of-the-art performance on the Franka Kitchen environment from images, requiring only 10 demos on no reward labels, as well as solving a complex dexterity manipulation task.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2403.15698.pdf' target='_blank'>https://arxiv.org/pdf/2403.15698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengqi Zhou, Yuxi Wang, Jun Hou, Shougao Zhang, Yiwei Li, Chuanchen Luo, Junran Peng, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15698">SceneX: Procedural Controllable Large-scale Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing comprehensive explicit world models is crucial for understanding and simulating real-world scenarios. Recently, Procedural Controllable Generation (PCG) has gained significant attention in large-scale scene generation by enabling the creation of scalable, high-quality assets. However, PCG faces challenges such as limited modular diversity, high expertise requirements, and challenges in managing the diverse elements and structures in complex scenes. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual descriptions. Specifically, the proposed method comprises two components, PCGHub and PCGPlanner. The former encompasses an extensive collection of accessible procedural assets and thousands of hand-craft API documents to perform as a standard protocol for PCG controller. The latter aims to generate executable actions for Blender to produce controllable and precise 3D assets guided by the user's instructions. Extensive experiments demonstrated the capability of our method in controllable large-scale scene generation, including nature scenes and unbounded cities, as well as scene editing such as asset placement and season translation.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2510.11682.pdf' target='_blank'>https://arxiv.org/pdf/2510.11682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Liu, Yuman Gao, Sangli Teng, Yufeng Chi, Yakun Sophia Shao, Zhongyu Li, Maani Ghaffari, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11682">Ego-Vision World Model for Humanoid Contact Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling humanoid robots to exploit physical contact, rather than simply avoid collisions, is crucial for autonomy in unstructured environments. Traditional optimization-based planners struggle with contact complexity, while on-policy reinforcement learning (RL) is sample-inefficient and has limited multi-task ability. We propose a framework combining a learned world model with sampling-based Model Predictive Control (MPC), trained on a demonstration-free offline dataset to predict future outcomes in a compressed latent space. To address sparse contact rewards and sensor noise, the MPC uses a learned surrogate value function for dense, robust planning. Our single, scalable model supports contact-aware tasks, including wall support after perturbation, blocking incoming objects, and traversing height-limited arches, with improved data efficiency and multi-task capability over on-policy RL. Deployed on a physical humanoid, our system achieves robust, real-time contact planning from proprioception and ego-centric depth images. Website: https://ego-vcp.github.io/
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2308.04206.pdf' target='_blank'>https://arxiv.org/pdf/2308.04206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04206">Exploring Transformers for Open-world Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-world instance segmentation is a rising task, which aims to segment all objects in the image by learning from a limited number of base-category objects. This task is challenging, as the number of unseen categories could be hundreds of times larger than that of seen categories. Recently, the DETR-like models have been extensively studied in the closed world while stay unexplored in the open world. In this paper, we utilize the Transformer for open-world instance segmentation and present SWORD. Firstly, we introduce to attach the stop-gradient operation before classification head and further add IoU heads for discovering novel objects. We demonstrate that a simple stop-gradient operation not only prevents the novel objects from being suppressed as background, but also allows the network to enjoy the merit of heuristic label assignment. Secondly, we propose a novel contrastive learning framework to enlarge the representations between objects and background. Specifically, we maintain a universal object queue to obtain the object center, and dynamically select positive and negative samples from the object queries for contrastive learning. While the previous works only focus on pursuing average recall and neglect average precision, we show the prominence of SWORD by giving consideration to both criteria. Our models achieve state-of-the-art performance in various open-world cross-category and cross-dataset generalizations. Particularly, in VOC to non-VOC setup, our method sets new state-of-the-art results of 40.0% on ARb100 and 34.9% on ARm100. For COCO to UVO generalization, SWORD significantly outperforms the previous best open-world model by 5.9% on APm and 8.1% on ARm100.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2510.07313.pdf' target='_blank'>https://arxiv.org/pdf/2510.07313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07313">WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2510.07313.pdf' target='_blank'>https://arxiv.org/pdf/2510.07313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07313">WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2509.23958.pdf' target='_blank'>https://arxiv.org/pdf/2509.23958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Ye, Tianyu He, Shuo Yang, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23958">Reinforcement Learning with Inverse Rewards for World Model Post-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2509.23958.pdf' target='_blank'>https://arxiv.org/pdf/2509.23958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Ye, Tianyu He, Shuo Yang, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23958">Reinforcement Learning with Inverse Rewards for World Model Post-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models simulate dynamic environments, enabling agents to interact with diverse input modalities. Although recent advances have improved the visual quality and temporal consistency of video world models, their ability of accurately modeling human-specified actions remains under-explored. Reinforcement learning presents a promising approach for directly improving the suboptimal action-following capability of pre-trained models, assuming that an appropriate reward function can be defined. However, transferring reinforcement learning post-training methods to world model is impractical due to the prohibitive cost of large-scale preference annotations and the infeasibility of constructing rule-based video verifiers. To address this gap, we propose Reinforcement Learning with Inverse Rewards (RLIR), a post-training framework that derives verifiable reward signals by recovering input actions from generated videos using an Inverse Dynamics Model. By mapping high-dimensional video modality to a low-dimensional action space, RLIR provides an objective and verifiable reward for optimization via Group Relative Policy Optimization. Experiments across autoregressive and diffusion paradigms demonstrate 5-10% gains in action-following, up to 10% improvements in visual quality, and higher human preference scores, establishing RLIR as the first post-training method specifically designed to enhance action-following in video world models.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2505.19017.pdf' target='_blank'>https://arxiv.org/pdf/2505.19017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaxuan Li, Yichen Zhu, Junjie Wen, Chaomin Shen, Yi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19017">WorldEval: World Model as Real-World Robot Policies Evaluator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of robotics has made significant strides toward developing generalist robot manipulation policies. However, evaluating these policies in real-world scenarios remains time-consuming and challenging, particularly as the number of tasks scales and environmental conditions change. In this work, we demonstrate that world models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation. A key challenge is generating accurate policy videos from world models that faithfully reflect the robot actions. We observe that directly inputting robot actions or using high-dimensional encoding methods often fails to generate action-following videos. To address this, we propose Policy2Vec, a simple yet effective approach to turn a video generation model into a world simulator that follows latent action to generate the robot video. We then introduce WorldEval, an automated pipeline designed to evaluate real-world robot policies entirely online. WorldEval effectively ranks various robot policies and individual checkpoints within a single policy, and functions as a safety detector to prevent dangerous actions by newly developed robot models. Through comprehensive paired evaluations of manipulation policies in real-world environments, we demonstrate a strong correlation between policy performance in WorldEval and real-world scenarios. Furthermore, our method significantly outperforms popular methods such as real-to-sim approach.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2505.06861.pdf' target='_blank'>https://arxiv.org/pdf/2505.06861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongxiu Liu, Haoyi Niu, Zhihao Wang, Jinliang Zheng, Yinan Zheng, Zhonghong Ou, Jianming Hu, Jianxiong Li, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06861">Efficient Robotic Policy Learning via Latent Space Backward Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2505.06482.pdf' target='_blank'>https://arxiv.org/pdf/2505.06482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minting Pan, Yitao Zheng, Jiajian Li, Yunbo Wang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06482">Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) enables policy optimization using static datasets, avoiding the risks and costs of extensive real-world exploration. However, it struggles with suboptimal offline behaviors and inaccurate value estimation due to the lack of environmental interaction. We present Video-Enhanced Offline RL (VeoRL), a model-based method that constructs an interactive world model from diverse, unlabeled video data readily available online. Leveraging model-based behavior guidance, our approach transfers commonsense knowledge of control policy and physical dynamics from natural videos to the RL agent within the target domain. VeoRL achieves substantial performance gains (over 100% in some cases) across visual control tasks in robotic manipulation, autonomous driving, and open-world video games.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2504.08388.pdf' target='_blank'>https://arxiv.org/pdf/2504.08388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08388">MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2503.07819.pdf' target='_blank'>https://arxiv.org/pdf/2503.07819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joey Wilson, Marcelino Almeida, Sachit Mahajan, Martin Labrie, Maani Ghaffari, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnab Sen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07819">POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel algorithm for quantifying uncertainty and information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality. While 3D-GS has proven to be a useful world model with high-quality rasterizations, it does not natively quantify uncertainty or information, posing a challenge for real-world applications such as 3D-GS SLAM. We propose to quantify information gain in 3D-GS by reformulating the problem through the lens of optimal experimental design, which is a classical solution widely used in literature. By restructuring information quantification of 3D-GS through optimal experimental design, we arrive at multiple solutions, of which T-Optimality and D-Optimality perform the best quantitatively and qualitatively as measured on two popular datasets. Additionally, we propose a block diagonal covariance approximation which provides a measure of correlation at the expense of a greater computation cost.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2411.00785.pdf' target='_blank'>https://arxiv.org/pdf/2411.00785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00785">IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2306.03360.pdf' target='_blank'>https://arxiv.org/pdf/2306.03360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minting Pan, Yitao Zheng, Yunbo Wang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03360">Model-Based Reinforcement Learning with Multi-Task Offline Pretraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretraining reinforcement learning (RL) models on offline datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in dynamics and behaviors across various tasks. We present a model-based RL method that learns to transfer potentially useful dynamics and action demonstrations from offline data to a novel task. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the task relevance for both dynamics representation transfer and policy transfer. We build a time-varying, domain-selective distillation loss to generate a set of offline-to-online similarity weights. These weights serve two purposes: (i) adaptively transferring the task-agnostic knowledge of physical dynamics to facilitate world model training, and (ii) learning to replay relevant source actions to guide the target policy. We demonstrate the advantages of our approach compared with the state-of-the-art methods in Meta-World and DeepMind Control Suite.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2303.14889.pdf' target='_blank'>https://arxiv.org/pdf/2303.14889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minting Pan, Xiangming Zhu, Yitao Zheng, Yunbo Wang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14889">Model-Based Reinforcement Learning with Isolated Imaginations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models learn the consequences of actions in vision-based interactive systems. However, in practical scenarios like autonomous driving, noncontrollable dynamics that are independent or sparsely dependent on action signals often exist, making it challenging to learn effective world models. To address this issue, we propose Iso-Dream++, a model-based reinforcement learning approach that has two main contributions. First, we optimize the inverse dynamics to encourage the world model to isolate controllable state transitions from the mixed spatiotemporal variations of the environment. Second, we perform policy optimization based on the decoupled latent imaginations, where we roll out noncontrollable states into the future and adaptively associate them with the current controllable state. This enables long-horizon visuomotor control tasks to benefit from isolating mixed dynamics sources in the wild, such as self-driving cars that can anticipate the movement of other vehicles, thereby avoiding potential risks. On top of our previous work, we further consider the sparse dependencies between controllable and noncontrollable states, address the training collapse problem of state decoupling, and validate our approach in transfer learning setups. Our empirical study demonstrates that Iso-Dream++ outperforms existing reinforcement learning models significantly on CARLA and DeepMind Control.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2303.06572.pdf' target='_blank'>https://arxiv.org/pdf/2303.06572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minting Pan, Wendong Zhang, Geng Chen, Xiangming Zhu, Siyu Gao, Yunbo Wang, Xiaokang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06572">Continual Visual Reinforcement Learning with A Life-Long World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning physical dynamics in a series of non-stationary environments is a challenging but essential task for model-based reinforcement learning (MBRL) with visual inputs. It requires the agent to consistently adapt to novel tasks without forgetting previous knowledge. In this paper, we present a new continual learning approach for visual dynamics modeling and explore its efficacy in visual control. The key assumption is that an ideal world model can provide a non-forgetting environment simulator, which enables the agent to optimize the policy in a multi-task learning manner based on the imagined trajectories from the world model. To this end, we first introduce the life-long world model, which learns task-specific latent dynamics using a mixture of Gaussians and incorporates generative experience replay to mitigate catastrophic forgetting. Then, we further address the value estimation challenge for previous tasks with the exploratory-conservative behavior learning approach. Our model remarkably outperforms the straightforward combinations of existing continual learning and visual RL algorithms on DeepMind Control Suite and Meta-World benchmarks with continual visual control tasks.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2206.03382.pdf' target='_blank'>https://arxiv.org/pdf/2206.03382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.03382">Tutel: Adaptive Mixture-of-Experts at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2510.01179.pdf' target='_blank'>https://arxiv.org/pdf/2510.01179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangchen Xu, Adriana Meza Soria, Shawn Tan, Anurag Roy, Ashish Sunil Agrawal, Radha Poovendran, Rameswar Panda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01179">TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2510.01179.pdf' target='_blank'>https://arxiv.org/pdf/2510.01179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangchen Xu, Adriana Meza Soria, Shawn Tan, Anurag Roy, Ashish Sunil Agrawal, Radha Poovendran, Rameswar Panda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01179">TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2505.20171.pdf' target='_blank'>https://arxiv.org/pdf/2505.20171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, Xun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20171">Long-Context State-Space Video World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video diffusion models have recently shown promise for world modeling through autoregressive frame prediction conditioned on actions. However, they struggle to maintain long-term memory due to the high computational cost associated with processing extended sequences in attention layers. To overcome this limitation, we propose a novel architecture leveraging state-space models (SSMs) to extend temporal memory without compromising computational efficiency. Unlike previous approaches that retrofit SSMs for non-causal vision tasks, our method fully exploits the inherent advantages of SSMs in causal sequence modeling. Central to our design is a block-wise SSM scanning scheme, which strategically trades off spatial consistency for extended temporal memory, combined with dense local attention to ensure coherence between consecutive frames. We evaluate the long-term memory capabilities of our model through spatial retrieval and reasoning tasks over extended horizons. Experiments on Memory Maze and Minecraft datasets demonstrate that our approach surpasses baselines in preserving long-range memory, while maintaining practical inference speeds suitable for interactive applications.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2411.02914.pdf' target='_blank'>https://arxiv.org/pdf/2411.02914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Fu, Yi Zhou, Tao Zhou, Yi Yang, Bojun Gao, Qun Li, Guobin Wu, Ling Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02914">Exploring the Interplay Between Video Generation and World Models in Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models and video generation are pivotal technologies in the domain of autonomous driving, each playing a critical role in enhancing the robustness and reliability of autonomous systems. World models, which simulate the dynamics of real-world environments, and video generation models, which produce realistic video sequences, are increasingly being integrated to improve situational awareness and decision-making capabilities in autonomous vehicles. This paper investigates the relationship between these two technologies, focusing on how their structural parallels, particularly in diffusion-based models, contribute to more accurate and coherent simulations of driving scenarios. We examine leading works such as JEPA, Genie, and Sora, which exemplify different approaches to world model design, thereby highlighting the lack of a universally accepted definition of world models. These diverse interpretations underscore the field's evolving understanding of how world models can be optimized for various autonomous driving tasks. Furthermore, this paper discusses the key evaluation metrics employed in this domain, such as Chamfer distance for 3D scene reconstruction and FrÃ©chet Inception Distance (FID) for assessing the quality of generated video content. By analyzing the interplay between video generation and world models, this survey identifies critical challenges and future research directions, emphasizing the potential of these technologies to jointly advance the performance of autonomous driving systems. The findings presented in this paper aim to provide a comprehensive understanding of how the integration of video generation and world models can drive innovation in the development of safer and more reliable autonomous vehicles.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2409.12005.pdf' target='_blank'>https://arxiv.org/pdf/2409.12005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Sai Rajeswar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12005">Representing Positional Information in Generative World Models for Object Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object manipulation capabilities are essential skills that set apart embodied agents engaging with the world, especially in the realm of robotics. The ability to predict outcomes of interactions with objects is paramount in this setting. While model-based control methods have started to be employed for tackling manipulation tasks, they have faced challenges in accurately manipulating objects. As we analyze the causes of this limitation, we identify the cause of underperformance in the way current world models represent crucial positional information, especially about the target's goal specification for object positioning tasks. We introduce a general approach that empowers world model-based agents to effectively solve object-positioning tasks. We propose two declinations of this approach for generative world models: position-conditioned (PCP) and latent-conditioned (LCP) policy learning. In particular, LCP employs object-centric latent representations that explicitly capture object positional information for goal specification. This naturally leads to the emergence of multimodal capabilities, enabling the specification of goals through spatial coordinates or a visual goal. Our methods are rigorously evaluated across several manipulation environments, showing favorable performance compared to current model-based control approaches.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2406.07381.pdf' target='_blank'>https://arxiv.org/pdf/2406.07381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07381">World Models with Hints of Large Language Models for Goal Achieving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\%, 21.1\%, and 9.9\%, respectively.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2403.05131.pdf' target='_blank'>https://arxiv.org/pdf/2403.05131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05131">Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of video generation from text, starting with animating MNIST numbers to simulating the physical world with Sora, has progressed at a breakneck speed over the past seven years. While often seen as a superficial expansion of the predecessor text-to-image generation model, text-to-video generation models are developed upon carefully engineered constituents. Here, we systematically discuss these elements consisting of but not limited to core building blocks (vision, language, and temporal) and supporting features from the perspective of their contributions to achieving a world model. We employ the PRISMA framework to curate 97 impactful research articles from renowned scientific databases primarily studying video synthesis using text conditions. Upon minute exploration of these manuscripts, we observe that text-to-video generation involves more intricate technologies beyond the plain extension of text-to-image generation. Our additional review into the shortcomings of Sora-generated videos pinpoints the call for more in-depth studies in various enabling aspects of video generation such as dataset, evaluation metric, efficient architecture, and human-controlled generation. Finally, we conclude that the study of the text-to-video generation may still be in its infancy, requiring contribution from the cross-discipline research community towards its advancement as the first step to realize artificial general intelligence (AGI).
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2310.15017.pdf' target='_blank'>https://arxiv.org/pdf/2310.15017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjian Qiao, Jiafei Lyu, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15017">Mind the Model, Not the Agent: The Primacy Bias in Model-based RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primacy bias in model-free reinforcement learning (MFRL), which refers to the agent's tendency to overfit early data and lose the ability to learn from new data, can significantly decrease the performance of MFRL algorithms. Previous studies have shown that employing simple techniques, such as resetting the agent's parameters, can substantially alleviate the primacy bias in MFRL. However, the primacy bias in model-based reinforcement learning (MBRL) remains unexplored. In this work, we focus on investigating the primacy bias in MBRL. We begin by observing that resetting the agent's parameters harms its performance in the context of MBRL. We further find that the primacy bias in MBRL is more closely related to the primacy bias of the world model instead of the primacy bias of the agent. Based on this finding, we propose \textit{world model resetting}, a simple yet effective technique to alleviate the primacy bias in MBRL. We apply our method to two different MBRL algorithms, MBPO and DreamerV2. We validate the effectiveness of our method on multiple continuous control tasks on MuJoCo and DeepMind Control Suite, as well as discrete control tasks on Atari 100k benchmark. The experimental results show that \textit{world model resetting} can significantly alleviate the primacy bias in the model-based setting and improve the algorithm's performance. We also give a guide on how to perform \textit{world model resetting} effectively.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2309.09864.pdf' target='_blank'>https://arxiv.org/pdf/2309.09864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daria de Tinguy, Toon Van de Maele, Tim Verbelen, Bart Dhoedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09864">Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for navigation in Multi-Room Maze Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive maps play a crucial role in facilitating flexible behaviour by representing spatial and conceptual relationships within an environment. The ability to learn and infer the underlying structure of the environment is crucial for effective exploration and navigation. This paper introduces a hierarchical active inference model addressing the challenge of inferring structure in the world from pixel-based observations. We propose a three-layer hierarchical model consisting of a cognitive map, an allocentric, and an egocentric world model, combining curiosity-driven exploration with goal-oriented behaviour at the different levels of reasoning from context to place to motion. This allows for efficient exploration and goal-directed search in room-structured mini-grid environments.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2308.15852.pdf' target='_blank'>https://arxiv.org/pdf/2308.15852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daria de Tinguy, Sven Remmery, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15852">Learning to Navigate from Scratch using World Models and Curiosity: the Good, the Bad, and the Ugly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to navigate unknown environments from scratch is a challenging problem. This work presents a system that integrates world models with curiosity-driven exploration for autonomous navigation in new environments. We evaluate performance through simulations and real-world experiments of varying scales and complexities. In simulated environments, the approach rapidly and comprehensively explores the surroundings. Real-world scenarios introduce additional challenges. Despite demonstrating promise in a small controlled environment, we acknowledge that larger and dynamic environments can pose challenges for the current system. Our analysis emphasizes the significance of developing adaptable and robust world models that can handle environmental changes to prevent repetitive exploration of the same areas.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2307.02427.pdf' target='_blank'>https://arxiv.org/pdf/2307.02427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02427">FOCUS: Object-Centric World Models for Robotics Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the world in terms of objects and the possible interplays with them is an important cognition ability, especially in robotics manipulation, where many tasks require robot-object interactions. However, learning such a structured world model, which specifically captures entities and relationships, remains a challenging and underexplored problem. To address this, we propose FOCUS, a model-based agent that learns an object-centric world model. Thanks to a novel exploration bonus that stems from the object-centric representation, FOCUS can be deployed on robotics manipulation tasks to explore object interactions more easily. Evaluating our approach on manipulation tasks across different settings, we show that object-centric world models allow the agent to solve tasks more efficiently and enable consistent exploration of robot-object interactions. Using a Franka Emika robot arm, we also showcase how FOCUS could be adopted in real-world settings.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2306.13546.pdf' target='_blank'>https://arxiv.org/pdf/2306.13546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daria de Tinguy, Toon Van de Maele, Tim Verbelen, Bart Dhoedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13546">Inferring Hierarchical Structure in Multi-Room Maze Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive maps play a crucial role in facilitating flexible behaviour by representing spatial and conceptual relationships within an environment. The ability to learn and infer the underlying structure of the environment is crucial for effective exploration and navigation. This paper introduces a hierarchical active inference model addressing the challenge of inferring structure in the world from pixel-based observations. We propose a three-layer hierarchical model consisting of a cognitive map, an allocentric, and an egocentric world model, combining curiosity-driven exploration with goal-oriented behaviour at the different levels of reasoning from context to place to motion. This allows for efficient exploration and goal-directed search in room-structured mini-grid environments.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2211.13350.pdf' target='_blank'>https://arxiv.org/pdf/2211.13350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, Sai Rajeswar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13350">Choreographer: Learning and Adapting Skills in Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exploration policy. The skills can be used to effectively adapt to downstream tasks, as we show in the URL benchmark, where we outperform previous approaches from both pixels and states inputs. The learned skills also explore the environment thoroughly, finding sparse rewards more frequently, as shown in goal-reaching tasks from the DMC Suite and Meta-World. Website and code: https://skillchoreographer.github.io/
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2508.13009.pdf' target='_blank'>https://arxiv.org/pdf/2508.13009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, Hao-Xiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li, Xuchen Song, Yang Liu, Eric Li, Yahui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13009">Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2507.03298.pdf' target='_blank'>https://arxiv.org/pdf/2507.03298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhao Wang, Kaixin Wang, Li Zhao, Peter Stone, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03298">Dyn-O: Building Structured World Models with Object-Centric Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models aim to capture the dynamics of the environment, enabling agents to predict and plan for future states. In most scenarios of interest, the dynamics are highly centered on interactions among objects within the environment. This motivates the development of world models that operate on object-centric rather than monolithic representations, with the goal of more effectively capturing environment dynamics and enhancing compositional generalization. However, the development of object-centric world models has largely been explored in environments with limited visual complexity (such as basic geometries). It remains underexplored whether such models can generalize to more complex settings with diverse textures and cluttered scenes. In this paper, we fill this gap by introducing Dyn-O, an enhanced structured world model built upon object-centric representations. Compared to prior work in object-centric representations, Dyn-O improves in both learning representations and modeling dynamics. On the challenging Procgen games, we find that our method can learn object-centric world models directly from pixel observations, outperforming DreamerV3 in rollout prediction accuracy. Furthermore, by decoupling object-centric features into dynamics-agnostic and dynamics-aware components, we enable finer-grained manipulation of these features and generate more diverse imagined trajectories.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2505.05495.pdf' target='_blank'>https://arxiv.org/pdf/2505.05495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05495">Learning 3D Persistent Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2504.20995.pdf' target='_blank'>https://arxiv.org/pdf/2504.20995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20995">TesserAct: Learning 4D Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2503.11299.pdf' target='_blank'>https://arxiv.org/pdf/2503.11299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Zhao, Hongqiu Wu, Dongjie Yang, Anni Zou, Jiale Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11299">BriLLM: Brain-inspired Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce BriLLM, a brain-inspired large language model that fundamentally redefines the foundations of machine learning through its implementation of Signal Fully-connected flowing (SiFu) learning. This work addresses the critical bottleneck hindering AI's progression toward Artificial General Intelligence (AGI)--the disconnect between language models and "world models"--as well as the fundamental limitations of Transformer-based architectures rooted in the conventional representation learning paradigm. BriLLM incorporates two pivotal neurocognitive principles: (1) static semantic mapping, where tokens are mapped to specialized nodes analogous to cortical areas, and (2) dynamic signal propagation, which simulates electrophysiological information dynamics observed in brain activity. This architecture enables multiple transformative breakthroughs: natural multi-modal compatibility, full model interpretability at the node level, context-length independent scaling, and the first global-scale simulation of brain-like information processing for language tasks. Our initial 1-2B parameter models successfully replicate GPT-1-level generative capabilities while demonstrating stable perplexity reduction. Scalability analyses confirm the feasibility of 100-200B parameter variants capable of processing 40,000-token vocabularies. The paradigm is reinforced by both Occam's Razor--evidenced in the simplicity of direct semantic mapping--and natural evolution--given the brain's empirically validated AGI architecture. BriLLM establishes a novel, biologically grounded framework for AGI advancement that addresses fundamental limitations of current approaches.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2503.11299.pdf' target='_blank'>https://arxiv.org/pdf/2503.11299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Zhao, Hongqiu Wu, Dongjie Yang, Anni Zou, Jiale Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11299">BriLLM: Brain-inspired Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce BriLLM, a brain-inspired large language model that fundamentally redefines the foundations of machine learning through its implementation of Signal Fully-connected flowing (SiFu) learning. This work addresses the critical bottleneck hindering AI's progression toward Artificial General Intelligence (AGI)--the disconnect between language models and "world models"--as well as the fundamental limitations of Transformer-based architectures rooted in the conventional representation learning paradigm. BriLLM incorporates two pivotal neurocognitive principles: (1) static semantic mapping, where tokens are mapped to specialized nodes analogous to cortical areas, and (2) dynamic signal propagation, which simulates electrophysiological information dynamics observed in brain activity. This architecture enables multiple transformative breakthroughs: natural multi-modal compatibility, full model interpretability at the node level, context-length independent scaling, and the first global-scale simulation of brain-like information processing for language tasks. Our initial 1-2B parameter models successfully replicate GPT-1-level generative capabilities while demonstrating stable perplexity reduction. Scalability analyses confirm the feasibility of 100-200B parameter variants capable of processing 40,000-token vocabularies. The paradigm is reinforced by both Occam's Razor--evidenced in the simplicity of direct semantic mapping--and natural evolution--given the brain's empirically validated AGI architecture. BriLLM establishes a novel, biologically grounded framework for AGI advancement that addresses fundamental limitations of current approaches.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2406.09513.pdf' target='_blank'>https://arxiv.org/pdf/2406.09513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madeline Navarro, Samuel Rey, Andrei Buciulea, Antonio G. Marques, Santiago Segarra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09513">Fair GLASSO: Estimating Fair Graphical Models with Unbiased Statistical Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose estimating Gaussian graphical models (GGMs) that are fair with respect to sensitive nodal attributes. Many real-world models exhibit unfair discriminatory behavior due to biases in data. Such discrimination is known to be exacerbated when data is equipped with pairwise relationships encoded in a graph. Additionally, the effect of biased data on graphical models is largely underexplored. We thus introduce fairness for graphical models in the form of two bias metrics to promote balance in statistical similarities across nodal groups with different sensitive attributes. Leveraging these metrics, we present Fair GLASSO, a regularized graphical lasso approach to obtain sparse Gaussian precision matrices with unbiased statistical dependencies across groups. We also propose an efficient proximal gradient algorithm to obtain the estimates. Theoretically, we express the tradeoff between fair and accurate estimated precision matrices. Critically, this includes demonstrating when accuracy can be preserved in the presence of a fairness regularizer. On top of this, we study the complexity of Fair GLASSO and demonstrate that our algorithm enjoys a fast convergence rate. Our empirical validation includes synthetic and real-world simulations that illustrate the value and effectiveness of our proposed optimization problem and iterative algorithm.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2406.06370.pdf' target='_blank'>https://arxiv.org/pdf/2406.06370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Bogdoll, NoÃ«l Ollick, Tim Joseph, Svetlana Pavlitska, J. Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06370">UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dealing with atypical traffic scenarios remains a challenging task in autonomous driving. However, most anomaly detection approaches cannot be trained on raw sensor data but require exposure to outlier data and powerful semantic segmentation models trained in a supervised fashion. This limits the representation of normality to labeled data, which does not scale well. In this work, we revisit unsupervised anomaly detection and present UMAD, leveraging generative world models and unsupervised image segmentation. Our method outperforms state-of-the-art unsupervised anomaly detection.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2404.12377.pdf' target='_blank'>https://arxiv.org/pdf/2404.12377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12377">RoboDreamer: Learning Compositional World Models for Robot Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2403.09631.pdf' target='_blank'>https://arxiv.org/pdf/2403.09631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09631">3D-VLA: A 3D Vision-Language-Action Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2402.16720.pdf' target='_blank'>https://arxiv.org/pdf/2402.16720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qifeng Li, Xiaosong Jia, Shaobo Wang, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16720">Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world autonomous driving (AD) especially urban driving involves many corner cases. The lately released AD simulator CARLA v2 adds 39 common events in the driving scene, and provide more quasi-realistic testbed compared to CARLA v1. It poses new challenge to the community and so far no literature has reported any success on the new scenarios in V2 as existing works mostly have to rely on specific rules for planning yet they cannot cover the more complex cases in CARLA v2. In this work, we take the initiative of directly training a planner and the hope is to handle the corner cases flexibly and effectively, which we believe is also the future of AD. To our best knowledge, we develop the first model-based RL method named Think2Drive for AD, with a world model to learn the transitions of the environment, and then it acts as a neural simulator to train the planner. This paradigm significantly boosts the training efficiency due to the low dimensional state space and parallel computing of tensors in the world model. As a result, Think2Drive is able to run in an expert-level proficiency in CARLA v2 within 3 days of training on a single A6000 GPU, and to our best knowledge, so far there is no reported success (100\% route completion)on CARLA v2. We also propose CornerCase-Repository, a benchmark that supports the evaluation of driving models by scenarios. Additionally, we propose a new and balanced metric to evaluate the performance by route completion, infraction number, and scenario density, so that the driving score could give more information about the actual driving performance.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2303.08266.pdf' target='_blank'>https://arxiv.org/pdf/2303.08266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariam Guizani, Aileen Abril Castro-Guzman, Anita Sarma, Igor Steinmacher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08266">Rules of Engagement: Why and How Companies Participate in OSS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the 'old world' model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2509.26255.pdf' target='_blank'>https://arxiv.org/pdf/2509.26255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26255">ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2509.26255.pdf' target='_blank'>https://arxiv.org/pdf/2509.26255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26255">ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2509.21027.pdf' target='_blank'>https://arxiv.org/pdf/2509.21027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Li, Qianyue Hao, Yu Shang, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21027">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2509.21027.pdf' target='_blank'>https://arxiv.org/pdf/2509.21027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibo Li, Qianyue Hao, Yu Shang, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21027">KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2509.12437.pdf' target='_blank'>https://arxiv.org/pdf/2509.12437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingrui Wang, Zhexiao Sun, Zhouheng Li, Cheng Wang, Youlun Peng, Hongyuan Ye, Baha Zarrouki, Wei Li, Mattia Piccinini, Lei Xie, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12437">Enhancing Physical Consistency in Lightweight World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in deploying world models is the trade-off between size and performance. Large world models can capture rich physical dynamics but require massive computing resources, making them impractical for edge devices. Small world models are easier to deploy but often struggle to learn accurate physics, leading to poor predictions. We propose the Physics-Informed BEV World Model (PIWM), a compact model designed to efficiently capture physical interactions in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training to improve dynamic object modeling and future prediction. We also introduce a simple yet effective technique, Warm Start, for inference to enhance prediction quality with a zero-shot model. Experiments show that at the same parameter scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score. Moreover, even when compared with the largest baseline model (400M), the smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score with a 28% faster inference speed.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2509.12437.pdf' target='_blank'>https://arxiv.org/pdf/2509.12437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingrui Wang, Zhexiao Sun, Zhouheng Li, Cheng Wang, Youlun Peng, Hongyuan Ye, Baha Zarrouki, Wei Li, Mattia Piccinini, Lei Xie, Johannes Betz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12437">Enhancing Physical Consistency in Lightweight World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in deploying world models is the trade-off between size and performance. Large world models can capture rich physical dynamics but require massive computing resources, making them impractical for edge devices. Small world models are easier to deploy but often struggle to learn accurate physics, leading to poor predictions. We propose the Physics-Informed BEV World Model (PIWM), a compact model designed to efficiently capture physical interactions in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training to improve dynamic object modeling and future prediction. We also introduce a simple yet effective technique, Warm Start, for inference to enhance prediction quality with a zero-shot model. Experiments show that at the same parameter scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score. Moreover, even when compared with the largest baseline model (400M), the smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score with a 28% faster inference speed.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2509.02722.pdf' target='_blank'>https://arxiv.org/pdf/2509.02722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, Pascale Fung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02722">Planning with Reasoning using Vision Language World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2509.02722.pdf' target='_blank'>https://arxiv.org/pdf/2509.02722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, Pascale Fung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02722">Planning with Reasoning using Vision Language World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2508.20840.pdf' target='_blank'>https://arxiv.org/pdf/2508.20840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20840">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2508.20840.pdf' target='_blank'>https://arxiv.org/pdf/2508.20840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Sun, Liujia Yang, Wei Tang, Wei Huang, Kaixin Xu, Yongchao Chen, Mingyu Liu, Jiange Yang, Haoyi Zhu, Yating Wang, Tong He, Yilun Chen, Xili Dai, Nanyang Ye, Qinying Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20840">Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a "GPT moment" in the embodied domain. There is a naive observation: the diversity of embodied data far exceeds the relatively small space of possible primitive motions. Based on this insight, we propose a novel paradigm for world modeling--Primitive Embodied World Models (PEWM). By restricting video generation to fixed short horizons, our approach 1) enables fine-grained alignment between linguistic concepts and visual representations of robotic actions, 2) reduces learning complexity, 3) improves data efficiency in embodied data collection, and 4) decreases inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2506.04363.pdf' target='_blank'>https://arxiv.org/pdf/2506.04363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, Pascale Fung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04363">WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans are known to have an internal "world model" that enables us to carry out action planning based on world states. AI agents need to have such a world model for action planning as well. It is not clear how current AI models, especially generative models, are able to learn such world models and carry out procedural planning in diverse environments. We introduce WorldPrediction, a video-based benchmark for evaluating world modeling and procedural planning capabilities of different AI models. In contrast to prior benchmarks that focus primarily on low-level world modeling and robotic motion planning, WorldPrediction is the first benchmark that emphasizes actions with temporal and semantic abstraction. Given initial and final world states, the task is to distinguish the proper action (WorldPrediction-WM) or the properly ordered sequence of actions (WorldPrediction-PP) from a set of counterfactual distractors. This discriminative task setup enable us to evaluate different types of world models and planners and realize a thorough comparison across different hypothesis. The benchmark represents states and actions using visual observations. In order to prevent models from exploiting low-level continuity cues in background scenes, we provide "action equivalents" - identical actions observed in different contexts - as candidates for selection. This benchmark is grounded in a formal framework of partially observable semi-MDP, ensuring better reliability and robustness of the evaluation. We conduct extensive human filtering and validation on our benchmark and show that current frontier models barely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP whereas humans are able to solve both tasks perfectly.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2506.01103.pdf' target='_blank'>https://arxiv.org/pdf/2506.01103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyi Chen, Haoyi Zhu, Xianglong He, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Zhoujie Fu, Jiangmiao Pang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01103">DeepVerse: 4D Autoregressive Video Generation as a World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models serve as essential building blocks toward Artificial General Intelligence (AGI), enabling intelligent agents to predict future states and plan actions by simulating complex physical interactions. However, existing interactive models primarily predict visual observations, thereby neglecting crucial hidden states like geometric structures and spatial coherence. This leads to rapid error accumulation and temporal inconsistency. To address these limitations, we introduce DeepVerse, a novel 4D interactive world model explicitly incorporating geometric predictions from previous timesteps into current predictions conditioned on actions. Experiments demonstrate that by incorporating explicit geometric constraints, DeepVerse captures richer spatio-temporal relationships and underlying physical dynamics. This capability significantly reduces drift and enhances temporal consistency, enabling the model to reliably generate extended future sequences and achieve substantial improvements in prediction accuracy, visual realism, and scene rationality. Furthermore, our method provides an effective solution for geometry-aware memory retrieval, effectively preserving long-term spatial consistency. We validate the effectiveness of DeepVerse across diverse scenarios, establishing its capacity for high-fidelity, long-horizon predictions grounded in geometry-aware dynamics.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2505.19386.pdf' target='_blank'>https://arxiv.org/pdf/2505.19386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nate Gillman, Charles Herrmann, Michael Freeman, Daksh Aggarwal, Evan Luo, Deqing Sun, Chen Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19386">Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied. In this work, we investigate using physical forces as a control signal for video generation and propose force prompts which enable users to interact with images through both localized point forces, such as poking a plant, and global wind force fields, such as wind blowing on fabric. We demonstrate that these force prompts can enable videos to respond realistically to physical control signals by leveraging the visual and motion prior in the original pretrained model, without using any 3D asset or physics simulator at inference. The primary challenge of force prompting is the difficulty in obtaining high quality paired force-video training data, both in the real world due to the difficulty of obtaining force signals, and in synthetic data due to limitations in the visual quality and domain diversity of physics simulators. Our key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. Our method can generate videos which simulate forces across diverse geometries, settings, and materials. We also try to understand the source of this generalization and perform ablations that reveal two key elements: visual diversity and the use of specific text keywords during training. Our approach is trained on only around 15k training examples for a single day on four A100 GPUs, and outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions. We release all datasets, code, weights, and interactive video demos at our project page.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2503.18945.pdf' target='_blank'>https://arxiv.org/pdf/2503.18945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18945">Aether: Geometric-Aware Unified World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates zero-shot synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Notably, even without real-world data, its reconstruction performance is comparable with or even better than that of domain-specific models. Additionally, Aether employs camera trajectories as geometry-informed action spaces, enabling effective action-conditioned prediction and visual planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2412.02700.pdf' target='_blank'>https://arxiv.org/pdf/2412.02700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02700">Motion Prompting: Controlling Video Generation with Motion Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2410.23156.pdf' target='_blank'>https://arxiv.org/pdf/2410.23156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, JoÃ£o F. Henriques, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23156">VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2407.00118.pdf' target='_blank'>https://arxiv.org/pdf/2407.00118.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00118">From Efficient Multimodal Models to World Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2301.12050.pdf' target='_blank'>https://arxiv.org/pdf/2301.12050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12050">Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2510.02538.pdf' target='_blank'>https://arxiv.org/pdf/2510.02538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Shangzhe Li, Haoyi Niu, Zhiao Huang, Weitong Zhang, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02538">A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are interested in solving the problem of imitation learning with a limited amount of real-world expert data. Existing offline imitation methods often struggle with poor data coverage and severe performance degradation. We propose a solution that leverages robot simulators to achieve online imitation learning. Our sim-to-real framework is based on world models and combines online imitation pretraining with offline finetuning. By leveraging online interactions, our approach alleviates the data coverage limitations of offline methods, leading to improved robustness and reduced performance degradation during finetuning. It also enhances generalization during domain transfer. Our empirical results demonstrate its effectiveness, improving success rates by at least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over existing offline imitation learning baselines.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2510.02538.pdf' target='_blank'>https://arxiv.org/pdf/2510.02538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Wang, Shangzhe Li, Haoyi Niu, Zhiao Huang, Weitong Zhang, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02538">A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are interested in solving the problem of imitation learning with a limited amount of real-world expert data. Existing offline imitation methods often struggle with poor data coverage and severe performance degradation. We propose a solution that leverages robot simulators to achieve online imitation learning. Our sim-to-real framework is based on world models and combines online imitation pretraining with offline finetuning. By leveraging online interactions, our approach alleviates the data coverage limitations of offline methods, leading to improved robustness and reduced performance degradation during finetuning. It also enhances generalization during domain transfer. Our empirical results demonstrate its effectiveness, improving success rates by at least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over existing offline imitation learning baselines.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2509.17808.pdf' target='_blank'>https://arxiv.org/pdf/2509.17808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Lu, Biao Wu, Zhidong Li, Kunqi Li, Chenya Huang, Huacan Wang, Qizhen Lan, Ronghao Chen, Ling Chen, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17808">Remote Sensing-Oriented World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have shown potential in artificial intelligence by predicting and reasoning about world states beyond direct observations. However, existing approaches are predominantly evaluated in synthetic environments or constrained scene settings, limiting their validation in real-world contexts with broad spatial coverage and complex semantics. Meanwhile, remote sensing applications urgently require spatial reasoning capabilities for disaster response and urban planning. This paper bridges these gaps by introducing the first framework for world modeling in remote sensing. We formulate remote sensing world modeling as direction-conditioned spatial extrapolation, where models generate semantically consistent adjacent image tiles given a central observation and directional instruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing World-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks across four scenarios: general, flood, urban, and rural. RSWISE combines visual fidelity assessment with instruction compliance evaluation using GPT-4o as a semantic judge, ensuring models genuinely perform spatial reasoning rather than simple replication. Afterwards, we present RemoteBAGEL, a unified multimodal model fine-tuned on remote sensing data for spatial extrapolation tasks. Extensive experiments demonstrate that RemoteBAGEL consistently outperforms state-of-the-art baselines on RSWISE.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2509.17808.pdf' target='_blank'>https://arxiv.org/pdf/2509.17808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Lu, Biao Wu, Zhidong Li, Kunqi Li, Chenya Huang, Huacan Wang, Qizhen Lan, Ronghao Chen, Ling Chen, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17808">Remote Sensing-Oriented World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have shown potential in artificial intelligence by predicting and reasoning about world states beyond direct observations. However, existing approaches are predominantly evaluated in synthetic environments or constrained scene settings, limiting their validation in real-world contexts with broad spatial coverage and complex semantics. Meanwhile, remote sensing applications urgently require spatial reasoning capabilities for disaster response and urban planning. This paper bridges these gaps by introducing the first framework for world modeling in remote sensing. We formulate remote sensing world modeling as direction-conditioned spatial extrapolation, where models generate semantically consistent adjacent image tiles given a central observation and directional instruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing World-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks across four scenarios: general, flood, urban, and rural. RSWISE combines visual fidelity assessment with instruction compliance evaluation using GPT-4o as a semantic judge, ensuring models genuinely perform spatial reasoning rather than simple replication. Afterwards, we present RemoteBAGEL, a unified multimodal model fine-tuned on remote sensing data for spatial extrapolation tasks. Extensive experiments demonstrate that RemoteBAGEL consistently outperforms state-of-the-art baselines on RSWISE.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2505.02228.pdf' target='_blank'>https://arxiv.org/pdf/2505.02228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangzhe Li, Zhiao Huang, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02228">Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2503.07338.pdf' target='_blank'>https://arxiv.org/pdf/2503.07338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07338">Temporal Triplane Transformers as Occupancy World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models aim to learn or construct representations of the environment that enable the prediction of future scenes, thereby supporting intelligent motion planning. However, existing models often struggle to produce fine-grained predictions and to operate in real time. In this work, we propose T$^3$Former, a novel 4D occupancy world model for autonomous driving. T$^3$Former begins by pre-training a compact {\em triplane} representation that efficiently encodes 3D occupancy. It then extracts multi-scale temporal motion features from historical triplanes and employs an autoregressive approach to iteratively predict future triplane changes. Finally, these triplane changes are combined with previous states to decode future occupancy and ego-motion trajectories. Experimental results show that T$^3$Former achieves 1.44$\times$ speedup (26 FPS), improves mean IoU to 36.09, and reduces mean absolute planning error to 1.0 meters. Demos are available in the supplementary material.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2503.07338.pdf' target='_blank'>https://arxiv.org/pdf/2503.07338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07338">Delta-Triplane Transformers as Occupancy World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occupancy World Models (OWMs) aim to predict future scenes via 3D voxelized representations of the environment to support intelligent motion planning. Existing approaches typically generate full future occupancy states from VAE-style latent encodings, which can be computationally expensive and redundant. We propose Delta-Triplane Transformers (DTT), a novel 4D OWM for autonomous driving, that introduces two key innovations: (1) a triplane based representation that encodes 3D occupancy more compactly than previous approaches, and (2) an incremental prediction strategy for OWM that models {\em changes} in occupancy rather than dealing with full states. The core insight is that changes in the compact 3D latent space are naturally sparser and easier to model, enabling higher accuracy with a lighter-weight architecture. Building on this representation, DTT extracts multi-scale motion features from historical data and iteratively predict future triplane deltas. These deltas are combined with past states to decode future occupancy and ego-motion trajectories. Extensive experiments demonstrate that DTT delivers a 1.44$\times$ speedup (26 FPS) over the state of the art, improves mean IoU to 30.85, and reduces the mean absolute planning error to 1.0 meters. Demo videos are provided in the supplementary material.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2503.07338.pdf' target='_blank'>https://arxiv.org/pdf/2503.07338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07338">Delta-Triplane Transformers as Occupancy World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Occupancy World Models (OWMs) aim to predict future scenes via 3D voxelized representations of the environment to support intelligent motion planning. Existing approaches typically generate full future occupancy states from VAE-style latent encodings, which can be computationally expensive and redundant. We propose Delta-Triplane Transformers (DTT), a novel 4D OWM for autonomous driving, that introduces two key innovations: (1) a triplane based representation that encodes 3D occupancy more compactly than previous approaches, and (2) an incremental prediction strategy for OWM that models {\em changes} in occupancy rather than dealing with full states. The core insight is that changes in the compact 3D latent space are naturally sparser and easier to model, enabling higher accuracy with a lighter-weight architecture. Building on this representation, DTT extracts multi-scale motion features from historical data and iteratively predict future triplane deltas. These deltas are combined with past states to decode future occupancy and ego-motion trajectories. Extensive experiments demonstrate that DTT delivers a 1.44$\times$ speedup (26 FPS) over the state of the art, improves mean IoU to 30.85, and reduces the mean absolute planning error to 1.0 meters. Demo videos are provided in the supplementary material.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2501.10476.pdf' target='_blank'>https://arxiv.org/pdf/2501.10476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Katherine M. Collins, Umang Bhatt, Ilia Sucholutsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10476">Revisiting Rogers' Paradox in the Context of Human-AI Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans learn about the world, and how to act in the world, in many ways: from individually conducting experiments to observing and reproducing others' behavior. Different learning strategies come with different costs and likelihoods of successfully learning more about the world. The choice that any one individual makes of how to learn can have an impact on the collective understanding of a whole population if people learn from each other. Alan Rogers developed simulations of a population of agents to study these network phenomena where agents could individually or socially learn amidst a dynamic, uncertain world and uncovered a confusing result: the availability of cheap social learning yielded no benefit to population fitness over individual learning. This paradox spawned decades of work trying to understand and uncover factors that foster the relative benefit of social learning that centuries of human behavior suggest exists. What happens in such network models now that humans can socially learn from AI systems that are themselves socially learning from us? We revisit Rogers' Paradox in the context of human-AI interaction to probe a simplified network of humans and AI systems learning together about an uncertain world. We propose and examine the impact of several learning strategies on the quality of the equilibrium of a society's 'collective world model'. We consider strategies that can be undertaken by various stakeholders involved in a single human-AI interaction: human, AI model builder, and society or regulators around the interaction. We then consider possible negative feedback loops that may arise from humans learning socially from AI: that learning from the AI may impact our own ability to learn about the world. We close with open directions into studying networks of human and AI systems that can be explored in enriched versions of our simulation framework.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2411.01342.pdf' target='_blank'>https://arxiv.org/pdf/2411.01342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emiliyan Gospodinov, Vaisakh Shaj, Philipp Becker, Stefan Geyer, Gerhard Neumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01342">Adaptive World Models: Learning Behaviors by Latent Imagination Under Non-Stationarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing foundational world models is a key research direction for embodied intelligence, with the ability to adapt to non-stationary environments being a crucial criterion. In this work, we introduce a new formalism, Hidden Parameter-POMDP, designed for control with adaptive world models. We demonstrate that this approach enables learning robust behaviors across a variety of non-stationary RL benchmarks. Additionally, this formalism effectively learns task abstractions in an unsupervised manner, resulting in structured, task-aware latent spaces.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2410.10859.pdf' target='_blank'>https://arxiv.org/pdf/2410.10859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Zeng, Yingyu Shan, Zeming Liu, Jiashu Yao, Yuhang Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10859">FAME: Towards Factual Multi-Task Model Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate knowledge within LLMs in a cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of the previous datasets only contain fabricated data in a single format, which diverges from real-world model editing scenarios, raising doubts about their usability in practice. To facilitate the application of model editing in real-world scenarios, we propose the challenge of practicality. To resolve such challenges and effectively enhance the capabilities of LLMs, we present FAME, an factual, comprehensive, and multi-task dataset, which is designed to enhance the practicality of model editing. We then propose SKEME, a model editing method that uses a novel caching mechanism to ensure synchronization with the real world. The experiments demonstrate that SKEME performs excellently across various tasks and scenarios, confirming its practicality.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2406.19800.pdf' target='_blank'>https://arxiv.org/pdf/2406.19800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William F. Whitney, Jacob Varley, Deepali Jain, Krzysztof Choromanski, Sumeet Singh, Vikas Sindhwani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19800">Modeling the Real World with High-Density Visual Particle Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present High-Density Visual Particle Dynamics (HD-VPD), a learned world model that can emulate the physical dynamics of real scenes by processing massive latent point clouds containing 100K+ particles. To enable efficiency at this scale, we introduce a novel family of Point Cloud Transformers (PCTs) called Interlacers leveraging intertwined linear-attention Performer layers and graph-based neighbour attention layers. We demonstrate the capabilities of HD-VPD by modeling the dynamics of high degree-of-freedom bi-manual robots with two RGB-D cameras. Compared to the previous graph neural network approach, our Interlacer dynamics is twice as fast with the same prediction quality, and can achieve higher quality using 4x as many particles. We illustrate how HD-VPD can evaluate motion plan quality with robotic box pushing and can grasping tasks. See videos and particle dynamics rendered by HD-VPD at https://sites.google.com/view/hd-vpd.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2405.05890.pdf' target='_blank'>https://arxiv.org/pdf/2405.05890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yarden As, Bhavya Sukhija, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05890">Safe Exploration Using Bayesian World Models and Log-Barrier Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in deploying reinforcement learning in online tasks is ensuring that safety is maintained throughout the learning process. In this work, we propose CERL, a new method for solving constrained Markov decision processes while keeping the policy safe during learning. Our method leverages Bayesian world models and suggests policies that are pessimistic w.r.t. the model's epistemic uncertainty. This makes CERL robust towards model inaccuracies and leads to safe exploration during learning. In our experiments, we demonstrate that CERL outperforms the current state-of-the-art in terms of safety and optimality in solving CMDPs from image observations.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2310.18239.pdf' target='_blank'>https://arxiv.org/pdf/2310.18239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Yang, Neel P. Bhatt, Tyler Ingebrand, William Ward, Steven Carr, Zhangyang Wang, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18239">Fine-Tuning Language Models Using Formal Methods Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily in autonomous driving, to demonstrate the method's effectiveness across multiple tasks. The results indicate an improvement in percentage of specifications satisfied by the controller from 60% to 90%.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2306.12672.pdf' target='_blank'>https://arxiv.org/pdf/2306.12672.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka, Jacob Andreas, Joshua B. Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12672">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2510.02387.pdf' target='_blank'>https://arxiv.org/pdf/2510.02387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>FAIR CodeGen team, Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estapé, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, François Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazaré, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02387">CWM: An Open-Weights LLM for Research on Code Generation with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2509.03887.pdf' target='_blank'>https://arxiv.org/pdf/2509.03887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bu Jin, Songen Gu, Xiaotao Hu, Yupeng Zheng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Wei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03887">OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose OccTENS, a generative occupancy world model that enables controllable, high-fidelity long-term occupancy generation while maintaining computational efficiency. Different from visual generation, the occupancy world model must capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models. Recent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from \textbf{inefficiency}, \textbf{temporal degradation} in long-term generation and \textbf{lack of controllability}. To holistically address these issues, we reformulate the occupancy world model as a temporal next-scale prediction (TENS) task, which decomposes the temporal sequence modeling problem into the modeling of spatial scale-by-scale generation and temporal scene-by-scene prediction. With a \textbf{TensFormer}, OccTENS can effectively manage the temporal causality and spatial relationships of occupancy sequences in a flexible and scalable way. To enhance the pose controllability, we further propose a holistic pose aggregation strategy, which features a unified sequence modeling for occupancy and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art method with both higher occupancy quality and faster inference time.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2509.03887.pdf' target='_blank'>https://arxiv.org/pdf/2509.03887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bu Jin, Songen Gu, Xiaotao Hu, Yupeng Zheng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Wei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03887">OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose OccTENS, a generative occupancy world model that enables controllable, high-fidelity long-term occupancy generation while maintaining computational efficiency. Different from visual generation, the occupancy world model must capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models. Recent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from \textbf{inefficiency}, \textbf{temporal degradation} in long-term generation and \textbf{lack of controllability}. To holistically address these issues, we reformulate the occupancy world model as a temporal next-scale prediction (TENS) task, which decomposes the temporal sequence modeling problem into the modeling of spatial scale-by-scale generation and temporal scene-by-scene prediction. With a \textbf{TensFormer}, OccTENS can effectively manage the temporal causality and spatial relationships of occupancy sequences in a flexible and scalable way. To enhance the pose controllability, we further propose a holistic pose aggregation strategy, which features a unified sequence modeling for occupancy and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art method with both higher occupancy quality and faster inference time.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2508.16876.pdf' target='_blank'>https://arxiv.org/pdf/2508.16876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhao, Xiaoyu Wang, Dan Wang, Zhonglin Jiang, Qingqing Gu, Teng Chen, Ningyuan Xi, Jinxian Qu, Yong Chen, Luo Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16876">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited. In this paper, we construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances. By defining a POMDP, we argue emotion, sentiment and intention can be modeled as the user belief and solved by maximizing the information bottleneck. By this user belief modeling, we apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB. Experiments show that the pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model. Further analysis shows that this manner holds a reasonable exploration-exploitation balance and also transfers well to out-of-domain scenarios such as empathetic dialogues.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2508.16876.pdf' target='_blank'>https://arxiv.org/pdf/2508.16876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhao, Xiaoyu Wang, Dan Wang, Zhonglin Jiang, Qingqing Gu, Teng Chen, Ningyuan Xi, Jinxian Qu, Yong Chen, Luo Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16876">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited. In this paper, we construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances. By defining a POMDP, we argue emotion, sentiment and intention can be modeled as the user belief and solved by maximizing the information bottleneck. By this user belief modeling, we apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB. Experiments show that the pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model. Further analysis shows that this manner holds a reasonable exploration-exploitation balance and also transfers well to out-of-domain scenarios such as empathetic dialogues.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2508.16876.pdf' target='_blank'>https://arxiv.org/pdf/2508.16876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhao, Xiaoyu Wang, Dan Wang, Zhonglin Jiang, Qingqing Gu, Teng Chen, Ningyuan Xi, Jinxian Qu, Yong Chen, Luo Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16876">Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited. In this paper, we construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances. By defining a POMDP, we argue emotion, sentiment and intention can be modeled as the user belief and solved by maximizing the information bottleneck. By this user belief modeling, we apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB. Experiments show that the pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model. Further analysis shows that this manner holds a reasonable exploration-exploitation balance and also transfers well to out-of-domain scenarios such as empathetic dialogues.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2412.03568.pdf' target='_blank'>https://arxiv.org/pdf/2412.03568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, Hongyang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03568">The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments. Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences. Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible. For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources. This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2410.08368.pdf' target='_blank'>https://arxiv.org/pdf/2410.08368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wilson Yan, Volodymyr Mnih, Aleksandra Faust, Matei Zaharia, Pieter Abbeel, Hao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08368">ElasticTok: Adaptive Tokenization for Image and Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient video tokenization remains a key bottleneck in learning general purpose vision models that are capable of processing long video sequences. Prevailing approaches are restricted to encoding videos to a fixed number of tokens, where too few tokens will result in overly lossy encodings, and too many tokens will result in prohibitively long sequence lengths. In this work, we introduce ElasticTok, a method that conditions on prior frames to adaptively encode a frame into a variable number of tokens. To enable this in a computationally scalable way, we propose a masking technique that drops a random number of tokens at the end of each frames's token encoding. During inference, ElasticTok can dynamically allocate tokens when needed -- more complex data can leverage more tokens, while simpler data only needs a few tokens. Our empirical evaluations on images and video demonstrate the effectiveness of our approach in efficient token usage, paving the way for future development of more powerful multimodal models, world models, and agents.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2406.15132.pdf' target='_blank'>https://arxiv.org/pdf/2406.15132.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxin Yang, Wanling Gao, Luzhou Peng, Yunyou Huang, Fei Tang, Jianfeng Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15132">Younger: The First Dataset for Artificial Intelligence-Generated Neural Network Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing and optimizing neural network architectures typically requires extensive expertise, starting with handcrafted designs and then manual or automated refinement. This dependency presents a significant barrier to rapid innovation. Recognizing the complexity of automatically generating neural network architecture from scratch, we introduce Younger, a pioneering dataset to advance this ambitious goal. Derived from over 174K real-world models across more than 30 tasks from various public model hubs, Younger includes 7,629 unique architectures, and each is represented as a directed acyclic graph with detailed operator-level information. The dataset facilitates two primary design paradigms: global, for creating complete architectures from scratch, and local, for detailed architecture component refinement. By establishing these capabilities, Younger contributes to a new frontier, Artificial Intelligence-Generated Neural Network Architecture (AIGNNA). Our experiments explore the potential and effectiveness of Younger for automated architecture generation and, as a secondary benefit, demonstrate that Younger can serve as a benchmark dataset, advancing the development of graph neural networks. We release the dataset and code publicly to lower the entry barriers and encourage further research in this challenging area.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2406.09976.pdf' target='_blank'>https://arxiv.org/pdf/2406.09976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siemen Herremans, Ali Anwar, Siegfried Mercelis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09976">Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has demonstrated impressive performance in various challenging problems such as robotics, board games, and classical arcade games. However, its real-world applications can be hindered by the absence of robustness and safety in the learned policies. More specifically, an RL agent that trains in a certain Markov decision process (MDP) often struggles to perform well in nearly identical MDPs. To address this issue, we employ the framework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel learned transition model. Our method specifically incorporates an auxiliary pessimistic model, updated adversarially, to estimate the worst-case MDP within a Kullback-Leibler uncertainty set. In comparison to several existing works, our work does not impose any additional conditions on the training environment, such as the need for a parametric simulator. To test the effectiveness of the proposed pessimistic model in enhancing policy robustness, we integrate it into a practical RL algorithm, called Robust Model-Based Policy Optimization (RMBPO). Our experimental results indicate a notable improvement in policy robustness on high-dimensional MuJoCo control tasks, with the auxiliary model enhancing the performance of the learned policy in distorted MDPs. We further explore the learned deviation between the proposed auxiliary world model and the nominal model, to examine how pessimism is achieved. By learning a pessimistic world model and demonstrating its role in improving policy robustness, our research contributes towards making (model-based) RL more robust.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2402.08268.pdf' target='_blank'>https://arxiv.org/pdf/2402.08268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08268">World Model on Million-Length Video And Language With Blockwise RingAttention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we aim to address these challenges by providing a comprehensive exploration of the full development process for producing 1M context language models and video-language models, setting new benchmarks in language retrieval and new capabilities in long video understanding. We detail our long context data curation process, progressive context extension from 4K to 1M tokens, and present an efficient open-source implementation for scalable training on long sequences. Additionally, we open-source a family of 7B parameter models capable of processing long text documents and videos exceeding 1M tokens.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2311.11762.pdf' target='_blank'>https://arxiv.org/pdf/2311.11762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Bogdoll, Yitian Yang, Tim Joseph, Melih Yazgan, J. Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11762">MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models for autonomous driving have the potential to dramatically improve the reasoning capabilities of today's systems. However, most works focus on camera data, with only a few that leverage lidar data or combine both to better represent autonomous vehicle sensor setups. In addition, raw sensor predictions are less actionable than 3D occupancy predictions, but there are no works examining the effects of combining both multimodal sensor data and 3D occupancy prediction. In this work, we perform a set of experiments with a MUltimodal World Model with Geometric VOxel representations (MUVO) to evaluate different sensor fusion strategies to better understand the effects on sensor data prediction. We also analyze potential weaknesses of current sensor fusion approaches and examine the benefits of additionally predicting 3D occupancy.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2310.05337.pdf' target='_blank'>https://arxiv.org/pdf/2310.05337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05337">What do larger image classifiers memorise?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of modern neural networks has prompted study of the connection between memorisation and generalisation: overparameterised models generalise well, despite being able to perfectly fit (memorise) completely random labels. To carefully study this issue, Feldman proposed a metric to quantify the degree of memorisation of individual training examples, and empirically computed the corresponding memorisation profile of a ResNet on image classification bench-marks. While an exciting first glimpse into what real-world models memorise, this leaves open a fundamental question: do larger neural models memorise more? We present a comprehensive empirical analysis of this question on image classification benchmarks. We find that training examples exhibit an unexpectedly diverse set of memorisation trajectories across model sizes: most samples experience decreased memorisation under larger models, while the rest exhibit cap-shaped or increasing memorisation. We show that various proxies for the Feldman memorization score fail to capture these fundamental trends. Lastly, we find that knowledge distillation, an effective and popular model compression technique, tends to inhibit memorisation, while also improving generalisation. Specifically, memorisation is mostly inhibited on examples with increasing memorisation trajectories, thus pointing at how distillation improves generalisation.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2308.05701.pdf' target='_blank'>https://arxiv.org/pdf/2308.05701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Bogdoll, Lukas Bosch, Tim Joseph, Helen Gremmelmaier, Yitian Yang, J. Marius ZÃ¶llner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05701">Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years there have been remarkable advancements in autonomous driving. While autonomous vehicles demonstrate high performance in closed-set conditions, they encounter difficulties when confronted with unexpected situations. At the same time, world models emerged in the field of model-based reinforcement learning as a way to enable agents to predict the future depending on potential actions. This led to outstanding results in sparse reward and complex control tasks. This work provides an overview of how world models can be leveraged to perform anomaly detection in the domain of autonomous driving. We provide a characterization of world models and relate individual components to previous works in anomaly detection to facilitate further research in the field.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2206.14244.pdf' target='_blank'>https://arxiv.org/pdf/2206.14244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.14244">Masked World Models for Visual Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9%. Code is available on the project website: https://sites.google.com/view/mwm-rl.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2510.02387.pdf' target='_blank'>https://arxiv.org/pdf/2510.02387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>FAIR CodeGen team, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estapé, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, François Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazaré, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02387">CWM: An Open-Weights LLM for Research on Code Generation with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2510.02387.pdf' target='_blank'>https://arxiv.org/pdf/2510.02387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>FAIR CodeGen team, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, David Zhang, Kunhao Zheng, Jordi Armengol-Estapé, Pedram Bashiri, Maximilian Beck, Pierre Chambon, Abhishek Charnalia, Chris Cummins, Juliette Decugis, Zacharias V. Fisches, François Fleuret, Fabian Gloeckle, Alex Gu, Michael Hassid, Daniel Haziza, Badr Youbi Idrissi, Christian Keller, Rahul Kindi, Hugh Leather, Gallil Maimon, Aram Markosyan, Francisco Massa, Pierre-Emmanuel Mazaré, Vegard Mella, Naila Murray, Keyur Muzumdar, Peter O'Hearn, Matteo Pagliardini, Dmitrii Pedchenko, Tal Remez, Volker Seeker, Marco Selvi, Oren Sultan, Sida Wang, Luca Wehrstedt, Ori Yoran, Lingming Zhang, Taco Cohen, Yossi Adi, Gabriel Synnaeve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02387">CWM: An Open-Weights LLM for Research on Code Generation with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2509.24948.pdf' target='_blank'>https://arxiv.org/pdf/2509.24948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjin Xiao, Yandan Yang, Xinyuan Chang, Ronghan Chen, Feng Xiong, Mu Xu, Wei-Shi Zheng, Qing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24948">World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2509.24948.pdf' target='_blank'>https://arxiv.org/pdf/2509.24948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjin Xiao, Yandan Yang, Xinyuan Chang, Ronghan Chen, Feng Xiong, Mu Xu, Wei-Shi Zheng, Qing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24948">World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models trained via imitation learning suffer from significant performance degradation in data-scarce scenarios due to their reliance on large-scale demonstration datasets. Although reinforcement learning (RL)-based post-training has proven effective in addressing data scarcity, its application to VLA models is hindered by the non-resettable nature of real-world environments. This limitation is particularly critical in high-risk domains such as industrial automation, where interactions often induce state changes that are costly or infeasible to revert. Furthermore, existing VLA approaches lack a reliable mechanism for detecting task completion, leading to redundant actions that reduce overall task success rates. To address these challenges, we propose World-Env, an RL-based post-training framework that replaces physical interaction with a low-cost, world model-based virtual simulator. World-Env consists of two key components: (1) a video-based world simulator that generates temporally consistent future visual observations, and (2) a vision-language model (VLM)-guided instant reflector that provides continuous reward signals and predicts action termination. This simulated environment enables VLA models to safely explore and generalize beyond their initial imitation learning distribution. Our method achieves notable performance gains with as few as five expert demonstrations per task. Experiments on complex robotic manipulation tasks demonstrate that World-Env effectively overcomes the data inefficiency, safety constraints, and inefficient execution of conventional VLA models that rely on real-world interaction, offering a practical and scalable solution for post-training in resource-constrained settings.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2509.22643.pdf' target='_blank'>https://arxiv.org/pdf/2509.22643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22643">VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2509.22643.pdf' target='_blank'>https://arxiv.org/pdf/2509.22643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22643">VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2509.03956.pdf' target='_blank'>https://arxiv.org/pdf/2509.03956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjong Yoo, Jinwoo Jang, Sihyung Yoon, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03956">World Model Implanting for Test-time Adaptation of Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.03956.pdf' target='_blank'>https://arxiv.org/pdf/2509.03956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjong Yoo, Jinwoo Jang, Sihyung Yoon, Honguk Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03956">World Model Implanting for Test-time Adaptation of Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In embodied AI, a persistent challenge is enabling agents to robustly adapt to novel domains without requiring extensive data collection or retraining. To address this, we present a world model implanting framework (WorMI) that combines the reasoning capabilities of large language models (LLMs) with independently learned, domain-specific world models through test-time composition. By allowing seamless implantation and removal of the world models, the embodied agent's policy achieves and maintains cross-domain adaptability. In the WorMI framework, we employ a prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching, to incorporate relevant models into test-time composition. We also develop a world-wise compound attention method that not only integrates the knowledge from the retrieved world models but also aligns their intermediate representations with the reasoning model's representation within the agent's policy. This framework design effectively fuses domain-specific knowledge from multiple world models, ensuring robust adaptation to unseen domains. We evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating superior zero-shot and few-shot performance compared to several LLM-based approaches across a range of unseen domains. These results highlight the frameworks potential for scalable, real-world deployment in embodied agent scenarios where adaptability and data efficiency are essential.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2507.23773.pdf' target='_blank'>https://arxiv.org/pdf/2507.23773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingkai Deng, Jinyu Hou, Yilin Shen, Hongxia Jin, Graham Neubig, Zhiting Hu, Eric Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23773">SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \modelname improves the success of flight search from 0\% to 32.2\%. World-model-based planning, in particular, shows consistent advantage of up to 124\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \modelname with pretrained LLMs, available as a research demo for public testing.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2507.21513.pdf' target='_blank'>https://arxiv.org/pdf/2507.21513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenneth Li, Fernanda ViÃ©gas, Martin Wattenberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21513">What Does it Mean for a Neural Network to Learn a "World Model"?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a set of precise criteria for saying a neural net learns and uses a "world model." The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent "state space" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a "world model" is not a trivial consequence of the neural net's data or task.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2505.22976.pdf' target='_blank'>https://arxiv.org/pdf/2505.22976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kewei Lian, Shaofei Cai, Yilun Du, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22976">Toward Memory-Aided World Models: Benchmarking via Spatial Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to simulate the world in a spatially consistent manner is a crucial requirements for effective world models. Such a model enables high-quality visual generation, and also ensures the reliability of world models for downstream tasks such as simulation and planning. Designing a memory module is a crucial component for addressing spatial consistency: such a model must not only retain long-horizon observational information, but also enables the construction of explicit or implicit internal spatial representations. However, there are no dataset designed to promote the development of memory modules by explicitly enforcing spatial consistency constraints. Furthermore, most existing benchmarks primarily emphasize visual coherence or generation quality, neglecting the requirement of long-range spatial consistency. To bridge this gap, we construct a dataset and corresponding benchmark by sampling 150 distinct locations within the open-world environment of Minecraft, collecting about 250 hours (20 million frames) of loop-based navigation videos with actions. Our dataset follows a curriculum design of sequence lengths, allowing models to learn spatial consistency on increasingly complex navigation trajectories. Furthermore, our data collection pipeline is easily extensible to new Minecraft environments and modules. Four representative world model baselines are evaluated on our benchmark. Dataset, benchmark, and code are open-sourced to support future research.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2505.17685.pdf' target='_blank'>https://arxiv.org/pdf/2505.17685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17685">FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language models (VLMs) have attracted increasing interest in autonomous driving due to their powerful reasoning capabilities. However, existing VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored to the current scenario, which essentially represents highly abstract and symbolic compression of visual information, potentially leading to spatio-temporal relationship ambiguity and fine-grained information loss. Is autonomous driving better modeled on real-world simulation and imagination than on pure symbolic logic? In this paper, we propose a spatio-temporal CoT reasoning method that enables models to think visually. First, VLM serves as a world model to generate unified image frame for predicting future world states: where perception results (e.g., lane divider and 3D detection) represent the future spatial relationships, and ordinary future frame represent the temporal evolution relationships. This spatio-temporal CoT then serves as intermediate reasoning steps, enabling the VLM to function as an inverse dynamics model for trajectory planning based on current observations and future predictions. To implement visual generation in VLMs, we propose a unified pretraining paradigm integrating visual generation and understanding, along with a progressive visual CoT enhancing autoregressive image generation. Extensive experimental results demonstrate the effectiveness of the proposed method, advancing autonomous driving towards visual reasoning.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2504.07257.pdf' target='_blank'>https://arxiv.org/pdf/2504.07257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elisabeth Dillies, Quentin Delfosse, Jannis BlÃ¼ml, Raban Emunds, Florian Peter Busch, Kristian Kersting
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07257">Better Decisions through the Right Causal World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability.
  By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2411.18289.pdf' target='_blank'>https://arxiv.org/pdf/2411.18289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minheng Ni, Lei Zhang, Zihan Chen, Kaixin Bai, Zhaopeng Chen, Jianwei Zhang, Lei Zhang, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18289">Don't Let Your Robot be Harmful: Responsible Robotic Manipulation via Safety-as-Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unthinking execution of human instructions in robotic manipulation can lead to severe safety risks, such as poisonings, fires, and even explosions. In this paper, we present responsible robotic manipulation, which requires robots to consider potential hazards in the real-world environment while completing instructions and performing complex operations safely and efficiently. However, such scenarios in real world are variable and risky for training. To address this challenge, we propose Safety-as-policy, which includes (i) a world model to automatically generate scenarios containing safety risks and conduct virtual interactions, and (ii) a mental model to infer consequences with reflections and gradually develop the cognition of safety, allowing robots to accomplish tasks while avoiding dangers. Additionally, we create the SafeBox synthetic dataset, which includes one hundred responsible robotic manipulation tasks with different safety risk scenarios and instructions, effectively reducing the risks associated with real-world experiments. Experiments demonstrate that Safety-as-policy can avoid risks and efficiently complete tasks in both synthetic dataset and real-world experiments, significantly outperforming baseline methods. Our SafeBox dataset shows consistent evaluation results with real-world scenarios, serving as a safe and effective benchmark for future research.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2411.16262.pdf' target='_blank'>https://arxiv.org/pdf/2411.16262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathis Immertreu, Achim Schilling, Andreas Maier, Patrick Krauss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16262">Probing for Consciousness in Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study explores the potential for artificial agents to develop core consciousness, as proposed by Antonio Damasio's theory of consciousness. According to Damasio, the emergence of core consciousness relies on the integration of a self model, informed by representations of emotions and feelings, and a world model. We hypothesize that an artificial agent, trained via reinforcement learning (RL) in a virtual environment, can develop preliminary forms of these models as a byproduct of its primary task. The agent's main objective is to learn to play a video game and explore the environment. To evaluate the emergence of world and self models, we employ probes-feedforward classifiers that use the activations of the trained agent's neural networks to predict the spatial positions of the agent itself. Our results demonstrate that the agent can form rudimentary world and self models, suggesting a pathway toward developing machine consciousness. This research provides foundational insights into the capabilities of artificial agents in mirroring aspects of human consciousness, with implications for future advancements in artificial intelligence.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2411.12967.pdf' target='_blank'>https://arxiv.org/pdf/2411.12967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunuo Zhang, Baiting Luo, Ayan Mukhopadhyay, Daniel Stojcsics, Daniel Elenius, Anirban Roy, Susmit Jha, Miklos Maroti, Xenofon Koutsoukos, Gabor Karsai, Abhishek Dubey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12967">Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient path optimization for drones in search and rescue operations faces challenges, including limited visibility, time constraints, and complex information gathering in urban environments. We present a comprehensive approach to optimize UAV-based search and rescue operations in neighborhood areas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path planning problem is formulated as a partially observable Markov decision process (POMDP), and we propose a novel ``Shrinking POMCP'' approach to address time constraints. In the AirSim environment, we integrate our approach with a probabilistic world model for belief maintenance and a neurosymbolic navigator for obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with equivalent functionality. We compare trajectories generated by different approaches in the 2D simulator and evaluate performance across various belief types in the 3D AirSim-ROS simulator. Experimental results from both simulators demonstrate that our proposed shrinking POMCP solution achieves significant improvements in search times compared to alternative methods, showcasing its potential for enhancing the efficiency of UAV-assisted search and rescue operations.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2411.00927.pdf' target='_blank'>https://arxiv.org/pdf/2411.00927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vardhan Dongre, Xiaocheng Yang, Emre Can Acikgoz, Suvodip Dey, Gokhan Tur, Dilek Hakkani-TÃ¼r
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00927">ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2410.03136.pdf' target='_blank'>https://arxiv.org/pdf/2410.03136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Xiong, Ali Payani, Yuan Yang, Faramarz Fekri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03136">Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2406.09455.pdf' target='_blank'>https://arxiv.org/pdf/2406.09455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, Zhengzhong Liu, Eric P. Xing, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09455">Pandora: Towards General World Model with Natural Language Actions and Video States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models simulate future states of the world in response to different actions. They facilitate interactive content creation and provides a foundation for grounded, long-horizon reasoning. Current foundation models do not fully meet the capabilities of general world models: large language models (LLMs) are constrained by their reliance on language modality and their limited understanding of the physical world, while video models lack interactive action control over the world simulations. This paper makes a step towards building a general world model by introducing Pandora, a hybrid autoregressive-diffusion model that simulates world states by generating videos and allows real-time control with free-text actions. Pandora achieves domain generality, video consistency, and controllability through large-scale pretraining and instruction tuning. Crucially, Pandora bypasses the cost of training-from-scratch by integrating a pretrained LLM (7B) and a pretrained video model, requiring only additional lightweight finetuning. We illustrate extensive outputs by Pandora across diverse domains (indoor/outdoor, natural/urban, human/robot, 2D/3D, etc.). The results indicate great potential of building stronger general world models with larger-scale training.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2509.24241.pdf' target='_blank'>https://arxiv.org/pdf/2509.24241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungwook Kim, Seunghyeon Lee, Minsu Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24241">FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic robot videos from explicit action trajectories is a critical step toward building effective world models and robotics foundation models. We introduce two training-free, inference-time techniques that fully exploit explicit action parameters in diffusion-based robot video generation. Instead of treating action vectors as passive conditioning signals, our methods actively incorporate them to guide both the classifier-free guidance process and the initialization of Gaussian latents. First, action-scaled classifier-free guidance dynamically modulates guidance strength in proportion to action magnitude, enhancing controllability over motion intensity. Second, action-scaled noise truncation adjusts the distribution of initially sampled noise to better align with the desired motion dynamics. Experiments on real robot manipulation datasets demonstrate that these techniques significantly improve action coherence and visual quality across diverse robot environments.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2509.24241.pdf' target='_blank'>https://arxiv.org/pdf/2509.24241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungwook Kim, Seunghyeon Lee, Minsu Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24241">FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating realistic robot videos from explicit action trajectories is a critical step toward building effective world models and robotics foundation models. We introduce two training-free, inference-time techniques that fully exploit explicit action parameters in diffusion-based robot video generation. Instead of treating action vectors as passive conditioning signals, our methods actively incorporate them to guide both the classifier-free guidance process and the initialization of Gaussian latents. First, action-scaled classifier-free guidance dynamically modulates guidance strength in proportion to action magnitude, enhancing controllability over motion intensity. Second, action-scaled noise truncation adjusts the distribution of initially sampled noise to better align with the desired motion dynamics. Experiments on real robot manipulation datasets demonstrate that these techniques significantly improve action coherence and visual quality across diverse robot environments.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2509.22353.pdf' target='_blank'>https://arxiv.org/pdf/2509.22353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Wang, Zhiyuan Chen, Yuxuan Zhong, Sunjian Zheng, Pengtao Shao, Bo Yu, Shaoshan Liu, Jianan Wang, Ning Ding, Yang Cao, Yu Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22353">Context and Diversity Matter: The Emergence of In-Context Learning in World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capability of predicting environmental dynamics underpins both biological neural systems and general embodied AI in adapting to their surroundings. Yet prevailing approaches rest on static world models that falter when confronted with novel or rare configurations. We investigate in-context environment learning (ICEL), shifting attention from zero-shot performance to the growth and asymptotic limits of the world model. Our contributions are three-fold: (1) we formalize in-context learning of a world model and identify two core mechanisms: environment recognition and environment learning; (2) we derive error upper-bounds for both mechanisms that expose how the mechanisms emerge; and (3) we empirically confirm that distinct ICL mechanisms exist in the world model, and we further investigate how data distribution and model architecture affect ICL in a manner consistent with theory. These findings demonstrate the potential of self-adapting world models and highlight the key factors behind the emergence of ICEL, most notably the necessity of long context and diverse environments.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2509.22353.pdf' target='_blank'>https://arxiv.org/pdf/2509.22353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Wang, Zhiyuan Chen, Yuxuan Zhong, Sunjian Zheng, Pengtao Shao, Bo Yu, Shaoshan Liu, Jianan Wang, Ning Ding, Yang Cao, Yu Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22353">Context and Diversity Matter: The Emergence of In-Context Learning in World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capability of predicting environmental dynamics underpins both biological neural systems and general embodied AI in adapting to their surroundings. Yet prevailing approaches rest on static world models that falter when confronted with novel or rare configurations. We investigate in-context environment learning (ICEL), shifting attention from zero-shot performance to the growth and asymptotic limits of the world model. Our contributions are three-fold: (1) we formalize in-context learning of a world model and identify two core mechanisms: environment recognition and environment learning; (2) we derive error upper-bounds for both mechanisms that expose how the mechanisms emerge; and (3) we empirically confirm that distinct ICL mechanisms exist in the world model, and we further investigate how data distribution and model architecture affect ICL in a manner consistent with theory. These findings demonstrate the potential of self-adapting world models and highlight the key factors behind the emergence of ICEL, most notably the necessity of long context and diverse environments.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2509.13341.pdf' target='_blank'>https://arxiv.org/pdf/2509.13341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet H. GÃ¼zel, Matthew Thomas Jackson, Jarek Luca Liesen, Tim RocktÃ¤schel, Jakob Nicolaus Foerster, Ilija Bogunovic, Jack Parker-Holder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13341">Imagined Autocurricula</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2509.13341.pdf' target='_blank'>https://arxiv.org/pdf/2509.13341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet H. GÃ¼zel, Matthew Thomas Jackson, Jarek Luca Liesen, Tim RocktÃ¤schel, Jakob Nicolaus Foerster, Ilija Bogunovic, Jack Parker-Holder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13341">Imagined Autocurricula</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2505.01458.pdf' target='_blank'>https://arxiv.org/pdf/2505.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01458">A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2502.20168.pdf' target='_blank'>https://arxiv.org/pdf/2502.20168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20168">Accelerating Model-Based Reinforcement Learning with State-Space World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is a powerful approach for robot learning. However, model-free RL (MFRL) requires a large number of environment interactions to learn successful control policies. This is due to the noisy RL training updates and the complexity of robotic systems, which typically involve highly non-linear dynamics and noisy sensor signals. In contrast, model-based RL (MBRL) not only trains a policy but simultaneously learns a world model that captures the environment's dynamics and rewards. The world model can either be used for planning, for data collection, or to provide first-order policy gradients for training. Leveraging a world model significantly improves sample efficiency compared to model-free RL. However, training a world model alongside the policy increases the computational complexity, leading to longer training times that are often intractable for complex real-world scenarios. In this work, we propose a new method for accelerating model-based RL using state-space world models. Our approach leverages state-space models (SSMs) to parallelize the training of the dynamics model, which is typically the main computational bottleneck. Additionally, we propose an architecture that provides privileged information to the world model during training, which is particularly relevant for partially observable environments. We evaluate our method in several real-world agile quadrotor flight tasks, involving complex dynamics, for both fully and partially observable environments. We demonstrate a significant speedup, reducing the world model training time by up to 10 times, and the overall MBRL training time by up to 4 times. This benefit comes without compromising performance, as our method achieves similar sample efficiency and task rewards to state-of-the-art MBRL methods.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2410.11359.pdf' target='_blank'>https://arxiv.org/pdf/2410.11359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Hanchen Jiang, Zhi Zhang, Dinghuai Zhang, Andrew Lizarraga, Chenheng Xu, Yasi Zhang, Siyan Zhao, Zhengjie Xu, Peiyu Yu, Yuer Tang, Deqian Kong, Ying Nian Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11359">DODT: Enhanced Online Decision Transformer Learning through Dreamer's Actor-Critic Trajectory Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in reinforcement learning have led to the development of sophisticated models capable of learning complex decision-making tasks. However, efficiently integrating world models with decision transformers remains a challenge. In this paper, we introduce a novel approach that combines the Dreamer algorithm's ability to generate anticipatory trajectories with the adaptive learning strengths of the Online Decision Transformer. Our methodology enables parallel training where Dreamer-produced trajectories enhance the contextual decision-making of the transformer, creating a bidirectional enhancement loop. We empirically demonstrate the efficacy of our approach on a suite of challenging benchmarks, achieving notable improvements in sample efficiency and reward maximization over existing methods. Our results indicate that the proposed integrated framework not only accelerates learning but also showcases robustness in diverse and dynamic scenarios, marking a significant step forward in model-based reinforcement learning.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2410.08896.pdf' target='_blank'>https://arxiv.org/pdf/2410.08896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claas A Voelcker, Marcel Hussing, Eric Eaton, Amir-massoud Farahmand, Igor Gilitschenski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08896">MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD), uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2405.02336.pdf' target='_blank'>https://arxiv.org/pdf/2405.02336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walid Saad, Omar Hashash, Christo Kurisummoottil Thomas, Christina Chaccour, Merouane Debbah, Narayan Mandayam, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02336">Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building future wireless systems that support services like digital twins (DTs) is challenging to achieve through advances to conventional technologies like meta-surfaces. While artificial intelligence (AI)-native networks promise to overcome some limitations of wireless technologies, developments still rely on AI tools like neural networks. Such tools struggle to cope with the non-trivial challenges of the network environment and the growing demands of emerging use cases. In this paper, we revisit the concept of AI-native wireless systems, equipping them with the common sense necessary to transform them into artificial general intelligence (AGI)-native systems. These systems acquire common sense by exploiting different cognitive abilities such as perception, analogy, and reasoning, that enable them to generalize and deal with unforeseen scenarios. Towards developing the components of such a system, we start by showing how the perception module can be built through abstracting real-world elements into generalizable representations. These representations are then used to create a world model, founded on principles of causality and hyper-dimensional (HD) computing, that aligns with intuitive physics and enables analogical reasoning, that define common sense. Then, we explain how methods such as integrated information theory play a role in the proposed intent-driven and objective-driven planning methods that maneuver the AGI-native network to take actions. Next, we discuss how an AGI-native network can enable use cases related to human and autonomous agents: a) analogical reasoning for next-generation DTs, b) synchronized and resilient experiences for cognitive avatars, and c) brain-level metaverse experiences like holographic teleportation. Finally, we conclude with a set of recommendations to build AGI-native systems. Ultimately, we envision this paper as a roadmap for the beyond 6G era.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2404.05221.pdf' target='_blank'>https://arxiv.org/pdf/2404.05221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05221">LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2402.15391.pdf' target='_blank'>https://arxiv.org/pdf/2402.15391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim RocktÃ¤schel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15391">Genie: Generative Interactive Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2304.12502.pdf' target='_blank'>https://arxiv.org/pdf/2304.12502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christo Kurisummoottil Thomas, Walid Saad, Yong Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12502">Causal Semantic Communication for Digital Twins: A Generalizable Imitation Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A digital twin (DT) leverages a virtual representation of the physical world, along with communication (e.g., 6G), computing (e.g., edge computing), and artificial intelligence (AI) technologies to enable many connected intelligence services. In order to handle the large amounts of network data based on digital twins (DTs), wireless systems can exploit the paradigm of semantic communication (SC) for facilitating informed decision-making under strict communication constraints by utilizing AI techniques such as causal reasoning. In this paper, a novel framework called causal semantic communication (CSC) is proposed for DT-based wireless systems. The CSC system is posed as an imitation learning (IL) problem, where the transmitter, with access to optimal network control policies using a DT, teaches the receiver using SC over a bandwidth limited wireless channel how to improve its knowledge to perform optimal control actions. The causal structure in the source data is extracted using novel approaches from the framework of deep end-to-end causal inference, thereby enabling the creation of a semantic representation that is causally invariant, which in turn helps generalize the learned knowledge of the system to unseen scenarios. The CSC decoder at the receiver is designed to extract and estimate semantic information while ensuring high semantic reliability. The receiver control policies, semantic decoder, and causal inference are formulated as a bi-level optimization problem within a variational inference framework. This problem is solved using a novel concept called network state models, inspired from world models in generative AI, that faithfully represents the environment dynamics leading to data generation. Simulation results demonstrate that the proposed CSC system outperforms state-of-the-art SC systems by achieving better semantic reliability and reduced semantic representation.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2302.02408.pdf' target='_blank'>https://arxiv.org/pdf/2302.02408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02408">Multi-View Masked World Models for Visual Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure. Video demonstrations are available at: https://sites.google.com/view/mv-mwm.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2510.10125.pdf' target='_blank'>https://arxiv.org/pdf/2510.10125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjiang Guo, Lucy Xiaoyang Shi, Jianyu Chen, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10125">Ctrl-World: A Controllable Generative World Model for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalist robot policies can now perform a wide range of manipulation skills, but evaluating and improving their ability with unfamiliar objects and instructions remains a significant challenge. Rigorous evaluation requires a large number of real-world rollouts, while systematic improvement demands additional corrective data with expert labels. Both of these processes are slow, costly, and difficult to scale. World models offer a promising, scalable alternative by enabling policies to rollout within imagination space. However, a key challenge is building a controllable world model that can handle multi-step interactions with generalist robot policies. This requires a world model compatible with modern generalist policies by supporting multi-view prediction, fine-grained action control, and consistent long-horizon interactions, which is not achieved by previous works. In this paper, we make a step forward by introducing a controllable multi-view world model that can be used to evaluate and improve the instruction-following ability of generalist robot policies. Our model maintains long-horizon consistency with a pose-conditioned memory retrieval mechanism and achieves precise action control through frame-level action conditioning. Trained on the DROID dataset (95k trajectories, 564 scenes), our model generates spatially and temporally consistent trajectories under novel scenarios and new camera placements for over 20 seconds. We show that our method can accurately rank policy performance without real-world robot rollouts. Moreover, by synthesizing successful trajectories in imagination and using them for supervised fine-tuning, our approach can improve policy success by 44.7\%.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2509.24116.pdf' target='_blank'>https://arxiv.org/pdf/2509.24116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Seung-won Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24116">Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have seen promising advances, yet they are still limited in "hard-exploration" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2509.24116.pdf' target='_blank'>https://arxiv.org/pdf/2509.24116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Seung-won Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24116">Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have seen promising advances, yet they are still limited in "hard-exploration" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2509.24116.pdf' target='_blank'>https://arxiv.org/pdf/2509.24116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Seung-won Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24116">Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have seen promising advances, yet they are still limited in "hard-exploration" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2509.24116.pdf' target='_blank'>https://arxiv.org/pdf/2509.24116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Seung-won Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24116">Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have seen promising advances, yet they are still limited in "hard-exploration" tasks requiring learning new knowledge through exploration. We present GLoW, a novel approach leveraging dual-scale world models, maintaining a trajectory frontier of high-value discoveries at the global scale, while learning from local trial-and-error in exploration through a Multi-path Advantage Reflection mechanism which infers advantage-based progress signals to guide exploration. To evaluate our framework for hard-exploration, we tackle the Jericho benchmark suite of text-based games, where GLoW achieves a new state-of-theart performance for LLM-based approaches. Compared to state-of-the-art RLbased methods, our approach achieves comparable performance while requiring 100-800x fewer environment interactions.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2507.22281.pdf' target='_blank'>https://arxiv.org/pdf/2507.22281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minsoo Kim, Seung-won Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22281">CoEx -- Co-evolving World-model and Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2507.05011.pdf' target='_blank'>https://arxiv.org/pdf/2507.05011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxence Boels, Harry Robertshaw, Thomas C Booth, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05011">When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Surgical action planning requires predicting future instrument-verb-target triplets for real-time assistance. While teleoperated robotic surgery provides natural expert demonstrations for imitation learning (IL), reinforcement learning (RL) could potentially discover superior strategies through exploration. We present the first comprehensive comparison of IL versus RL for surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and 33.6% next frame prediction mAP with smooth planning degradation to 29.2% at 10-second horizons. We evaluated three RL variants: world model-based RL, direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while direct video RL achieved only 15.9%. Our analysis reveals that distribution matching on expert-annotated test sets systematically favors IL over potentially valid RL policies that differ from training demonstrations. This challenges assumptions about RL superiority in sequential decision making and provides crucial insights for surgical AI development.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2505.24784.pdf' target='_blank'>https://arxiv.org/pdf/2505.24784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24784">AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2505.11528.pdf' target='_blank'>https://arxiv.org/pdf/2505.11528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Huang, Jiazhao Zhang, Shilong Zou, Xinwang Liu, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11528">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2505.11528.pdf' target='_blank'>https://arxiv.org/pdf/2505.11528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Huang, Jiazhao Zhang, Shilong Zou, Xinwang Liu, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11528">LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2504.16693.pdf' target='_blank'>https://arxiv.org/pdf/2504.16693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16693">PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2502.19544.pdf' target='_blank'>https://arxiv.org/pdf/2502.19544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhao, Aidan Scannell, Wenshuai Zhao, Yuxin Hou, Tianyu Cui, Le Chen, Dieter BÃ¼chler, Arno Solin, Juho Kannala, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19544">Efficient Reinforcement Learning by Guiding Generalist World Models with Non-Curated Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Leveraging offline data is a promising way to improve the sample efficiency of online reinforcement learning (RL). This paper expands the pool of usable data for offline-to-online RL by leveraging abundant non-curated data that is reward-free, of mixed quality, and collected across multiple embodiments. Although learning a world model appears promising for utilizing such data, we find that naive fine-tuning fails to accelerate RL training on many tasks. Through careful investigation, we attribute this failure to the distributional shift between offline and online data during fine-tuning. To address this issue and effectively use the offline data, we propose two essential techniques: \emph{i)} experience rehearsal and \emph{ii)} execution guidance. With these modifications, the non-curated offline data substantially improves RL's sample efficiency. Under limited sample budgets, our method achieves a 102.8\% relative improvement in aggregate score over learning-from-scratch baselines across 72 visuomotor tasks spanning 6 embodiments. On challenging tasks such as locomotion and robotic manipulation, it outperforms prior methods that utilize offline data by a decent margin.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2408.14472.pdf' target='_blank'>https://arxiv.org/pdf/2408.14472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, Jianyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14472">Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2407.04942.pdf' target='_blank'>https://arxiv.org/pdf/2407.04942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Cao, Yucheng Xin, Silang Wu, Longxiang He, Zichen Yan, Junbo Tan, Xueqian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04942">FOSP: Fine-tuning Offline Safe Policy through World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline Safe Reinforcement Learning (RL) seeks to address safety constraints by learning from static datasets and restricting exploration. However, these approaches heavily rely on the dataset and struggle to generalize to unseen scenarios safely. In this paper, we aim to improve safety during the deployment of vision-based robotic tasks through online fine-tuning an offline pretrained policy. To facilitate effective fine-tuning, we introduce model-based RL, which is known for its data efficiency. Specifically, our method employs in-sample optimization to improve offline training efficiency while incorporating reachability guidance to ensure safety. After obtaining an offline safe policy, a safe policy expansion approach is leveraged for online fine-tuning. The performance of our method is validated on simulation benchmarks with five vision-only tasks and through real-world robot deployment using limited data. It demonstrates that our approach significantly improves the generalization of offline policies to unseen safety-constrained scenarios. To the best of our knowledge, this is the first work to explore offline-to-online RL for safe generalization tasks.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2405.19883.pdf' target='_blank'>https://arxiv.org/pdf/2405.19883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianliang He, Siyu Chen, Fengzhuo Zhang, Zhuoran Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19883">From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $Îµ$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2405.04390.pdf' target='_blank'>https://arxiv.org/pdf/2405.04390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04390">DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2306.11488.pdf' target='_blank'>https://arxiv.org/pdf/2306.11488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaspard Lambrechts, Adrien Bolland, Damien Ernst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11488">Informed POMDP: Leveraging Additional Information in Model-Based RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the information at training and the observation at execution. Next, we propose an objective that leverages this information for learning a sufficient statistic of the history for the optimal control. We then adapt this informed objective to learn a world model able to sample latent trajectories. Finally, we empirically show a learning speed improvement in several environments using this informed world model in the Dreamer algorithm. These results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2003.04315.pdf' target='_blank'>https://arxiv.org/pdf/2003.04315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Charles Germain Lee, Doug Downey, Kyle Lo, Daniel S. Weld
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.04315">LIMEADE: From AI Explanations to Advice Taking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow an AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA$^2$Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This paper introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post-hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on seventy real-world models across two broad domains: image classification and text recommendation. We show our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2509.13095.pdf' target='_blank'>https://arxiv.org/pdf/2509.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13095">Empowering Multi-Robot Cooperation via Sequential World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) has shown significant potential in robotics due to its high sample efficiency and planning capability. However, extending MBRL to multi-robot cooperation remains challenging due to the complexity of joint dynamics and the reliance on synchronous communication. SeqWM employs independent, autoregressive agent-wise world models to represent joint dynamics, where each agent generates its future trajectory and plans its actions based on the predictions of its predecessors. This design lowers modeling complexity, alleviates the reliance on communication synchronization, and enables the emergence of advanced cooperative behaviors through explicit intention sharing. Experiments in challenging simulated environments (Bi-DexHands and Multi-Quad) demonstrate that SeqWM outperforms existing state-of-the-art model-based and model-free baselines in both overall performance and sample efficiency, while exhibiting advanced cooperative behaviors such as predictive adaptation, temporal alignment, and role division. Furthermore, SeqWM has been success fully deployed on physical quadruped robots, demonstrating its effectiveness in real-world multi-robot systems. Demos and code are available at: https://sites.google.com/view/seqwm-marl
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2509.13095.pdf' target='_blank'>https://arxiv.org/pdf/2509.13095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13095">Empowering Multi-Robot Cooperation via Sequential World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) has shown significant potential in robotics due to its high sample efficiency and planning capability. However, extending MBRL to multi-robot cooperation remains challenging due to the complexity of joint dynamics and the reliance on synchronous communication. SeqWM employs independent, autoregressive agent-wise world models to represent joint dynamics, where each agent generates its future trajectory and plans its actions based on the predictions of its predecessors. This design lowers modeling complexity, alleviates the reliance on communication synchronization, and enables the emergence of advanced cooperative behaviors through explicit intention sharing. Experiments in challenging simulated environments (Bi-DexHands and Multi-Quad) demonstrate that SeqWM outperforms existing state-of-the-art model-based and model-free baselines in both overall performance and sample efficiency, while exhibiting advanced cooperative behaviors such as predictive adaptation, temporal alignment, and role division. Furthermore, SeqWM has been success fully deployed on physical quadruped robots, demonstrating its effectiveness in real-world multi-robot systems. Demos and code are available at: https://sites.google.com/view/seqwm-marl
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2506.22991.pdf' target='_blank'>https://arxiv.org/pdf/2506.22991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Bennis, Sumudu Samarakoon, Tamara Alshammari, Chathuranga Weeraddana, Zhoujun Tian, Chaouki Ben Issaid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22991">Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Just like power, water, and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient. This requires them to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Unlike robustness and reliability, resilience is based on the understanding that disruptions will inevitably happen. Resilience, as elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents and networks that can flexibly expand their states and hypotheses through real-time adaptation and reconfiguration. This situational awareness and active preparedness, adapting world models and counterfactually reasoning about potential system failures and the best responses, is a core aspect of resilience. This article will first disambiguate resilience from reliability and robustness, before delving into key mathematical foundations of resilience grounded in abstraction, compositionality and emergence. Subsequently, we focus our attention on a plethora of techniques and methodologies pertaining to the unique characteristics of resilience, as well as their applications through a comprehensive set of use cases. Ultimately, the goal of this paper is to establish a unified foundation for understanding, modeling, and engineering resilience in wireless communication systems, while laying a roadmap for the next-generation of resilient-native and intelligent wireless systems.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2506.08149.pdf' target='_blank'>https://arxiv.org/pdf/2506.08149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Dechen Gao, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08149">Ego-centric Learning of Communicative World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. MARL is known to suffer from the \textit{partial observability} and \textit{non-stationarity} issues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. By making use of generative AI embodied in world model together with its latent representation, we develop {\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d Mode\underline{l}, for MARL, where 1) each agent first learns its world model that encodes its state and intention into low-dimensional latent representation with smaller memory footprint, which can be shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich her world model, and then exploits its generalization capacity to improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of using \textit{CALL}.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2506.00613.pdf' target='_blank'>https://arxiv.org/pdf/2506.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Quevedo, Percy Liang, Sherry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00613">Evaluating Robot Policies in a World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotics has broad applications from automating house chores to taking care of patients. However, evaluating robot control policies is challenging, as real-world testing is expensive, while handcrafted simulations often fail to accurately reflect real-world conditions, resulting in poor correlation between simulated evaluation and real-world outcomes. In this work, we investigate World-model-based Policy Evaluation (WPE). We first train an action-conditioned video generation model as a proxy to real-world environments. To enable efficient rollouts of hundreds of interactive steps while mitigating error accumulation in the world model, we propose an inference scheme which we call Blockwise-Autoregressive Diffusion Transformer with adjustable context and decoding horizon lengths. To ensure that the world model indeed follows action input, we propose metrics based on the agreement between the ground truth video and generated video conditioned on the same sequence of actions to evaluate the world model. We then use the world model for policy evaluation by performing Monte Carlo rollouts in the world model while employing a vision-language model (VLM) as a reward function. Interestingly, we found that WPE tends to underestimate the policy values for in-distribution actions and overestimate policy values for out-of-distribution actions. Nevertheless, WPE preserves the relative rankings of different policies. In emulating real robot executions, WPE achieves high fidelity in mimicing robot arm movements as in real videos, while emulating highly realistic object interaction remains challenging. Despite this limitation, we show that a world model can serve as a starting point for evaluating robot policies before real-world deployment.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2506.00613.pdf' target='_blank'>https://arxiv.org/pdf/2506.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, Sherry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00613">WorldGym: World Model as An Environment for Policy Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2506.00613.pdf' target='_blank'>https://arxiv.org/pdf/2506.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, Sherry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00613">WorldGym: World Model as An Environment for Policy Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2506.00613.pdf' target='_blank'>https://arxiv.org/pdf/2506.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, Sherry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00613">WorldGym: World Model as An Environment for Policy Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2506.00613.pdf' target='_blank'>https://arxiv.org/pdf/2506.00613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Quevedo, Ansh Kumar Sharma, Yixiang Sun, Varad Suryavanshi, Percy Liang, Sherry Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00613">WorldGym: World Model as An Environment for Policy Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating robot control policies is difficult: real-world testing is costly, and handcrafted simulators require manual effort to improve in realism and generality. We propose a world-model-based policy evaluation environment (WorldGym), an autoregressive, action-conditioned video generation model which serves as a proxy to real world environments. Policies are evaluated via Monte Carlo rollouts in the world model, with a vision-language model providing rewards. We evaluate a set of VLA-based real-robot policies in the world model using only initial frames from real robots, and show that policy success rates within the world model highly correlate with real-world success rates. Moreoever, we show that WorldGym is able to preserve relative policy rankings across different policy versions, sizes, and training checkpoints. Due to requiring only a single start frame as input, the world model further enables efficient evaluation of robot policies' generalization ability on novel tasks and environments. We find that modern VLA-based robot policies still struggle to distinguish object shapes and can become distracted by adversarial facades of objects. While generating highly realistic object interaction remains challenging, WorldGym faithfully emulates robot motions and offers a practical starting point for safe and reproducible policy evaluation before deployment.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2505.20922.pdf' target='_blank'>https://arxiv.org/pdf/2505.20922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20922">Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2505.19239.pdf' target='_blank'>https://arxiv.org/pdf/2505.19239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Shi, Shaoshuai Shi, Kehua Sheng, Bo Zhang, Li Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19239">DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data-driven learning has advanced autonomous driving, yet task-specific models struggle with out-of-distribution scenarios due to their narrow optimization objectives and reliance on costly annotated data. We present DriveX, a self-supervised world model that learns generalizable scene dynamics and holistic representations (geometric, semantic, and motion) from large-scale driving videos. DriveX introduces Omni Scene Modeling (OSM), a module that unifies multimodal supervision-3D point cloud forecasting, 2D semantic representation, and image generation-to capture comprehensive scene evolution. To simplify learning complex dynamics, we propose a decoupled latent world modeling strategy that separates world representation learning from future state decoding, augmented by dynamic-aware ray sampling to enhance motion modeling. For downstream adaptation, we design Future Spatial Attention (FSA), a unified paradigm that dynamically aggregates spatiotemporal features from DriveX's predictions to enhance task-specific inference. Extensive experiments demonstrate DriveX's effectiveness: it achieves significant improvements in 3D future point cloud prediction over prior work, while attaining state-of-the-art results on diverse tasks including occupancy prediction, flow estimation, and end-to-end driving. These results validate DriveX's capability as a general-purpose world model, paving the way for robust and unified autonomous driving frameworks.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2504.10191.pdf' target='_blank'>https://arxiv.org/pdf/2504.10191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Veniamin Veselovsky, Berke Argin, Benedikt Stroebl, Chris Wendler, Robert West, James Evans, Thomas L. Griffiths, Arvind Narayanan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10191">Localized Cultural Knowledge is Conserved and Controllable in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Just as humans display language patterns influenced by their native tongue when speaking new languages, LLMs often default to English-centric responses even when generating in other languages. Nevertheless, we observe that local cultural information persists within the models and can be readily activated for cultural customization. We first demonstrate that explicitly providing cultural context in prompts significantly improves the models' ability to generate culturally localized responses. We term the disparity in model performance with versus without explicit cultural context the explicit-implicit localization gap, indicating that while cultural knowledge exists within LLMs, it may not naturally surface in multilingual interactions if cultural context is not explicitly provided. Despite the explicit prompting benefit, however, the answers reduce in diversity and tend toward stereotypes. Second, we identify an explicit cultural customization vector, conserved across all non-English languages we explore, which enables LLMs to be steered from the synthetic English cultural world-model toward each non-English cultural world. Steered responses retain the diversity of implicit prompting and reduce stereotypes to dramatically improve the potential for customization. We discuss the implications of explicit cultural customization for understanding the conservation of alternative cultural world models within LLMs, and their controllable utility for translation, cultural customization, and the possibility of making the explicit implicit through soft control for expanded LLM function and appeal.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2503.08122.pdf' target='_blank'>https://arxiv.org/pdf/2503.08122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soonwoo Kwon, Jin-Young Kim, Hyojun Go, Kyungjune Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08122">Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel study on enhancing the capability of preserving the content in world models, focusing on a property we term World Stability. Recent diffusion-based generative models have advanced the synthesis of immersive and realistic environments that are pivotal for applications such as reinforcement learning and interactive game engines. However, while these models excel in quality and diversity, they often neglect the preservation of previously generated scenes over time--a shortfall that can introduce noise into agent learning and compromise performance in safety-critical settings. In this work, we introduce an evaluation framework that measures world stability by having world models perform a sequence of actions followed by their inverses to return to their initial viewpoint, thereby quantifying the consistency between the starting and ending observations. Our comprehensive assessment of state-of-the-art diffusion-based world models reveals significant challenges in achieving high world stability. Moreover, we investigate several improvement strategies to enhance world stability. Our results underscore the importance of world stability in world modeling and provide actionable insights for future research in this domain.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2501.17310.pdf' target='_blank'>https://arxiv.org/pdf/2501.17310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun-Shiuan Chuang, Sameer Narendran, Nikunj Harlalka, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17310">Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guesstimation -- the task of making approximate quantitative estimates about objects or events -- is a common real-world skill, yet remains underexplored in large language model (LLM) research. We introduce three guesstimation datasets: MARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many marbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential election). Inspired by the social science concept of Wisdom of Crowds (WOC)- where the median of multiple estimates improves accuracy-we propose WOC decoding for LLMs. We replicate WOC effects in human participants and find that LLMs exhibit similar benefits: median aggregation across sampled responses consistently improves accuracy over greedy decoding, self-consistency decoding, and mean decoding. This suggests that LLMs encode a world model that supports approximate reasoning. Our results position guesstimation as a useful probe of LLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM guesstimation performance on real-world tasks.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2501.17310.pdf' target='_blank'>https://arxiv.org/pdf/2501.17310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun-Shiuan Chuang, Sameer Narendran, Nikunj Harlalka, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17310">Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guesstimation -- the task of making approximate quantitative estimates about objects or events -- is a common real-world skill, yet remains underexplored in large language model (LLM) research. We introduce three guesstimation datasets: MARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many marbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential election). Inspired by the social science concept of Wisdom of Crowds (WOC)- where the median of multiple estimates improves accuracy-we propose WOC decoding for LLMs. We replicate WOC effects in human participants and find that LLMs exhibit similar benefits: median aggregation across sampled responses consistently improves accuracy over greedy decoding, self-consistency decoding, and mean decoding. This suggests that LLMs encode a world model that supports approximate reasoning. Our results position guesstimation as a useful probe of LLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM guesstimation performance on real-world tasks.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2501.13072.pdf' target='_blank'>https://arxiv.org/pdf/2501.13072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13072">AdaWM: Adaptive World Model based Planning for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model, due to distribution shift. We further analyze the effects of these factors on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed using efficient low-rank updates. Extensive experiments on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2501.10116.pdf' target='_blank'>https://arxiv.org/pdf/2501.10116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong, Ping Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10116">GAWM: Global-Aware World Model for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Model-based Multi-Agent Reinforcement Learning (MARL) has demonstrated significant advantages over model-free methods in terms of sample efficiency by using independent environment dynamics world models for data sample augmentation. However, without considering the limited sample size, these methods still lag behind model-free methods in terms of final convergence performance and stability. This is primarily due to the world model's insufficient and unstable representation of global states in partially observable environments. This limitation hampers the ability to ensure global consistency in the data samples and results in a time-varying and unstable distribution mismatch between the pseudo data samples generated by the world model and the real samples. This issue becomes particularly pronounced in more complex multi-agent environments. To address this challenge, we propose a model-based MARL method called GAWM, which enhances the centralized world model's ability to achieve globally unified and accurate representation of state information while adhering to the CTDE paradigm. GAWM uniquely leverages an additional Transformer architecture to fuse local observation information from different agents, thereby improving its ability to extract and represent global state information. This enhancement not only improves sample efficiency but also enhances training stability, leading to superior convergence performance, particularly in complex and challenging multi-agent environments. This advancement enables model-based methods to be effectively applied to more complex multi-agent environments. Experimental results demonstrate that GAWM outperforms various model-free and model-based approaches, achieving exceptional performance in the challenging domains of SMAC.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2501.00195.pdf' target='_blank'>https://arxiv.org/pdf/2501.00195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoyi Fang, Weiyu Du, Hang Wang, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00195">Towards Unraveling and Improving Generalization in World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have recently emerged as a promising approach to reinforcement learning (RL), achieving state-of-the-art performance across a wide range of visual control tasks. This work aims to obtain a deep understanding of the robustness and generalization capabilities of world models. Thus motivated, we develop a stochastic differential equation formulation by treating the world model learning as a stochastic dynamical system, and characterize the impact of latent representation errors on robustness and generalization, for both cases with zero-drift representation errors and with non-zero-drift representation errors. Our somewhat surprising findings, based on both theoretic and experimental studies, reveal that for the case with zero drift, modest latent representation errors can in fact function as implicit regularization and hence result in improved robustness. We further propose a Jacobian regularization scheme to mitigate the compounding error propagation effects of non-zero drift, thereby enhancing training stability and robustness. Our experimental studies corroborate that this regularization approach not only stabilizes training but also accelerates convergence and improves accuracy of long-horizon prediction.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2412.11198.pdf' target='_blank'>https://arxiv.org/pdf/2412.11198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro M B Rezende, Yasaman Haghighi, David BrÃ¼ggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, Marco Cannici, Elie Aljalbout, Botao Ye, Xi Wang, Aram Davtyan, Mathieu Salzmann, Davide Scaramuzza, Marc Pollefeys, Paolo Favaro, Alexandre Alahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11198">GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present GEM, a Generalizable Ego-vision Multimodal world model that predicts future frames using a reference frame, sparse features, human poses, and ego-trajectories. Hence, our model has precise control over object dynamics, ego-agent motion and human poses. GEM generates paired RGB and depth outputs for richer spatial understanding. We introduce autoregressive noise schedules to enable stable long-horizon generations. Our dataset is comprised of 4000+ hours of multimodal data across domains like autonomous driving, egocentric human activities, and drone flights. Pseudo-labels are used to get depth maps, ego-trajectories, and human poses. We use a comprehensive evaluation framework, including a new Control of Object Manipulation (COM) metric, to assess controllability. Experiments show GEM excels at generating diverse, controllable scenarios and temporal consistency over long generations. Code, models, and datasets are fully open-sourced.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2412.06461.pdf' target='_blank'>https://arxiv.org/pdf/2412.06461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Tu, Weijian Deng, Dylan Campbell, Yu Yao, Jiyang Zheng, Tom Gedeon, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06461">Ranked from Within: Ranking Large Multimodal Models for Visual Question Answering Without Labels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large multimodal models (LMMs) are increasingly deployed across diverse applications, the need for adaptable, real-world model ranking has become paramount. Traditional evaluation methods are largely dataset-centric, relying on fixed, labeled datasets and supervised metrics, which are resource-intensive and may lack generalizability to novel scenarios, highlighting the importance of unsupervised ranking. In this work, we explore unsupervised model ranking for LMMs by leveraging their uncertainty signals, such as softmax probabilities. We evaluate state-of-the-art LMMs (e.g., LLaVA) across visual question answering benchmarks, analyzing how uncertainty-based metrics can reflect model performance. Our findings show that uncertainty scores derived from softmax distributions provide a robust, consistent basis for ranking models across varied tasks. This finding enables the ranking of LMMs on real-world, unlabeled data for visual question answering, providing a practical approach for selecting models across diverse domains without requiring manual annotation.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2411.19639.pdf' target='_blank'>https://arxiv.org/pdf/2411.19639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19639">RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss in Some Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, model-based reinforcement learning (MBRL) has emerged as a solution to address sample complexity in multi-agent reinforcement learning (MARL) by modeling agent-environment dynamics to improve sample efficiency. However, most MBRL methods assume complete and continuous observations from each agent during the inference stage, which can be overly idealistic in practical applications. A novel model-based MARL approach called RMIO is introduced to address this limitation, specifically designed for scenarios where observation is lost in some agent. RMIO leverages the world model to reconstruct missing observations, and further reduces reconstruction errors through inter-agent information integration to ensure stable multi-agent decision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the CTDE paradigm in standard environment, and enabling limited communication only when agents lack observation data, thereby reducing reliance on communication. Additionally, RMIO improves asymptotic performance through strategies such as reward smoothing, a dual-layer experience replay buffer, and an RNN-augmented policy model, surpassing previous work. Our experiments conducted in both the SMAC and MaMuJoCo environments demonstrate that RMIO outperforms current state-of-the-art approaches in terms of asymptotic convergence performance and policy robustness, both in standard mission settings and in scenarios involving observation loss.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2409.03272.pdf' target='_blank'>https://arxiv.org/pdf/2409.03272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Zhongxue Gan, Wenchao Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03272">OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of multi-modal large language models(MLLMs) has spurred their applications in autonomous driving. Recent MLLM-based methods perform action by learning a direct mapping from perception to action, neglecting the dynamics of the world and the relations between action and world dynamics. In contrast, human beings possess world model that enables them to simulate the future states based on 3D internal visual representation and plan actions accordingly. To this end, we propose OccLLaMA, an occupancy-language-action generative world model, which uses semantic occupancy as a general visual representation and unifies vision-language-action(VLA) modalities through an autoregressive model. Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently discretize and reconstruct semantic occupancy scenes, considering its sparsity and classes imbalance. Then, we build a unified multi-modal vocabulary for vision, language and action. Furthermore, we enhance LLM, specifically LLaMA, to perform the next token/scene prediction on the unified vocabulary to complete multiple tasks in autonomous driving. Extensive experiments demonstrate that OccLLaMA achieves competitive performance across multiple tasks, including 4D occupancy forecasting, motion planning, and visual question answering, showcasing its potential as a foundation model in autonomous driving.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2406.19861.pdf' target='_blank'>https://arxiv.org/pdf/2406.19861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pietro Novelli, Marco PratticÃ², Massimiliano Pontil, Carlo Ciliberto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19861">Operator World Models for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2405.18193.pdf' target='_blank'>https://arxiv.org/pdf/2405.18193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharut Gupta, Chenyu Wang, Yifei Wang, Tommi Jaakkola, Stefanie Jegelka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18193">In-Context Symmetries: Self-Supervised Learning through Contextual World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>At the core of self-supervised learning for vision is the idea of learning invariant or equivariant representations with respect to a set of data transformations. This approach, however, introduces strong inductive biases, which can render the representations fragile in downstream tasks that do not conform to these symmetries. In this work, drawing insights from world models, we propose to instead learn a general representation that can adapt to be invariant or equivariant to different transformations by paying attention to context -- a memory module that tracks task-specific states, actions, and future states. Here, the action is the transformation, while the current and future states respectively represent the input's representation before and after the transformation. Our proposed algorithm, Contextual Self-Supervised Learning (ContextSSL), learns equivariance to all transformations (as opposed to invariance). In this way, the model can learn to encode all relevant features as general representations while having the versatility to tail down to task-wise symmetries when given a few examples as the context. Empirically, we demonstrate significant performance gains over existing methods on equivariance-related tasks, supported by both qualitative and quantitative evaluations.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2405.12573.pdf' target='_blank'>https://arxiv.org/pdf/2405.12573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Steckel, Wouter Jansen, Nico Huebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12573">EchoPT: A Pretrained Transformer Architecture that Predicts 2D In-Air Sonar Images for Mobile Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The predictive brain hypothesis suggests that perception can be interpreted as the process of minimizing the error between predicted perception tokens generated by an internal world model and actual sensory input tokens. When implementing working examples of this hypothesis in the context of in-air sonar, significant difficulties arise due to the sparse nature of the reflection model that governs ultrasonic sensing. Despite these challenges, creating consistent world models using sonar data is crucial for implementing predictive processing of ultrasound data in robotics. In an effort to enable robust robot behavior using ultrasound as the sole exteroceptive sensor modality, this paper introduces EchoPT, a pretrained transformer architecture designed to predict 2D sonar images from previous sensory data and robot ego-motion information. We detail the transformer architecture that drives EchoPT and compare the performance of our model to several state-of-the-art techniques. In addition to presenting and evaluating our EchoPT model, we demonstrate the effectiveness of this predictive perception approach in two robotic tasks.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2403.15306.pdf' target='_blank'>https://arxiv.org/pdf/2403.15306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Lenz, Rohit Menon, Michael Schreiber, Melvin Paul Jacob, Sven Behnke, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15306">HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Horticultural tasks such as pruning and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates in our indoor pepper plant mock-up.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2211.06236.pdf' target='_blank'>https://arxiv.org/pdf/2211.06236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Burcu KÃ¼Ã§Ã¼koÄlu, Walraaf Borkent, Bodo Rueckauer, Nasir Ahmad, Umut GÃ¼Ã§lÃ¼, Marcel van Gerven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.06236">Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in reinforcement learning (RL) often rely on massive compute resources and remain notoriously sample inefficient. In contrast, the human brain is able to efficiently learn effective control strategies using limited resources. This raises the question whether insights from neuroscience can be used to improve current RL methods. Predictive processing is a popular theoretical framework which maintains that the human brain is actively seeking to minimize surprise. We show that recurrent neural networks which predict their own sensory states can be leveraged to minimise surprise, yielding substantial gains in cumulative reward. Specifically, we present the Predictive Processing Proximal Policy Optimization (P4O) agent; an actor-critic reinforcement learning agent that applies predictive processing to a recurrent variant of the PPO algorithm by integrating a world model in its hidden state. Even without hyperparameter tuning, P4O significantly outperforms a baseline recurrent variant of the PPO algorithm on multiple Atari games using a single GPU. It also outperforms other state-of-the-art agents given the same wall-clock time and exceeds human gamer performance on multiple games including Seaquest, which is a particularly challenging environment in the Atari domain. Altogether, our work underscores how insights from the field of neuroscience may support the development of more capable and efficient artificial agents.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2510.06448.pdf' target='_blank'>https://arxiv.org/pdf/2510.06448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prabhant Singh, Sibylle Hess, Joaquin Vanschoren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06448">How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transferability estimation metrics are used to find a high-performing pre-trained model for a given target task without fine-tuning models and without access to the source dataset. Despite the growing interest in developing such metrics, the benchmarks used to measure their progress have gone largely unexamined. In this work, we empirically show the shortcomings of widely used benchmark setups to evaluate transferability estimation metrics. We argue that the benchmarks on which these metrics are evaluated are fundamentally flawed. We empirically demonstrate that their unrealistic model spaces and static performance hierarchies artificially inflate the perceived performance of existing metrics, to the point where simple, dataset-agnostic heuristics can outperform sophisticated methods. Our analysis reveals a critical disconnect between current evaluation protocols and the complexities of real-world model selection. To address this, we provide concrete recommendations for constructing more robust and realistic benchmarks to guide future research in a more meaningful direction.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2510.06448.pdf' target='_blank'>https://arxiv.org/pdf/2510.06448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prabhant Singh, Sibylle Hess, Joaquin Vanschoren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06448">How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transferability estimation metrics are used to find a high-performing pre-trained model for a given target task without fine-tuning models and without access to the source dataset. Despite the growing interest in developing such metrics, the benchmarks used to measure their progress have gone largely unexamined. In this work, we empirically show the shortcomings of widely used benchmark setups to evaluate transferability estimation metrics. We argue that the benchmarks on which these metrics are evaluated are fundamentally flawed. We empirically demonstrate that their unrealistic model spaces and static performance hierarchies artificially inflate the perceived performance of existing metrics, to the point where simple, dataset-agnostic heuristics can outperform sophisticated methods. Our analysis reveals a critical disconnect between current evaluation protocols and the complexities of real-world model selection. To address this, we provide concrete recommendations for constructing more robust and realistic benchmarks to guide future research in a more meaningful direction.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2510.06209.pdf' target='_blank'>https://arxiv.org/pdf/2510.06209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06209">Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2510.06209.pdf' target='_blank'>https://arxiv.org/pdf/2510.06209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06209">Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2509.11943.pdf' target='_blank'>https://arxiv.org/pdf/2509.11943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonin Sulc, Thorsten Hellert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11943">Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent agents, particularly those powered by language models (LMs), has shown the critical role in various environments that require intelligent and autonomous decision. Environments are not passive testing grounds and they represent the data required for agents to learn and exhibit very challenging conditions that require adaptive, complex and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \emph{possibility} and \emph{necessity} using the formal language of modal logic. In this work, we use of immutable, domain-specific knowledge to make infere information, which is encoded as logical constraints essential for proper diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2509.11943.pdf' target='_blank'>https://arxiv.org/pdf/2509.11943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonin Sulc, Thorsten Hellert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11943">Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of intelligent agents, particularly those powered by language models (LMs), has shown the critical role in various environments that require intelligent and autonomous decision. Environments are not passive testing grounds and they represent the data required for agents to learn and exhibit very challenging conditions that require adaptive, complex and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \emph{possibility} and \emph{necessity} using the formal language of modal logic. In this work, we use of immutable, domain-specific knowledge to make infere information, which is encoded as logical constraints essential for proper diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2507.18365.pdf' target='_blank'>https://arxiv.org/pdf/2507.18365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie He, Yuechun Gu, Keke Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18365">RecPS: Privacy Risk Scoring for Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2507.18365.pdf' target='_blank'>https://arxiv.org/pdf/2507.18365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie He, Yuechun Gu, Keke Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18365">RecPS: Privacy Risk Scoring for Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2506.22355.pdf' target='_blank'>https://arxiv.org/pdf/2506.22355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, HervÃ© JÃ©gou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Louis-Philippe Morency, ThÃ©o Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Paden Tomasello, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22355">Embodied AI Agents: Modeling the World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2506.21976.pdf' target='_blank'>https://arxiv.org/pdf/2506.21976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhan Tan, John Lambert, Hong Jeon, Sakshum Kulshrestha, Yijing Bai, Jing Luo, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21976">SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles that is available for testing and validation, with a much larger amount of simulated synthetic miles. The culmination of this vision would be a generative simulated city, where given a map of the city and an autonomous vehicle (AV) software stack, the simulator can seamlessly simulate the trip from point A to point B by populating the city around the AV and controlling all aspects of the scene, from animating the dynamic agents (e.g., vehicles, pedestrians) to controlling the traffic light states. We refer to this vision as CitySim, which requires an agglomeration of simulation technologies: scene generation to populate the initial scene, agent behavior modeling to animate the scene, occlusion reasoning, dynamic scene generation to seamlessly spawn and remove agents, and environment simulation for factors such as traffic lights. While some key technologies have been separately studied in various works, others such as dynamic scene generation and environment simulation have received less attention in the research community. We propose SceneDiffuser++, the first end-to-end generative world model trained on a single loss function capable of point A-to-B simulation on a city scale integrating all the requirements above. We demonstrate the city-scale traffic simulation capability of SceneDiffuser++ and study its superior realism under long simulation conditions. We evaluate the simulation quality on an augmented version of the Waymo Open Motion Dataset (WOMD) with larger map regions to support trip-level simulation.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2506.09171.pdf' target='_blank'>https://arxiv.org/pdf/2506.09171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Holt, Max Ruiz Luyten, Thomas Pouplin, Mihaela van der Schaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09171">Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts'' from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2506.06199.pdf' target='_blank'>https://arxiv.org/pdf/2506.06199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyan Zhi, Peihao Chen, Siyuan Zhou, Yubo Dong, Quanxi Wu, Lei Han, Mingkui Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06199">3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2505.14396.pdf' target='_blank'>https://arxiv.org/pdf/2505.14396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GaÃ«l Gendron, JoÅ¾e M. RoÅ¾anec, Michael Witbrock, Gillian Dobbie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14396">Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal world models are systems that can answer counterfactual questions about an environment of interest, i.e. predict how it would have evolved if an arbitrary subset of events had been realized differently. It requires understanding the underlying causes behind chains of events and conducting causal inference for arbitrary unseen distributions. So far, this task eludes foundation models, notably large language models (LLMs), which do not have demonstrated causal reasoning capabilities beyond the memorization of existing causal relationships. Furthermore, evaluating counterfactuals in real-world applications is challenging since only the factual world is observed, limiting evaluation to synthetic datasets. We address these problems by explicitly extracting and modeling causal relationships and propose the Causal Cartographer framework. First, we introduce a graph retrieval-augmented generation agent tasked to retrieve causal relationships from data. This approach allows us to construct a large network of real-world causal relationships that can serve as a repository of causal knowledge and build real-world counterfactuals. In addition, we create a counterfactual reasoning agent constrained by causal relationships to perform reliable step-by-step causal inference. We show that our approach can extract causal knowledge and improve the robustness of LLMs for causal reasoning tasks while reducing inference costs and spurious correlations.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2503.00692.pdf' target='_blank'>https://arxiv.org/pdf/2503.00692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wandong Sun, Baoshi Cao, Long Chen, Yongbo Su, Yang Liu, Zongwu Xie, Hong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00692">Learning Perceptive Humanoid Locomotion over Challenging Terrain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are engineered to navigate terrains akin to those encountered by humans, which necessitates human-like locomotion and perceptual abilities. Currently, the most reliable controllers for humanoid motion rely exclusively on proprioception, a reliance that becomes both dangerous and unreliable when coping with rugged terrain. Although the integration of height maps into perception can enable proactive gait planning, robust utilization of this information remains a significant challenge, especially when exteroceptive perception is noisy. To surmount these challenges, we propose a solution based on a teacher-student distillation framework. In this paradigm, an oracle policy accesses noise-free data to establish an optimal reference policy, while the student policy not only imitates the teacher's actions but also simultaneously trains a world model with a variational information bottleneck for sensor denoising and state estimation. Extensive evaluations demonstrate that our approach markedly enhances performance in scenarios characterized by unreliable terrain estimations. Moreover, we conducted rigorous testing in both challenging urban settings and off-road environments, the model successfully traverse 2 km of varied terrain without external intervention.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2502.16230.pdf' target='_blank'>https://arxiv.org/pdf/2502.16230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wandong Sun, Long Chen, Yongbo Su, Baoshi Cao, Yang Liu, Zongwu Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16230">Learning Humanoid Locomotion with World Model Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot's understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estimator to explicitly reconstruct the world state and utilize it to enhance the locomotion policy. The locomotion policy takes inputs entirely from the reconstructed information. The policy and the estimator are trained jointly; however, the gradient between them is intentionally cut off. This ensures that the estimator focuses solely on world reconstruction, independent of the locomotion policy's updates. We evaluated our model on rough, deformable, and slippery surfaces in real-world scenarios, demonstrating robust adaptability and resistance to interference. The robot successfully completed a 3.2 km hike without any human assistance, mastering terrains covered with ice and snow.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2501.01263.pdf' target='_blank'>https://arxiv.org/pdf/2501.01263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiali Wei, Ming Fan, Xicheng Zhang, Wenjing Jiao, Haijun Wang, Ting Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01263">Stealthy Backdoor Attack to Real-world Models in Android Apps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Powered by their superior performance, deep neural networks (DNNs) have found widespread applications across various domains. Many deep learning (DL) models are now embedded in mobile apps, making them more accessible to end users through on-device DL. However, deploying on-device DL to users' smartphones simultaneously introduces several security threats. One primary threat is backdoor attacks. Extensive research has explored backdoor attacks for several years and has proposed numerous attack approaches. However, few studies have investigated backdoor attacks on DL models deployed in the real world, or they have shown obvious deficiencies in effectiveness and stealthiness. In this work, we explore more effective and stealthy backdoor attacks on real-world DL models extracted from mobile apps. Our main justification is that imperceptible and sample-specific backdoor triggers generated by DNN-based steganography can enhance the efficacy of backdoor attacks on real-world models. We first confirm the effectiveness of steganography-based backdoor attacks on four state-of-the-art DNN models. Subsequently, we systematically evaluate and analyze the stealthiness of the attacks to ensure they are difficult to perceive. Finally, we implement the backdoor attacks on real-world models and compare our approach with three baseline methods. We collect 38,387 mobile apps, extract 89 DL models from them, and analyze these models to obtain the prerequisite model information for the attacks. After identifying the target models, our approach achieves an average of 12.50% higher attack success rate than DeepPayload while better maintaining the normal performance of the models. Extensive experimental results demonstrate that our method enables more effective, robust, and stealthy backdoor attacks on real-world models.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2412.07446.pdf' target='_blank'>https://arxiv.org/pdf/2412.07446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev Lal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07446">A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Are generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learning a world model from which sequences are generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT and presenting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences, and introduce a corresponding confidence score. Empirical tests were conducted in controlled environments using the setups of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, was tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases where it generates illegal moves, it also fails to capture a causal structure.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2411.15998.pdf' target='_blank'>https://arxiv.org/pdf/2411.15998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Light, Sixue Xing, Yuanzhe Liu, Weiqin Chen, Min Cai, Xiusi Chen, Guanzhi Wang, Wei Cheng, Yisong Yue, Ziniu Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15998">PIANIST: Learning Partially Observable World Models with LLMs for Multi-Agent Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective extraction of the world knowledge in LLMs for complex decision-making tasks remains a challenge. We propose a framework PIANIST for decomposing the world model into seven intuitive components conducive to zero-shot LLM generation. Given only the natural language description of the game and how input observations are formatted, our method can generate a working world model for fast and efficient MCTS simulation. We show that our method works well on two different games that challenge the planning and decision making skills of the agent for both language and non-language based action taking, without any training on domain-specific training data or explicitly defined world model.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2408.15511.pdf' target='_blank'>https://arxiv.org/pdf/2408.15511.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanglong Yao, Yuanchang Yue, Youzhi Liu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15511">AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerospace embodied intelligence aims to empower unmanned aerial vehicles (UAVs) and other aerospace platforms to achieve autonomous perception, cognition, and action, as well as egocentric active interaction with humans and the environment. The aerospace embodied world model serves as an effective means to realize the autonomous intelligence of UAVs and represents a necessary pathway toward aerospace embodied intelligence. However, existing embodied world models primarily focus on ground-level intelligent agents in indoor scenarios, while research on UAV intelligent agents remains unexplored. To address this gap, we construct the first large-scale real-world image-text pre-training dataset, AerialAgent-Ego10k, featuring urban drones from a first-person perspective. We also create a virtual image-text-pose alignment dataset, CyberAgent Ego500k, to facilitate the pre-training of the aerospace embodied world model. For the first time, we clearly define 5 downstream tasks, i.e., aerospace embodied scene awareness, spatial reasoning, navigational exploration, task planning, and motion decision, and construct corresponding instruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k and SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace embodiment world model. Simultaneously, we develop SkyAgentEval, the downstream task evaluation metrics based on GPT-4, to comprehensively, flexibly, and objectively assess the results, revealing the potential and limitations of 2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over 10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning datasets, more than 10 evaluation metrics, and a simulator into the benchmark suite, i.e., AeroVerse, which will be released to the community to promote exploration and development of aerospace embodied intelligence.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2408.11326.pdf' target='_blank'>https://arxiv.org/pdf/2408.11326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11326">Automating Thought of Search: A Journey Towards Soundness and Completeness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are being used to solve planning problems that require search. Most of the literature uses LLMs as world models to define the search space, forgoing soundness for the sake of flexibility. A recent work, Thought of Search (ToS), proposed defining the search space with code, having LLMs produce that code. ToS requires a human in the loop, collaboratively producing a sound successor function and goal test. The result, however, is worth the effort: all the tested datasets were solved with 100% accuracy. Consequently, there is great potential to automate the ToS process. We take a first major step towards automating ToS (AutoToS), taking the human out of the loop of interactions with the language model. AutoToS guides the language model step by step towards the generation of sound and complete search components, through feedback from both generic and domain specific unit tests. We show that AutoToS is able to achieve 100% accuracy on all the evaluated domains with a small number of LLM calls.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2406.10429.pdf' target='_blank'>https://arxiv.org/pdf/2406.10429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pietro Astolfi, Marlene Careil, Melissa Hall, Oscar MaÃ±as, Matthew Muckley, Jakob Verbeek, Adriana Romero Soriano, Michal Drozdzal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10429">Consistency-diversity-realism Pareto fronts of conditional image generative models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building world models that accurately and comprehensively represent the real world is the utmost aspiration for conditional image generative models as it would enable their use as world simulators. For these models to be successful world models, they should not only excel at image quality and prompt-image consistency but also ensure high representation diversity. However, current research in generative models mostly focuses on creative applications that are predominantly concerned with human preferences of image quality and aesthetics. We note that generative models have inference time mechanisms - or knobs - that allow the control of generation consistency, quality, and diversity. In this paper, we use state-of-the-art text-to-image and image-and-text-to-image models and their knobs to draw consistency-diversity-realism Pareto fronts that provide a holistic view on consistency-diversity-realism multi-objective. Our experiments suggest that realism and consistency can both be improved simultaneously; however there exists a clear tradeoff between realism/consistency and diversity. By looking at Pareto optimal points, we note that earlier models are better at representation diversity and worse in consistency/realism, and more recent models excel in consistency/realism while decreasing significantly the representation diversity. By computing Pareto fronts on a geodiverse dataset, we find that the first version of latent diffusion models tends to perform better than more recent models in all axes of evaluation, and there exist pronounced consistency-diversity-realism disparities between geographical regions. Overall, our analysis clearly shows that there is no best model and the choice of model should be determined by the downstream application. With this analysis, we invite the research community to consider Pareto fronts as an analytical tool to measure progress towards world models.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2406.08691.pdf' target='_blank'>https://arxiv.org/pdf/2406.08691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Agro, Quinlan Sykora, Sergio Casas, Thomas Gilles, Raquel Urtasun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08691">UnO: Unsupervised Occupancy Fields for Perception and Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving the world and forecasting its future state is a critical task for self-driving. Supervised approaches leverage annotated object labels to learn a model of the world -- traditionally with object detections and trajectory predictions, or temporal bird's-eye-view (BEV) occupancy fields. However, these annotations are expensive and typically limited to a set of predefined categories that do not cover everything we might encounter on the road. Instead, we learn to perceive and forecast a continuous 4D (spatio-temporal) occupancy field with self-supervision from LiDAR data. This unsupervised world model can be easily and effectively transferred to downstream tasks. We tackle point cloud forecasting by adding a lightweight learned renderer and achieve state-of-the-art performance in Argoverse 2, nuScenes, and KITTI. To further showcase its transferability, we fine-tune our model for BEV semantic occupancy forecasting and show that it outperforms the fully supervised state-of-the-art, especially when labeled data is scarce. Finally, when compared to prior state-of-the-art on spatio-temporal geometric occupancy prediction, our 4D world model achieves a much higher recall of objects from classes relevant to self-driving.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2406.00765.pdf' target='_blank'>https://arxiv.org/pdf/2406.00765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wakana Haijima, Kou Nakakubo, Masahiro Suzuki, Yutaka Matsuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00765">The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, as machine learning, particularly for vision and language understanding, has been improved, research in embedded AI has also evolved. VOYAGER is a well-known LLM-based embodied AI that enables autonomous exploration in the Minecraft world, but it has issues such as underutilization of visual data and insufficient functionality as a world model. In this research, the possibility of utilizing visual data and the function of LLM as a world model were investigated with the aim of improving the performance of embodied AI. The experimental results revealed that LLM can extract necessary information from visual data, and the utilization of the information improves its performance as a world model. It was also suggested that devised prompts could bring out the LLM's function as a world model.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2405.17039.pdf' target='_blank'>https://arxiv.org/pdf/2405.17039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengxing Jia, Pengyuan Wang, Ziniu Li, Yi-Chen Li, Zhilong Zhang, Nan Tang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17039">BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have catalyzed a paradigm shift in natural language processing, yet their limited controllability poses a significant challenge for downstream applications. We aim to address this by drawing inspiration from the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension, respectively. In particular, Broca's area receives cognitive decision signals from Wernicke's area, treating the language generation as an intricate decision-making process, which differs from the fully auto-regressive language generation of existing LLMs. In a similar vein, our proposed system, the BWArea model, conceptualizes language generation as a decision-making task. This model has three components: a language world model, an inverse dynamics model, and a cognitive policy. Like Wernicke's area, the inverse dynamics model is designed to deduce the underlying cognitive intentions, or latent actions, behind each token. The BWArea model is amenable to both pre-training and fine-tuning like existing LLMs. With 30B clean pre-training tokens, we have trained a BWArea model, which achieves competitive performance with LLMs of equal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training performance does not degenerate if dirty data unintentionally appears. This shows the advantage of a decomposed structure of BWArea model in reducing efforts in laborious data selection and labeling. Finally, we reveal that the BWArea model offers enhanced controllability via fine-tuning the cognitive policy with downstream reward metrics, thereby facilitating alignment with greater simplicity. On 9 out of 10 tasks from two suites, TextWorld and BigBench Hard, our method shows superior performance to auto-regressive LLMs.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2403.07944.pdf' target='_blank'>https://arxiv.org/pdf/2403.07944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang, Cindy Yang, Yuexian Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07944">WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several text-to-video diffusion models have demonstrated commendable capabilities in synthesizing high-quality video content. However, it remains a formidable challenge pertaining to maintaining temporal consistency and ensuring action smoothness throughout the generated sequences. In this paper, we present an innovative video generation AI agent that harnesses the power of Sora-inspired multimodal learning to build skilled world models framework based on textual prompts and accompanying images. The framework includes two parts: prompt enhancer and full video translation. The first part employs the capabilities of ChatGPT to meticulously distill and proactively construct precise prompts for each subsequent step, thereby guaranteeing the utmost accuracy in prompt communication and accurate execution in following model operations. The second part employ compatible with existing advanced diffusion techniques to expansively generate and refine the key frame at the conclusion of a video. Then we can expertly harness the power of leading and trailing key frames to craft videos with enhanced temporal consistency and action smoothness. The experimental results confirm that our method has strong effectiveness and novelty in constructing world models from text and image inputs over the other methods.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2312.08533.pdf' target='_blank'>https://arxiv.org/pdf/2312.08533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Rigter, Jun Yamada, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08533">World Models via Policy-Guided Trajectory Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models. Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion. For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements. For long trajectories, PolyGRAD obtains comparable performance to baselines. Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains. Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2311.03622.pdf' target='_blank'>https://arxiv.org/pdf/2311.03622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Yamada, Marc Rigter, Jack Collins, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03622">TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2311.01017.pdf' target='_blank'>https://arxiv.org/pdf/2311.01017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01017">Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose Copilot4D, a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer as discrete diffusion and enhance it with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, Copilot4D reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotics.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2310.13394.pdf' target='_blank'>https://arxiv.org/pdf/2310.13394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Shu, Jiuzhou Han, Fangyu Liu, Ehsan Shareghi, Nigel Collier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13394">POSQA: Probe the World Models of LLMs with Size Comparisons</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs.
  We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt formats and report bias of different objects. Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2307.05219.pdf' target='_blank'>https://arxiv.org/pdf/2307.05219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado-Rincon, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05219">MinkSORT: A 3D deep feature extractor using sparse convolutions to improve 3D multi-object tracking in greenhouse tomato plants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The agro-food industry is turning to robots to address the challenge of labour shortage. However, agro-food environments pose difficulties for robots due to high variation and occlusions. In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately. Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues. In this paper, MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach, is proposed to improve the accuracy of world models in agro-food environments. MinkSORT was evaluated using real-world data collected in a tomato greenhouse, where it significantly improved the performance of a baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance. MinkSORT improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%. Different contrastive loss functions for training MinkSORT were also evaluated, and it was demonstrated that it leads to improved performance in terms of three separate precision and recall detection outcomes. The proposed method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2306.11297.pdf' target='_blank'>https://arxiv.org/pdf/2306.11297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dev Gurung, Shiva Raj Pokhrel, Gang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11297">Decentralized Quantum Federated Learning for Metaverse: Analysis, Design and Implementation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the emerging developments of the Metaverse, a virtual world where people can interact, socialize, play, and conduct their business, it has become critical to ensure that the underlying systems are transparent, secure, and trustworthy. To this end, we develop a decentralized and trustworthy quantum federated learning (QFL) framework. The proposed QFL leverages the power of blockchain to create a secure and transparent system that is robust against cyberattacks and fraud. In addition, the decentralized QFL system addresses the risks associated with a centralized server-based approach. With extensive experiments and analysis, we evaluate classical federated learning (CFL) and QFL in a distributed setting and demonstrate the practicality and benefits of the proposed design. Our theoretical analysis and discussions develop a genuinely decentralized financial system essential for the Metaverse. Furthermore, we present the application of blockchain-based QFL in a hybrid metaverse powered by a metaverse observer and world model. Our implementation details and code are publicly available 1.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2304.06607.pdf' target='_blank'>https://arxiv.org/pdf/2304.06607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N. Asokan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06607">False Claims against Model Ownership Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.
  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our core idea is that a malicious accuser can deviate (without detection) from the specified MOR process by finding (transferable) adversarial examples that successfully serve as evidence against independent suspect models. To this end, we first generalize the procedures of common MOR schemes and show that, under this generalization, defending against false claims is as challenging as preventing (transferable) adversarial examples. Via systematic empirical evaluation, we show that our false claim attacks always succeed in the MOR schemes that follow our generalization, including in a real-world model: Amazon's Rekognition API.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2303.12410.pdf' target='_blank'>https://arxiv.org/pdf/2303.12410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12410">EDGI: Equivariant Diffusion for Planning with Embodied Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group SE(3), the discrete-time translation group Z, and the object permutation group Sn. EDGI follows the Diffuser framework (Janner et al., 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new SE(3)xZxSn-equivariant diffusion model that supports multiple representations. We integrate this model in a planning loop, where conditioning and classifier guidance let us softly break the symmetry for specific tasks as needed. On object manipulation and navigation tasks, EDGI is substantially more sample efficient and generalizes better across the symmetry group than non-equivariant models.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2211.02760.pdf' target='_blank'>https://arxiv.org/pdf/2211.02760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado Rincon, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02760">Development and evaluation of automated localisation and reconstruction of all fruits on tomato plants in a greenhouse based on multi-view perception and 3D multi-object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to accurately represent and localise relevant objects is essential for robots to carry out tasks effectively. Traditional approaches, where robots simply capture an image, process that image to take an action, and then forget the information, have proven to struggle in the presence of occlusions. Methods using multi-view perception, which have the potential to address some of these problems, require a world model that guides the collection, integration and extraction of information from multiple viewpoints. Furthermore, constructing a generic representation that can be applied in various environments and tasks is a difficult challenge. In this paper, a novel approach for building generic representations in occluded agro-food environments using multi-view perception and 3D multi-object tracking is introduced. The method is based on a detection algorithm that generates partial point clouds for each detected object, followed by a 3D multi-object tracking algorithm that updates the representation over time. The accuracy of the representation was evaluated in a real-world environment, where successful representation and localisation of tomatoes in tomato plants were achieved, despite high levels of occlusion, with the total count of tomatoes estimated with a maximum error of 5.08% and the tomatoes tracked with an accuracy up to 71.47%. Novel tracking metrics were introduced, demonstrating that valuable insight into the errors in localising and representing the fruits can be provided by their use. This approach presents a novel solution for building representations in occluded agro-food environments, demonstrating potential to enable robots to perform tasks effectively in these challenging environments.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2508.17588.pdf' target='_blank'>https://arxiv.org/pdf/2508.17588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanjian Song, Xinyu Wang, Donghao Zhou, Jingyu Lin, Cunjian Chen, Yue Ma, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17588">HERO: Hierarchical Extrapolation and Refresh for Efficient World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generation-driven world models create immersive virtual environments but suffer slow inference due to the iterative nature of diffusion models. While recent advances have improved diffusion model efficiency, directly applying these techniques to world models introduces limitations such as quality degradation. In this paper, we present HERO, a training-free hierarchical acceleration framework tailored for efficient world models. Owing to the multi-modal nature of world models, we identify a feature coupling phenomenon, wherein shallow layers exhibit high temporal variability, while deeper layers yield more stable feature representations. Motivated by this, HERO adopts hierarchical strategies to accelerate inference: (i) In shallow layers, a patch-wise refresh mechanism efficiently selects tokens for recomputation. With patch-wise sampling and frequency-aware tracking, it avoids extra metric computation and remain compatible with FlashAttention. (ii) In deeper layers, a linear extrapolation scheme directly estimates intermediate features. This completely bypasses the computations in attention modules and feed-forward networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2508.09346.pdf' target='_blank'>https://arxiv.org/pdf/2508.09346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Mrinall Eashaan Umasudhan, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09346">How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots that rely on deep neural network controllers pose critical challenges for safety prediction, especially under partial observability and distribution shift. Traditional model-based verification techniques are limited in scalability and require access to low-dimensional state models, while model-free methods often lack reliability guarantees. This paper addresses these limitations by introducing a framework for calibrated safety prediction in end-to-end vision-controlled systems, where neither the state-transition model nor the observation model is accessible. Building on the foundation of world models, we leverage variational autoencoders and recurrent predictors to forecast future latent trajectories from raw image sequences and estimate the probability of satisfying safety properties. We distinguish between monolithic and composite prediction pipelines and introduce a calibration mechanism to quantify prediction confidence. In long-horizon predictions from high-dimensional observations, the forecasted inputs to the safety evaluator can deviate significantly from the training distribution due to compounding prediction errors and changing environmental conditions, leading to miscalibrated risk estimates. To address this, we incorporate unsupervised domain adaptation to ensure robustness of safety evaluation under distribution shift in predictions without requiring manual labels. Our formulation provides theoretical calibration guarantees and supports practical evaluation across long prediction horizons. Experimental results on three benchmarks show that our UDA-equipped evaluators maintain high accuracy and substantially lower false positive rates under distribution shift. Similarly, world model-based composite predictors outperform their monolithic counterparts on long-horizon tasks, and our conformal calibration provides reliable statistical bounds.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2508.09346.pdf' target='_blank'>https://arxiv.org/pdf/2508.09346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Mrinall Eashaan Umasudhan, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09346">How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots that rely on deep neural network controllers pose critical challenges for safety prediction, especially under partial observability and distribution shift. Traditional model-based verification techniques are limited in scalability and require access to low-dimensional state models, while model-free methods often lack reliability guarantees. This paper addresses these limitations by introducing a framework for calibrated safety prediction in end-to-end vision-controlled systems, where neither the state-transition model nor the observation model is accessible. Building on the foundation of world models, we leverage variational autoencoders and recurrent predictors to forecast future latent trajectories from raw image sequences and estimate the probability of satisfying safety properties. We distinguish between monolithic and composite prediction pipelines and introduce a calibration mechanism to quantify prediction confidence. In long-horizon predictions from high-dimensional observations, the forecasted inputs to the safety evaluator can deviate significantly from the training distribution due to compounding prediction errors and changing environmental conditions, leading to miscalibrated risk estimates. To address this, we incorporate unsupervised domain adaptation to ensure robustness of safety evaluation under distribution shift in predictions without requiring manual labels. Our formulation provides theoretical calibration guarantees and supports practical evaluation across long prediction horizons. Experimental results on three benchmarks show that our UDA-equipped evaluators maintain high accuracy and substantially lower false positive rates under distribution shift. Similarly, world model-based composite predictors outperform their monolithic counterparts on long-horizon tasks, and our conformal calibration provides reliable statistical bounds.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2508.09346.pdf' target='_blank'>https://arxiv.org/pdf/2508.09346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Mrinall Eashaan Umasudhan, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09346">How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous robots that rely on deep neural network controllers pose critical challenges for safety prediction, especially under partial observability and distribution shift. Traditional model-based verification techniques are limited in scalability and require access to low-dimensional state models, while model-free methods often lack reliability guarantees. This paper addresses these limitations by introducing a framework for calibrated safety prediction in end-to-end vision-controlled systems, where neither the state-transition model nor the observation model is accessible. Building on the foundation of world models, we leverage variational autoencoders and recurrent predictors to forecast future latent trajectories from raw image sequences and estimate the probability of satisfying safety properties. We distinguish between monolithic and composite prediction pipelines and introduce a calibration mechanism to quantify prediction confidence. In long-horizon predictions from high-dimensional observations, the forecasted inputs to the safety evaluator can deviate significantly from the training distribution due to compounding prediction errors and changing environmental conditions, leading to miscalibrated risk estimates. To address this, we incorporate unsupervised domain adaptation to ensure robustness of safety evaluation under distribution shift in predictions without requiring manual labels. Our formulation provides theoretical calibration guarantees and supports practical evaluation across long prediction horizons. Experimental results on three benchmarks show that our UDA-equipped evaluators maintain high accuracy and substantially lower false positive rates under distribution shift. Similarly, world model-based composite predictors outperform their monolithic counterparts on long-horizon tasks, and our conformal calibration provides reliable statistical bounds.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2506.22112.pdf' target='_blank'>https://arxiv.org/pdf/2506.22112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzheng Shu, Yanxiang Zeng, Yongxiang Tang, Teng Sha, Ning Luo, Yanhua Cheng, Xialong Liu, Fan Zhou, Peng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22112">Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) has emerged as a prevalent and effective methodology for real-world recommender systems, enabling learning policies from historical data and capturing user preferences. In offline RL, reward shaping encounters significant challenges, with past efforts to incorporate prior strategies for uncertainty to improve world models or penalize underexplored state-action pairs. Despite these efforts, a critical gap remains: the simultaneous balancing of intrinsic biases in world models and the diversity of policy recommendations. To address this limitation, we present an innovative offline RL framework termed Reallocated Reward for Recommender Systems (R3S). By integrating inherent model uncertainty to tackle the intrinsic fluctuations in reward predictions, we boost diversity for decision-making to align with a more interactive paradigm, incorporating extra penalizers with decay that deter actions leading to diminished state variety at both local and global scales. The experimental results demonstrate that R3S improves the accuracy of world models and efficiently harmonizes the heterogeneous preferences of the users.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2506.21876.pdf' target='_blank'>https://arxiv.org/pdf/2506.21876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang, Benhao Huang, Zeming Chen, David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21876">Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2506.19055.pdf' target='_blank'>https://arxiv.org/pdf/2506.19055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zefan Yang, Xinrui Song, Xuanang Xu, Yongyi Shi, Ge Wang, Mannudeep K. Kalra, Pingkun Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19055">Xray2Xray: World Model from Chest X-rays with Volumetric Context</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chest X-rays (CXRs) are the most widely used medical imaging modality and play a pivotal role in diagnosing diseases. However, as 2D projection images, CXRs are limited by structural superposition, which constrains their effectiveness in precise disease diagnosis and risk prediction. To address the limitations of 2D CXRs, this study introduces Xray2Xray, a novel World Model that learns latent representations encoding 3D structural information from chest X-rays. Xray2Xray captures the latent representations of the chest volume by modeling the transition dynamics of X-ray projections across different angular positions with a vision model and a transition model. We employed the latent representations of Xray2Xray for downstream risk prediction and disease diagnosis tasks. Experimental results showed that Xray2Xray outperformed both supervised methods and self-supervised pretraining methods for cardiovascular disease risk estimation and achieved competitive performance in classifying five pathologies in CXRs. We also assessed the quality of Xray2Xray's latent representations through synthesis tasks and demonstrated that the latent representations can be used to reconstruct volumetric context.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2506.10778.pdf' target='_blank'>https://arxiv.org/pdf/2506.10778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Li, Wan Han, Ning Lin, Yu-Liang Zhan, Ruizhi Chengze, Haining Wang, Yi Zhang, Hongsheng Liu, Zidong Wang, Fan Yu, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10778">SlotPi: Physics-informed Object-centric Reasoning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and reasoning about dynamics governed by physical laws through visual observation, akin to human capabilities in the real world, poses significant challenges. Currently, object-centric dynamic simulation methods, which emulate human behavior, have achieved notable progress but overlook two critical aspects: 1) the integration of physical knowledge into models. Humans gain physical insights by observing the world and apply this knowledge to accurately reason about various dynamic scenarios; 2) the validation of model adaptability across diverse scenarios. Real-world dynamics, especially those involving fluids and objects, demand models that not only capture object interactions but also simulate fluid flow characteristics. To address these gaps, we introduce SlotPi, a slot-based physics-informed object-centric reasoning model. SlotPi integrates a physical module based on Hamiltonian principles with a spatio-temporal prediction module for dynamic forecasting. Our experiments highlight the model's strengths in tasks such as prediction and Visual Question Answering (VQA) on benchmark and fluid datasets. Furthermore, we have created a real-world dataset encompassing object interactions, fluid dynamics, and fluid-object interactions, on which we validated our model's capabilities. The model's robust performance across all datasets underscores its strong adaptability, laying a foundation for developing more advanced world models.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2506.06355.pdf' target='_blank'>https://arxiv.org/pdf/2506.06355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyao Li, Dawei Li, Zhenhui Ou, Xiaoran Xu, Jingxiao Liu, Zihui Ma, Runlong Yu, Min Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06355">LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2506.06006.pdf' target='_blank'>https://arxiv.org/pdf/2506.06006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06006">Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To what extent do vision-and-language foundation models possess a realistic world model (observation $\times$ action $\rightarrow$ observation) and a dynamics model (observation $\times$ observation $\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2506.05217.pdf' target='_blank'>https://arxiv.org/pdf/2506.05217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Hu, Xuexiang Wen, Xi Li, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05217">DSG-World: Learning a 3D Gaussian World Model from Dual State Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building an efficient and physically consistent world model from limited observations is a long standing challenge in vision and robotics. Many existing world modeling pipelines are based on implicit generative models, which are hard to train and often lack 3D or physical consistency. On the other hand, explicit 3D methods built from a single state often require multi-stage processing-such as segmentation, background completion, and inpainting-due to occlusions. To address this, we leverage two perturbed observations of the same scene under different object configurations. These dual states offer complementary visibility, alleviating occlusion issues during state transitions and enabling more stable and complete reconstruction. In this paper, we present DSG-World, a novel end-to-end framework that explicitly constructs a 3D Gaussian World model from Dual State observations. Our approach builds dual segmentation-aware Gaussian fields and enforces bidirectional photometric and semantic consistency. We further introduce a pseudo intermediate state for symmetric alignment and design collaborative co-pruning trategies to refine geometric completeness. DSG-World enables efficient real-to-simulation transfer purely in the explicit Gaussian representation space, supporting high-fidelity rendering and object-level scene manipulation without relying on dense observations or multi-stage pipelines. Extensive experiments demonstrate strong generalization to novel views and scene states, highlighting the effectiveness of our approach for real-world 3D reconstruction and simulation.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2506.01600.pdf' target='_blank'>https://arxiv.org/pdf/2506.01600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa, Anirudha Majumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01600">WoMAP: World Models For Embodied Open-Vocabulary Object Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2505.01712.pdf' target='_blank'>https://arxiv.org/pdf/2505.01712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01712">World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2504.16680.pdf' target='_blank'>https://arxiv.org/pdf/2504.16680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Li, Andreas Krause, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16680">Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2504.03353.pdf' target='_blank'>https://arxiv.org/pdf/2504.03353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kentaro Nomura, Tatsuya Aoki, Tadahiro Taniguchi, Takato Horii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03353">Decentralized Collective World Model for Emergent Communication and Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2504.03353.pdf' target='_blank'>https://arxiv.org/pdf/2504.03353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kentaro Nomura, Tatsuya Aoki, Tadahiro Taniguchi, Takato Horii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03353">Decentralized Collective World Model for Emergent Communication and Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2504.03353.pdf' target='_blank'>https://arxiv.org/pdf/2504.03353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kentaro Nomura, Tatsuya Aoki, Tadahiro Taniguchi, Takato Horii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03353">Decentralized Collective World Model for Emergent Communication and Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents' internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2503.02143.pdf' target='_blank'>https://arxiv.org/pdf/2503.02143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02143">Four Principles for Physically Interpretable World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As autonomous systems are increasingly deployed in open and uncertain settings, there is a growing need for trustworthy world models that can reliably predict future high-dimensional observations. The learned latent representations in world models lack direct mapping to meaningful physical quantities and dynamics, limiting their utility and interpretability in downstream planning, control, and safety verification. In this paper, we argue for a fundamental shift from physically informed to physically interpretable world models - and crystallize four principles that leverage symbolic knowledge to achieve these ends: (1) functionally organizing the latent space according to the physical intent, (2) learning aligned invariant and equivariant representations of the physical world, (3) integrating multiple forms and strengths of supervision into a unified training process, and (4) partitioning generative outputs to support scalability and verifiability. We experimentally demonstrate the value of each principle on two benchmarks. This paper opens several intriguing research directions to achieve and capitalize on full physical interpretability in world models.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2503.00653.pdf' target='_blank'>https://arxiv.org/pdf/2503.00653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aidan Scannell, Mohammadreza Nakhaei, Kalle KujanpÃ¤Ã¤, Yi Zhao, Kevin Sebastian Luck, Arno Solin, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00653">Discrete Codebook World Models for Continuous Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While previous approaches leveraging discrete latent spaces, such as DreamerV3, have demonstrated strong performance in discrete action settings and visual control tasks, their comparative performance in state-based continuous control remains underexplored. In contrast, methods with continuous latent spaces, such as TD-MPC2, have shown notable success in state-based continuous control benchmarks. In this paper, we demonstrate that modeling discrete latent states has benefits over continuous latent states and that discrete codebook encodings are more effective representations for continuous control, compared to alternative encodings, such as one-hot and label-based encodings. Based on these insights, we introduce DCWM: Discrete Codebook World Model, a self-supervised world model with a discrete and stochastic latent space, where latent states are codes from a codebook. We combine DCWM with decision-time planning to get our model-based RL algorithm, named DC-MPC: Discrete Codebook Model Predictive Control, which performs competitively against recent state-of-the-art algorithms, including TD-MPC2 and DreamerV3, on continuous control benchmarks. See our project website www.aidanscannell.com/dcmpc.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2502.07591.pdf' target='_blank'>https://arxiv.org/pdf/2502.07591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07591">DMWM: Dual-Mind World Model with Long-Term Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2502.07309.pdf' target='_blank'>https://arxiv.org/pdf/2502.07309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Pengfei Li, Yupeng Zheng, Wei Sun, Yan Wang, Yilun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07309">Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding world dynamics is crucial for planning in autonomous driving. Recent methods attempt to achieve this by learning a 3D occupancy world model that forecasts future surrounding scenes based on current observation. However, 3D occupancy labels are still required to produce promising results. Considering the high annotation cost for 3D outdoor scenes, we propose a semi-supervised vision-centric 3D occupancy world model, PreWorld, to leverage the potential of 2D labels through a novel two-stage training paradigm: the self-supervised pre-training stage and the fully-supervised fine-tuning stage. Specifically, during the pre-training stage, we utilize an attribute projection head to generate different attribute fields of a scene (e.g., RGB, density, semantic), thus enabling temporal supervision from 2D labels via volume rendering techniques. Furthermore, we introduce a simple yet effective state-conditioned forecasting module to recursively forecast future occupancy and ego trajectory in a direct manner. Extensive experiments on the nuScenes dataset validate the effectiveness and scalability of our method, and demonstrate that PreWorld achieves competitive performance across 3D occupancy prediction, 4D occupancy forecasting and motion planning tasks.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2501.10100.pdf' target='_blank'>https://arxiv.org/pdf/2501.10100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Li, Andreas Krause, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10100">Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2412.12870.pdf' target='_blank'>https://arxiv.org/pdf/2412.12870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12870">Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models are increasingly employed for perception, prediction, and control in robotic systems. For for achieving realistic and consistent outputs, it is crucial to embed physical knowledge into their learned representations. However, doing so is difficult due to high-dimensional observation data, such as images, particularly under conditions of incomplete system knowledge and imprecise state sensing. To address this, we propose Physically Interpretable World Models, a novel architecture that aligns learned latent representations with real-world physical quantities. To this end, our architecture combines three key elements: (1) a vector-quantized image autoencoder, (2) a transformer-based physically interpretable autoencoder, and (3) a partially known dynamical model. The training incorporates weak interval-based supervision to eliminate the impractical reliance on ground-truth physical knowledge. Three case studies demonstrate that our approach achieves physical interpretability and accurate state predictions, thus advancing representation learning for robotics.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2412.03572.pdf' target='_blank'>https://arxiv.org/pdf/2412.03572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03572">Navigation World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2410.13787.pdf' target='_blank'>https://arxiv.org/pdf/2410.13787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13787">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.
  We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).
  In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2410.13569.pdf' target='_blank'>https://arxiv.org/pdf/2410.13569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13569">Learning on Model Weights using Tree Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The number of publicly available models is rapidly increasing, yet most remain undocumented. Users looking for suitable models for their tasks must first determine what each model does. Training machine learning models to infer missing documentation directly from model weights is challenging, as these weights often contain significant variation unrelated to model functionality (denoted nuisance). Here, we identify a key property of real-world models: most public models belong to a small set of Model Trees, where all models within a tree are fine-tuned from a common ancestor (e.g., a foundation model). Importantly, we find that within each tree there is less nuisance variation between models. Concretely, while learning across Model Trees requires complex architectures, even a linear classifier trained on a single model layer often works within trees. While effective, these linear classifiers are computationally expensive, especially when dealing with larger models that have many parameters. To address this, we introduce Probing Experts (ProbeX), a theoretically motivated and lightweight method. Notably, ProbeX is the first probing method specifically designed to learn from the weights of a single hidden model layer. We demonstrate the effectiveness of ProbeX by predicting the categories in a model's training dataset based only on its weights. Excitingly, ProbeX can map the weights of Stable Diffusion into a weight-language embedding space, enabling model search via text, i.e., zero-shot model classification.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2409.11356.pdf' target='_blank'>https://arxiv.org/pdf/2409.11356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11356">RenderWorld: World Model with Self-Supervised 3D Label</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving with vision-only is not only more cost-effective compared to LiDAR-vision fusion but also more reliable than traditional methods. To achieve a economical and robust purely visual autonomous driving system, we propose RenderWorld, a vision-only end-to-end autonomous driving framework, which generates 3D occupancy labels using a self-supervised gaussian-based Img2Occ Module, then encodes the labels by AM-VAE, and uses world model for forecasting and planning. RenderWorld employs Gaussian Splatting to represent 3D scenes and render 2D images greatly improves segmentation accuracy and reduces GPU memory consumption compared with NeRF-based methods. By applying AM-VAE to encode air and non-air separately, RenderWorld achieves more fine-grained scene element representation, leading to state-of-the-art performance in both 4D occupancy forecasting and motion planning from autoregressive world model.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2406.14540.pdf' target='_blank'>https://arxiv.org/pdf/2406.14540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, Tao Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14540">IRASim: A Fine-Grained World Model for Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models allow autonomous agents to plan and explore by predicting the visual outcomes of different actions. However, for robot manipulation, it is challenging to accurately model the fine-grained robot-object interaction within the visual space using existing methods which overlooks precise alignment between each action and the corresponding frame. In this paper, we present IRASim, a novel world model capable of generating videos with fine-grained robot-object interaction details, conditioned on historical observations and robot action trajectories. We train a diffusion transformer and introduce a novel frame-level action-conditioning module within each transformer block to explicitly model and strengthen the action-frame alignment. Extensive experiments show that: (1) the quality of the videos generated by our method surpasses all the baseline methods and scales effectively with increased model size and computation; (2) policy evaluations using IRASim exhibit a strong correlation with those using the ground-truth simulator, highlighting its potential to accelerate real-world policy evaluation; (3) testing-time scaling through model-based planning with IRASim significantly enhances policy performance, as evidenced by an improvement in the IoU metric on the Push-T benchmark from 0.637 to 0.961; (4) IRASim provides flexible action controllability, allowing virtual robotic arms in datasets to be controlled via a keyboard or VR controller.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2405.18418.pdf' target='_blank'>https://arxiv.org/pdf/2405.18418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicklas Hansen, Jyothir S, Vlad Sobal, Yann LeCun, Xiaolong Wang, Hao Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18418">Hierarchical World Models as Visual Whole-Body Humanoid Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2404.00462.pdf' target='_blank'>https://arxiv.org/pdf/2404.00462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Siqi Dai, Yuang Geng, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00462">Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by comparing estimated states instead of aggregating observation-wide error.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2403.04253.pdf' target='_blank'>https://arxiv.org/pdf/2403.04253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04253">Mastering Memory Tasks with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2401.17835.pdf' target='_blank'>https://arxiv.org/pdf/2401.17835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tankred Saanum, Peter Dayan, Eric Schulz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17835">Simplifying Latent Dynamics with Softly State-Invariant World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To solve control problems via model-based reasoning or planning, an agent needs to know how its actions affect the state of the world. The actions an agent has at its disposal often change the state of the environment in systematic ways. However, existing techniques for world modelling do not guarantee that the effect of actions are represented in such systematic ways. We introduce the Parsimonious Latent Space Model (PLSM), a world model that regularizes the latent dynamics to make the effect of the agent's actions more predictable. Our approach minimizes the mutual information between latent states and the change that an action produces in the agent's latent state, in turn minimizing the dependence the state has on the dynamics. This makes the world model softly state-invariant. We combine PLSM with different model classes used for i) future latent state prediction, ii) planning, and iii) model-free reinforcement learning. We find that our regularization improves accuracy, generalization, and performance in downstream tasks, highlighting the importance of systematic treatment of actions in world models.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2401.16972.pdf' target='_blank'>https://arxiv.org/pdf/2401.16972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Savant Aira, Diego Valsesia, Andrea Bordone Molini, Giulia Fracastoro, Enrico Magli, Andrea Mirabile
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16972">Deep 3D World Models for Multi-Image Super-Resolution Beyond Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-image super-resolution (MISR) allows to increase the spatial resolution of a low-resolution (LR) acquisition by combining multiple images carrying complementary information in the form of sub-pixel offsets in the scene sampling, and can be significantly more effective than its single-image counterpart. Its main difficulty lies in accurately registering and fusing the multi-image information. Currently studied settings, such as burst photography, typically involve assumptions of small geometric disparity between the LR images and rely on optical flow for image registration. We study a MISR method that can increase the resolution of sets of images acquired with arbitrary, and potentially wildly different, camera positions and orientations, generalizing the currently studied MISR settings. Our proposed model, called EpiMISR, moves away from optical flow and explicitly uses the epipolar geometry of the acquisition process, together with transformer-based processing of radiance feature fields to substantially improve over state-of-the-art MISR methods in presence of large disparities in the LR images.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2310.16828.pdf' target='_blank'>https://arxiv.org/pdf/2310.16828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicklas Hansen, Hao Su, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16828">TD-MPC2: Scalable, Robust World Models for Continuous Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2308.12252.pdf' target='_blank'>https://arxiv.org/pdf/2308.12252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12252">How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a cartpole.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2308.10901.pdf' target='_blank'>https://arxiv.org/pdf/2308.10901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Russell Mendonca, Shikhar Bahl, Deepak Pathak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10901">Structured World Models from Human Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2303.09534.pdf' target='_blank'>https://arxiv.org/pdf/2303.09534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mai Nishimura, Shohei Nobuhara, Ko Nishino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09534">InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce an on-ground Pedestrian World Model, a computational model that can predict how pedestrians move around an observer in the crowd on the ground plane, but from just the egocentric-views of the observer. Our model, InCrowdFormer, fully leverages the Transformer architecture by modeling pedestrian interaction and egocentric to top-down view transformation with attention, and autoregressively predicts on-ground positions of a variable number of people with an encoder-decoder architecture. We encode the uncertainties arising from unknown pedestrian heights with latent codes to predict the posterior distributions of pedestrian positions. We validate the effectiveness of InCrowdFormer on a novel prediction benchmark of real movements. The results show that InCrowdFormer accurately predicts the future coordination of pedestrians. To the best of our knowledge, InCrowdFormer is the first-of-its-kind pedestrian world model which we believe will benefit a wide range of egocentric-view applications including crowd navigation, tracking, and synthesis.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2303.08690.pdf' target='_blank'>https://arxiv.org/pdf/2303.08690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Harm van Seijen, Sarath Chandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08690">Replay Buffer with Local Forgetting for Adapting to Local Environment Changes in Deep Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment, a particular form of adaptivity that is the focus of this work. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to local environment changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out replay buffer precludes effective adaptation due to maintaining stale data. In this work, we show that a conceptually simple variation of this traditional replay buffer is able to overcome this limitation. By removing only samples from the buffer from the local neighbourhood of the newly observed samples, deep world models can be built that maintain their accuracy across the state-space, while also being able to effectively adapt to local changes in the reward function. We demonstrate this by applying our replay-buffer variation to a deep version of the classical Dyna method, as well as to recent methods such as PlaNet and DreamerV2, demonstrating that deep model-based methods can adapt effectively as well to local changes in the environment.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2211.14860.pdf' target='_blank'>https://arxiv.org/pdf/2211.14860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Snir Vitrack Tamam, Raz Lapid, Moshe Sipper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14860">Foiling Explanations in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our method's performance on two benchmark datasets -- CIFAR100 and ImageNet -- using four different pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet, MobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can be manipulated without the use of gradients or other model internals. Our novel algorithm is successfully able to manipulate an image in a manner imperceptible to the human eye, such that the XAI method outputs a specific explanation map. To our knowledge, this is the first such method in a black-box setting, and we believe it has significant value where explainability is desired, required, or legally mandatory.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2510.06492.pdf' target='_blank'>https://arxiv.org/pdf/2510.06492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Kim, Kensuke Nakamura, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06492">What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe control techniques, such as Hamilton-Jacobi reachability, provide principled methods for synthesizing safety-preserving robot policies but typically assume hand-designed state spaces and full observability. Recent work has relaxed these assumptions via latent-space safe control, where state representations and dynamics are learned jointly through world models that reconstruct future high-dimensional observations (e.g., RGB images) from current observations and actions. This enables safety constraints that are difficult to specify analytically (e.g., spilling) to be framed as classification problems in latent space, allowing controllers to operate directly from raw observations. However, these methods assume that safety-critical features are observable in the learned latent state. We ask: when are latent state spaces sufficient for safe control? To study this, we examine temperature-based failures, comparable to overheating in cooking or manufacturing tasks, and find that RGB-only observations can produce myopic safety behaviors, e.g., avoiding seeing failure states rather than preventing failure itself. To predict such behaviors, we introduce a mutual information-based measure that identifies when observations fail to capture safety-relevant features. Finally, we propose a multimodal-supervised training strategy that shapes the latent state with additional sensory inputs during training, but requires no extra modalities at deployment, and validate our approach in simulation and on hardware with a Franka Research 3 manipulator preventing a pot of wax from overheating.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2510.06492.pdf' target='_blank'>https://arxiv.org/pdf/2510.06492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Kim, Kensuke Nakamura, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06492">What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe control techniques, such as Hamilton-Jacobi reachability, provide principled methods for synthesizing safety-preserving robot policies but typically assume hand-designed state spaces and full observability. Recent work has relaxed these assumptions via latent-space safe control, where state representations and dynamics are learned jointly through world models that reconstruct future high-dimensional observations (e.g., RGB images) from current observations and actions. This enables safety constraints that are difficult to specify analytically (e.g., spilling) to be framed as classification problems in latent space, allowing controllers to operate directly from raw observations. However, these methods assume that safety-critical features are observable in the learned latent state. We ask: when are latent state spaces sufficient for safe control? To study this, we examine temperature-based failures, comparable to overheating in cooking or manufacturing tasks, and find that RGB-only observations can produce myopic safety behaviors, e.g., avoiding seeing failure states rather than preventing failure itself. To predict such behaviors, we introduce a mutual information-based measure that identifies when observations fail to capture safety-relevant features. Finally, we propose a multimodal-supervised training strategy that shapes the latent state with additional sensory inputs during training, but requires no extra modalities at deployment, and validate our approach in simulation and on hardware with a Franka Research 3 manipulator preventing a pot of wax from overheating.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2510.05865.pdf' target='_blank'>https://arxiv.org/pdf/2510.05865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Baraldi, Zifan Zeng, Chongzhe Zhang, Aradhana Nayak, Hongbo Zhu, Feng Liu, Qunli Zhang, Peng Wang, Shiming Liu, Zheng Hu, Angelo Cangelosi, Lorenzo Baraldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05865">The Safety Challenge of World Models for Embodied AI Agents: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2510.05865.pdf' target='_blank'>https://arxiv.org/pdf/2510.05865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Baraldi, Zifan Zeng, Chongzhe Zhang, Aradhana Nayak, Hongbo Zhu, Feng Liu, Qunli Zhang, Peng Wang, Shiming Liu, Zheng Hu, Angelo Cangelosi, Lorenzo Baraldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05865">The Safety Challenge of World Models for Embodied AI Agents: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2509.21657.pdf' target='_blank'>https://arxiv.org/pdf/2509.21657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiang Dai, Fan Jiang, Chiyu Wang, Mu Xu, Yonggang Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21657">FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2509.21657.pdf' target='_blank'>https://arxiv.org/pdf/2509.21657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiang Dai, Fan Jiang, Chiyu Wang, Mu Xu, Yonggang Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21657">FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2509.19555.pdf' target='_blank'>https://arxiv.org/pdf/2509.19555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sankalp Agrawal, Junwon Seo, Kensuke Nakamura, Ran Tian, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19555">AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown that foundational safe control methods, such as Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter's adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model's imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on https://any-safe.github.io
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.19555.pdf' target='_blank'>https://arxiv.org/pdf/2509.19555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sankalp Agrawal, Junwon Seo, Kensuke Nakamura, Ran Tian, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19555">AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown that foundational safe control methods, such as Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter's adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model's imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on https://any-safe.github.io
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2507.12496.pdf' target='_blank'>https://arxiv.org/pdf/2507.12496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucen Wang, Rui Yu, Shenghua Wan, Le Gan, De-Chuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12496">FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is https://sites.google.com/view/founder-rl.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2506.09985.pdf' target='_blank'>https://arxiv.org/pdf/2506.09985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09985">V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2505.21996.pdf' target='_blank'>https://arxiv.org/pdf/2505.21996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taiye Chen, Xun Hu, Zihan Ding, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21996">Learning World Models for Interactive Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2503.10241.pdf' target='_blank'>https://arxiv.org/pdf/2503.10241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dimitri Ognibene, Sabrina Patania, Luca Annese, Cansu Koyuturk, Franca Garzotto, Giuseppe Vizzari, Azzurra Ruggeri, Simone Colombani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10241">SCOOP: A Framework for Proactive Collaboration and Social Continual Learning through Natural Language Interaction andCausal Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal information-gathering settings, where users collaborate with AI in dynamic environments, are increasingly common. These involve complex processes with textual and multimodal interactions, often requiring additional structural information via cost-incurring requests. AI helpers lack access to users' true goals, beliefs, and preferences and struggle to integrate diverse information effectively.
  We propose a social continual learning framework for causal knowledge acquisition and collaborative decision-making. It focuses on autonomous agents learning through dialogues, question-asking, and interaction in open, partially observable environments. A key component is a natural language oracle that answers the agent's queries about environmental mechanisms and states, refining causal understanding while balancing exploration or learning, and exploitation or knowledge use.
  Evaluation tasks inspired by developmental psychology emphasize causal reasoning and question-asking skills. They complement benchmarks by assessing the agent's ability to identify knowledge gaps, generate meaningful queries, and incrementally update reasoning. The framework also evaluates how knowledge acquisition costs are amortized across tasks within the same environment.
  We propose two architectures: 1) a system combining Large Language Models (LLMs) with the ReAct framework and question-generation, and 2) an advanced system with a causal world model, symbolic, graph-based, or subsymbolic, for reasoning and decision-making. The latter builds a causal knowledge graph for efficient inference and adaptability under constraints. Challenges include integrating causal reasoning into ReAct and optimizing exploration and question-asking in error-prone scenarios. Beyond applications, this framework models developmental processes combining causal reasoning, question generation, and social learning.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2503.09817.pdf' target='_blank'>https://arxiv.org/pdf/2503.09817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jesse Farebrother, Matteo Pirotta, Andrea Tirinzoni, RÃ©mi Munos, Alessandro Lazaric, Ahmed Touati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09817">Temporal Difference Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Predictive models of the future are fundamental for an agent's ability to reason and plan. A common strategy learns a world model and unrolls it step-by-step at inference, where small errors can rapidly compound. Geometric Horizon Models (GHMs) offer a compelling alternative by directly making predictions of future states, avoiding cumulative inference errors. While GHMs can be conveniently learned by a generative analog to temporal difference (TD) learning, existing methods are negatively affected by bootstrapping predictions at train time and struggle to generate high-quality predictions at long horizons. This paper introduces Temporal Difference Flows (TD-Flow), which leverages the structure of a novel Bellman equation on probability paths alongside flow-matching techniques to learn accurate GHMs at over 5x the horizon length of prior methods. Theoretically, we establish a new convergence result and primarily attribute TD-Flow's efficacy to reduced gradient variance during training. We further show that similar arguments can be extended to diffusion-based methods. Empirically, we validate TD-Flow across a diverse set of domains on both generative metrics and downstream tasks including policy evaluation. Moreover, integrating TD-Flow with recent behavior foundation models for planning over pre-trained policies demonstrates substantial performance gains, underscoring its promise for long-horizon decision-making.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2502.03072.pdf' target='_blank'>https://arxiv.org/pdf/2502.03072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqi Huang, Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Luhui Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03072">RoboGrasp: A Universal Grasping Policy for Robust Robotic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning and world models have shown significant promise in advancing generalizable robotic learning, with robotic grasping remaining a critical challenge for achieving precise manipulation. Existing methods often rely heavily on robot arm state data and RGB images, leading to overfitting to specific object shapes or positions. To address these limitations, we propose RoboGrasp, a universal grasping policy framework that integrates pretrained grasp detection models with robotic learning. By leveraging robust visual guidance from object detection and segmentation tasks, RoboGrasp significantly enhances grasp precision, stability, and generalizability, achieving up to 34% higher success rates in few-shot learning and grasping box prompt tasks. Built on diffusion-based methods, RoboGrasp is adaptable to various robotic learning paradigms, enabling precise and reliable manipulation across diverse and complex scenarios. This framework represents a scalable and versatile solution for tackling real-world challenges in robotic grasping.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2502.00935.pdf' target='_blank'>https://arxiv.org/pdf/2502.00935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kensuke Nakamura, Lasse Peters, Andrea Bajcsy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00935">Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision-avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. In this work, our goal is to generalize safe robot controllers to prevent failures that are hard--if not impossible--to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag. We propose Latent Safety Filters, a latent-space generalization of HJ reachability that tractably operates directly on raw observation data (e.g., RGB images) to automatically compute safety-preserving actions without explicit recovery demonstrations by performing safety analysis in the latent embedding space of a generative world model. Our method leverages diverse robot observation-action data of varying quality (including successes, random exploration, and unsafe demonstrations) to learn a world model. Constraint specification is then transformed into a classification problem in the latent space of the learned world model. In simulation and hardware experiments, we compute an approximation of Latent Safety Filters to safeguard arbitrary policies (from imitation- learned policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag or toppling cluttered objects.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2502.00466.pdf' target='_blank'>https://arxiv.org/pdf/2502.00466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Hua Lee, Bor-Jiun Lin, Wei-Fang Sun, Chun-Yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00466">EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models represent a promising approach for training reinforcement learning agents with significantly improved sample efficiency. While most world model methods primarily rely on sequences of discrete latent variables to model environment dynamics, this compression often neglects critical visual details essential for reinforcement learning. Recent diffusion-based world models condition generation on a fixed context length of frames to predict the next observation, using separate recurrent neural networks to model rewards and termination signals. Although this architecture effectively enhances visual fidelity, the fixed context length approach inherently limits memory capacity. In this paper, we introduce EDELINE, a unified world model architecture that integrates state space models with diffusion models. Our approach outperforms existing baselines across visually challenging Atari 100k tasks, memory-demanding Crafter benchmark, and 3D first-person ViZDoom environments, demonstrating superior performance in all these diverse challenges.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2501.07468.pdf' target='_blank'>https://arxiv.org/pdf/2501.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07468">From Screens to Scenes: A Survey of Embodied AI in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Healthcare systems worldwide face persistent challenges in efficiency, accessibility, and personalization. Powered by modern AI technologies such as multimodal large language models and world models, Embodied AI (EmAI) represents a transformative frontier, offering enhanced autonomy and the ability to interact with the physical world to address these challenges. As an interdisciplinary and rapidly evolving research domain, "EmAI in healthcare" spans diverse fields such as algorithms, robotics, and biomedicine. This complexity underscores the importance of timely reviews and analyses to track advancements, address challenges, and foster cross-disciplinary collaboration. In this paper, we provide a comprehensive overview of the "brain" of EmAI for healthcare, wherein we introduce foundational AI algorithms for perception, actuation, planning, and memory, and focus on presenting the healthcare applications spanning clinical interventions, daily care & companionship, infrastructure support, and biomedical research. Despite its promise, the development of EmAI for healthcare is hindered by critical challenges such as safety concerns, gaps between simulation platforms and real-world applications, the absence of standardized benchmarks, and uneven progress across interdisciplinary domains. We discuss the technical barriers and explore ethical considerations, offering a forward-looking perspective on the future of EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI systems is also introduced to guide further development. By providing systematic insights, this work aims to inspire innovation and practical applications, paving the way for a new era of intelligent, patient-centered healthcare.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2412.09237.pdf' target='_blank'>https://arxiv.org/pdf/2412.09237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijun Liu, Wu Liu, Xiaoyan Gu, Yong Rui, Xiaodong He, Yongdong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09237">LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The believable simulation of multi-user behavior is crucial for understanding complex social systems. Recently, large language models (LLMs)-based AI agents have made significant progress, enabling them to achieve human-like intelligence across various tasks. However, real human societies are often dynamic and complex, involving numerous individuals engaging in multimodal interactions. In this paper, taking e-commerce scenarios as an example, we present LMAgent, a very large-scale and multimodal agents society based on multimodal LLMs. In LMAgent, besides freely chatting with friends, the agents can autonomously browse, purchase, and review products, even perform live streaming e-commerce. To simulate this complex system, we introduce a self-consistency prompting mechanism to augment agents' multimodal capabilities, resulting in significantly improved decision-making performance over the existing multi-agent system. Moreover, we propose a fast memory mechanism combined with the small-world model to enhance system efficiency, which supports more than 10,000 agent simulations in a society. Experiments on agents' behavior show that these agents achieve comparable performance to humans in behavioral indicators. Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2410.10062.pdf' target='_blank'>https://arxiv.org/pdf/2410.10062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan DeCastro, Andrew Silva, Deepak Gopinath, Emily Sumner, Thomas M. Balch, Laporsha Dees, Guy Rosman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10062">Dreaming to Assist: Learning to Align with Human Objectives for Shared Control in High-Speed Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tight coordination is required for effective human-robot teams in domains involving fast dynamics and tactical decisions, such as multi-car racing. In such settings, robot teammates must react to cues of a human teammate's tactical objective to assist in a way that is consistent with the objective (e.g., navigating left or right around an obstacle). To address this challenge, we present Dream2Assist, a framework that combines a rich world model able to infer human objectives and value functions, and an assistive agent that provides appropriate expert assistance to a given human teammate. Our approach builds on a recurrent state space model to explicitly infer human intents, enabling the assistive agent to select actions that align with the human and enabling a fluid teaming interaction. We demonstrate our approach in a high-speed racing domain with a population of synthetic human drivers pursuing mutually exclusive objectives, such as "stay-behind" and "overtake". We show that the combined human-robot team, when blending its actions with those of the human, outperforms the synthetic humans alone as well as several baseline assistance strategies, and that intent-conditioning enables adherence to human preferences during task execution, leading to improved performance while satisfying the human's objective.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2410.08172.pdf' target='_blank'>https://arxiv.org/pdf/2410.08172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Chen, Botian Xu, Pu Hua, Peiqi Duan, Yanchao Yang, Yi Ma, Huazhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08172">On the Evaluation of Generative Robotic Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the difficulty of acquiring extensive real-world data, robot simulation has become crucial for parallel training and sim-to-real transfer, highlighting the importance of scalable simulated robotic tasks. Foundation models have demonstrated impressive capacities in autonomously generating feasible robotic tasks. However, this new paradigm underscores the challenge of adequately evaluating these autonomously generated tasks. To address this, we propose a comprehensive evaluation framework tailored to generative simulations. Our framework segments evaluation into three core aspects: quality, diversity, and generalization. For single-task quality, we evaluate the realism of the generated task and the completeness of the generated trajectories using large language models and vision-language models. In terms of diversity, we measure both task and data diversity through text similarity of task descriptions and world model loss trained on collected task trajectories. For task-level generalization, we assess the zero-shot generalization ability on unseen tasks of a policy trained with multiple generated tasks. Experiments conducted on three representative task generation pipelines demonstrate that the results from our framework are highly consistent with human evaluations, confirming the feasibility and validity of our approach. The findings reveal that while metrics of quality and diversity can be achieved through certain methods, no single approach excels across all metrics, suggesting a need for greater focus on balancing these different metrics. Additionally, our analysis further highlights the common challenge of low generalization capability faced by current works. Our anonymous website: https://sites.google.com/view/evaltasks.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2410.07836.pdf' target='_blank'>https://arxiv.org/pdf/2410.07836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07836">Masked Generative Priors Improve World Models Sequence Modelling Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2410.02664.pdf' target='_blank'>https://arxiv.org/pdf/2410.02664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, Xuguang Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02664">Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2409.10196.pdf' target='_blank'>https://arxiv.org/pdf/2409.10196.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixi Cai, Cristian Rojas Cardenas, Kevin Leo, Chenyuan Zhang, Kal Backman, Hanbing Li, Boying Li, Mahsa Ghorbanali, Stavya Datta, Lizhen Qu, Julian Gutierrez Santiago, Alexey Ignatiev, Yuan-Fang Li, Mor Vered, Peter J Stuckey, Maria Garcia de la Banda, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10196">NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of autonomous UAV search missions, where a UAV must locate specific Entities of Interest (EOIs) within a time limit, based on brief descriptions in large, hazard-prone environments with keep-out zones. The UAV must perceive, reason, and make decisions with limited and uncertain information. We propose NEUSIS, a compositional neuro-symbolic system designed for interpretable UAV search and navigation in realistic scenarios. NEUSIS integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to process raw sensory inputs, maintains a probabilistic world model for environment representation, and uses a hierarchical planning component (SNaC) for efficient path planning. Experimental results from simulated urban search missions using AirSim and Unreal Engine show that NEUSIS outperforms a state-of-the-art (SOTA) vision-language model and a SOTA search planning model in success rate, search efficiency, and 3D localization. These results demonstrate the effectiveness of our compositional neuro-symbolic approach in handling complex, real-world scenarios, making it a promising solution for autonomous UAV systems in search missions.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2405.06624.pdf' target='_blank'>https://arxiv.org/pdf/2405.06624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David "davidad" Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, Joshua Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06624">Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2404.03386.pdf' target='_blank'>https://arxiv.org/pdf/2404.03386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaichen Huang, Minghao Shao, Shenghua Wan, Hai-Hang Sun, Shuai Feng, Le Gan, De-Chuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03386">SENSOR: Imitate Third-Person Expert's Behaviors via Active Sensoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world visual Imitation Learning (IL) scenarios, there is a misalignment between the agent's and the expert's perspectives, which might lead to the failure of imitation. Previous methods have generally solved this problem by domain alignment, which incurs extra computation and storage costs, and these methods fail to handle the \textit{hard cases} where the viewpoint gap is too large. To alleviate the above problems, we introduce active sensoring in the visual IL setting and propose a model-based SENSory imitatOR (SENSOR) to automatically change the agent's perspective to match the expert's. SENSOR jointly learns a world model to capture the dynamics of latent states, a sensor policy to control the camera, and a motor policy to control the agent. Experiments on visual locomotion tasks show that SENSOR can efficiently simulate the expert's perspective and strategy, and outperforms most baseline methods.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2403.09976.pdf' target='_blank'>https://arxiv.org/pdf/2403.09976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucen Wang, Shenghua Wan, Le Gan, Shuai Feng, De-Chuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09976">AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based methods have significantly contributed to distinguishing task-irrelevant distractors for visual control. However, prior research has primarily focused on heterogeneous distractors like noisy background videos, leaving homogeneous distractors that closely resemble controllable agents largely unexplored, which poses significant challenges to existing methods. To tackle this problem, we propose Implicit Action Generator (IAG) to learn the implicit actions of visual distractors, and present a new algorithm named implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that leverages the action inferred by IAG to train separated world models. Implicit actions effectively capture the behavior of background distractors, aiding in distinguishing the task-irrelevant components, and the agent can optimize the policy within the task-relevant state space. Our method achieves superior performance on various visual control tasks featuring both heterogeneous and homogeneous distractors. The indispensable role of implicit actions learned by IAG is also empirically validated.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2403.01962.pdf' target='_blank'>https://arxiv.org/pdf/2403.01962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Shi, Tingguang Li, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Q. -H. Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01962">An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based methods have improved locomotion skills of quadruped robots through deep reinforcement learning. However, the sim-to-real gap and low sample efficiency still limit the skill transfer. To address this issue, we propose an efficient model-based learning framework that combines a world model with a policy network. We train a differentiable world model to predict future states and use it to directly supervise a Variational Autoencoder (VAE)-based policy network to imitate real animal behaviors. This significantly reduces the need for real interaction data and allows for rapid policy updates. We also develop a high-level network to track diverse commands and trajectories. Our simulated results show a tenfold sample efficiency increase compared to reinforcement learning methods such as PPO. In real-world testing, our policy achieves proficient command-following performance with only a two-minute data collection period and generalizes well to new speeds and paths.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2403.00504.pdf' target='_blank'>https://arxiv.org/pdf/2403.00504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, Yann LeCun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00504">Learning and Leveraging World Models in Visual Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2312.16815.pdf' target='_blank'>https://arxiv.org/pdf/2312.16815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Yuan, Zhang Jiang, Aobo Lyu, Jiayun Wu, Zhipeng Wang, Mingzhe Yang, Kaiwei Liu, Muyun Mou, Peng Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16815">Emergence and Causality in Complex Systems: A Survey on Causal Emergence and Related Quantitative Studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergence and causality are two fundamental concepts for understanding complex systems. They are interconnected. On one hand, emergence refers to the phenomenon where macroscopic properties cannot be solely attributed to the cause of individual properties. On the other hand, causality can exhibit emergence, meaning that new causal laws may arise as we increase the level of abstraction. Causal emergence theory aims to bridge these two concepts and even employs measures of causality to quantify emergence. This paper provides a comprehensive review of recent advancements in quantitative theories and applications of causal emergence. Two key problems are addressed: quantifying causal emergence and identifying it in data. Addressing the latter requires the use of machine learning techniques, thus establishing a connection between causal emergence and artificial intelligence. We highlighted that the architectures used for identifying causal emergence are shared by causal representation learning, causal model abstraction, and world model-based reinforcement learning. Consequently, progress in any of these areas can benefit the others. Potential applications and future perspectives are also discussed in the final section of the review.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2312.01203.pdf' target='_blank'>https://arxiv.org/pdf/2312.01203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edan Meyer, Adam White, Marlos C. Machado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01203">Harnessing Discrete Representations For Continual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents make decisions using nothing but observations from the environment, and consequently, heavily rely on the representations of those observations. Though some recent breakthroughs have used vector-based categorical representations of observations, often referred to as discrete representations, there is little work explicitly assessing the significance of such a choice. In this work, we provide a thorough empirical investigation of the advantages of representing observations as vectors of categorical values within the context of reinforcement learning. We perform evaluations on world-model learning, model-free RL, and ultimately continual RL problems, where the benefits best align with the needs of the problem setting. We find that, when compared to traditional continuous representations, world models learned over discrete representations accurately model more of the world with less capacity, and that agents trained with discrete representations learn better policies with less data. In the context of continual RL, these benefits translate into faster adapting agents. Additionally, our analysis suggests that the observed performance improvements can be attributed to the information contained within the latent vectors and potentially the encoding of the discrete representation itself.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2312.00215.pdf' target='_blank'>https://arxiv.org/pdf/2312.00215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jean-FranÃ§ois Tremblay, David Meger, Francois Hogan, Gregory Dudek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00215">Learning active tactile perception through belief-space control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots operating in an open world will encounter novel objects with unknown physical properties, such as mass, friction, or size. These robots will need to sense these properties through interaction prior to performing downstream tasks with the objects. We propose a method that autonomously learns tactile exploration policies by developing a generative world model that is leveraged to 1) estimate the object's physical parameters using a differentiable Bayesian filtering algorithm and 2) develop an exploration policy using an information-gathering model predictive controller. We evaluate our method on three simulated tasks where the goal is to estimate a desired object property (mass, height or toppling height) through physical interaction. We find that our method is able to discover policies that efficiently gather information about the desired property in an intuitive manner. Finally, we validate our method on a real robot system for the height estimation task, where our method is able to successfully learn and execute an information-gathering policy from scratch.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2304.01826.pdf' target='_blank'>https://arxiv.org/pdf/2304.01826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineel Nagisetty, Laura Graves, Guanting Pan, Piyush Jha, Vijay Ganesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01826">CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new Deep Neural Network (DNN) testing algorithm called the Constrained Gradient Descent (CGD) method, and an implementation we call CGDTest aimed at exposing security and robustness issues such as adversarial robustness and bias in DNNs. Our CGD algorithm is a gradient-descent (GD) method, with the twist that the user can also specify logical properties that characterize the kinds of inputs that the user may want. This functionality sets CGDTest apart from other similar DNN testing tools since it allows users to specify logical constraints to test DNNs not only for $\ell_p$ ball-based adversarial robustness but, more importantly, includes richer properties such as disguised and flow adversarial constraints, as well as adversarial robustness in the NLP domain. We showcase the utility and power of CGDTest via extensive experimentation in the context of vision and NLP domains, comparing against 32 state-of-the-art methods over these diverse domains. Our results indicate that CGDTest outperforms state-of-the-art testing tools for $\ell_p$ ball-based adversarial robustness, and is significantly superior in testing for other adversarial robustness, with improvements in PAR2 scores of over 1500% in some cases over the next best tool. Our evaluation shows that our CGD method outperforms competing methods we compared against in terms of expressibility (i.e., a rich constraint language and concomitant tool support to express a wide variety of properties), scalability (i.e., can be applied to very large real-world models with up to 138 million parameters), and generality (i.e., can be used to test a plethora of model architectures).
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2302.07457.pdf' target='_blank'>https://arxiv.org/pdf/2302.07457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siliang Zeng, Chenliang Li, Alfredo Garcia, Mingyi Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07457">When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world'' model). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally, we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2510.04542.pdf' target='_blank'>https://arxiv.org/pdf/2510.04542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04542">Code World Models for General Game Playing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2510.04542.pdf' target='_blank'>https://arxiv.org/pdf/2510.04542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wolfgang Lehrach, Daniel Hennes, Miguel Lazaro-Gredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, John Schultz, Marcus Chiam, Ian Gemp, Piotr Zielinski, Satinder Singh, Kevin P. Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04542">Code World Models for General Game Playing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2510.04374.pdf' target='_blank'>https://arxiv.org/pdf/2510.04374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, Jerry Tworek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04374">GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GDPval, a benchmark evaluating AI model capabilities on real-world economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. We find that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. We analyze the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. We also demonstrate that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, we open-source a gold subset of 220 tasks and provide a public automated grading service at evals.openai.com to facilitate future research in understanding real-world model capabilities.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2510.04374.pdf' target='_blank'>https://arxiv.org/pdf/2510.04374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, Jerry Tworek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04374">GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce GDPval, a benchmark evaluating AI model capabilities on real-world economically valuable tasks. GDPval covers the majority of U.S. Bureau of Labor Statistics Work Activities for 44 occupations across the top 9 sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are constructed from the representative work of industry professionals with an average of 14 years of experience. We find that frontier model performance on GDPval is improving roughly linearly over time, and that the current best frontier models are approaching industry experts in deliverable quality. We analyze the potential for frontier models, when paired with human oversight, to perform GDPval tasks cheaper and faster than unaided experts. We also demonstrate that increased reasoning effort, increased task context, and increased scaffolding improves model performance on GDPval. Finally, we open-source a gold subset of 220 tasks and provide a public automated grading service at evals.openai.com to facilitate future research in understanding real-world model capabilities.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2509.20623.pdf' target='_blank'>https://arxiv.org/pdf/2509.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satyajeet Das, Darren Chiu, Zhehui Huang, Lars Lindemann, Gaurav S. Sukhatme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20623">Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2509.20623.pdf' target='_blank'>https://arxiv.org/pdf/2509.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satyajeet Das, Darren Chiu, Zhehui Huang, Lars Lindemann, Gaurav S. Sukhatme
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20623">Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2509.15915.pdf' target='_blank'>https://arxiv.org/pdf/2509.15915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15915">Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2509.15915.pdf' target='_blank'>https://arxiv.org/pdf/2509.15915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15915">Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2508.19851.pdf' target='_blank'>https://arxiv.org/pdf/2508.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Romain Harang, Jason Naradowsky, Yaswitha Gujju, Yusuke Miyao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19851">Tracking World States with Language Models: State-Based Evaluation Using Chess</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) exhibit emergent capabilities in structured domains, suggesting they may implicitly internalize high-fidelity representations of world models. While probing techniques have shown promising signs of this in scientific and game-based settings, they rely on model-specific internal activations, which limit interpretability and generalizability. In this work, we propose a model-agnostic, state-based evaluation framework using chess as a benchmark to assess whether LLMs preserve the semantics of structured environments. Our method analyzes the downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states. This approach offers a more meaningful evaluation than conventional string-based metrics by aligning more closely with the strategic and rule-governed nature of chess. Experimental results demonstrate that our metrics capture deficiencies in state-tracking, highlighting limitations of LLMs in maintaining coherent internal models over long sequences. Our framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access, and generalizes to a wide class of symbolic environments.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2508.06571.pdf' target='_blank'>https://arxiv.org/pdf/2508.06571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06571">IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language-Action (VLA) models have demonstrated potential in autonomous driving. However, two critical challenges hinder their development: (1) Existing VLA architectures are typically based on imitation learning in open-loop setup which tends to capture the recorded behaviors in the dataset, leading to suboptimal and constrained performance, (2) Close-loop training relies heavily on high-fidelity sensor simulation, where domain gaps and computational inefficiencies pose significant barriers. In this paper, we introduce IRL-VLA, a novel close-loop Reinforcement Learning via \textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model with a self-built VLA approach. Our framework proceeds in a three-stage paradigm: In the first stage, we propose a VLA architecture and pretrain the VLA policy via imitation learning. In the second stage, we construct a lightweight reward world model via inverse reinforcement learning to enable efficient close-loop reward computation. To further enhance planning performance, finally, we design specialized reward world model guidence reinforcement learning via PPO(Proximal Policy Optimization) to effectively balance the safety incidents, comfortable driving, and traffic efficiency. Our approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that our framework will accelerate VLA research in close-loop autonomous driving.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2507.19468.pdf' target='_blank'>https://arxiv.org/pdf/2507.19468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, Piotr Bojanowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19468">Back to the Features: DINO as a Foundation for Video World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2506.03709.pdf' target='_blank'>https://arxiv.org/pdf/2506.03709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03709">AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2505.14948.pdf' target='_blank'>https://arxiv.org/pdf/2505.14948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tang, Kevin Ellis, Suhas Lohit, Michael J. Jones, Moitreya Chatterjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14948">Programmatic Video Prediction Using Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2503.01584.pdf' target='_blank'>https://arxiv.org/pdf/2503.01584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cansu Sancaktar, Christian Gumbsch, Andrii Zadaianchuk, Pavel Kolev, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01584">SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration is a cornerstone of reinforcement learning (RL). Intrinsic motivation attempts to decouple exploration from external, task-based rewards. However, established approaches to intrinsic motivation that follow general principles such as information gain, often only uncover low-level interactions. In contrast, children's play suggests that they engage in meaningful high-level behavior by imitating or interacting with their caregivers. Recent work has focused on using foundation models to inject these semantic biases into exploration. However, these methods often rely on unrealistic assumptions, such as language-embedded environments or access to high-level actions. We propose SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL agents with an intrinsic motivation for semantically meaningful behavior. SENSEI distills a reward signal of interestingness from Vision Language Model (VLM) annotations, enabling an agent to predict these rewards through a world model. Using model-based RL, SENSEI trains an exploration policy that jointly maximizes semantic rewards and uncertainty. We show that in both robotic and video game-like simulations SENSEI discovers a variety of meaningful behaviors from image observations and low-level actions. SENSEI provides a general tool for learning from foundation model feedback, a crucial research direction, as VLMs become more powerful.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2502.09297.pdf' target='_blank'>https://arxiv.org/pdf/2502.09297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Zhang, Guanyu Chen, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09297">When Do Neural Networks Learn World Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2502.09297.pdf' target='_blank'>https://arxiv.org/pdf/2502.09297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Zhang, Guanyu Chen, Feng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09297">When Do Neural Networks Learn World Models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2501.08617.pdf' target='_blank'>https://arxiv.org/pdf/2501.08617.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime FernÃ¡ndez Fisac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08617">RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning generative AI, we present empirical evidence that it can also cause severe, systematic misalignment. We hypothesize that this stems from evaluator feedback depending on downstream outcome predictions (foresight) that can be influenced by the AI's output, inducing Goodhart's law dynamics. We present a theoretical analysis showing that conditioning evaluator feedback on downstream observations (hindsight) inhibits this effect by decoupling the alignment signal from potentially compromised predictions--crucially, the result holds even if the observed outcomes are sampled from the AI's own world model. Building on this insight, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which presents plausible simulated outcomes to evaluators before eliciting feedback. We validate RLHS across three consultancy settings--marketplace interactions, restaurant recommendations, and online course advising--using both online (PPO) and offline (DPO) fine-tuning methods, and show that it substantially improves alignment over RLHF in experiments and human evaluations. We perform post-hoc benchmark evaluations on TruthfulQA, HaluEval, and TrustLLM, finding that even after single-task fine-tuning, RLHF misalignment persists, whereas RLHS consistently outperforms baselines and demonstrates robust alignment generalization. The project webpage and code are available at https://rl-hindsight.github.io.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2412.03758.pdf' target='_blank'>https://arxiv.org/pdf/2412.03758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Ming, Jingwei Wu, Zhewei Huang, Zhuoxuan Ju, Jianming HU, Lihui Peng, Shuchang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03758">ARCON: Advancing Auto-Regressive Continuation for Driving Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in auto-regressive large language models (LLMs) have led to their application in video generation. This paper explores the use of Large Vision Models (LVMs) for video continuation, a task essential for building world models and predicting future frames. We introduce ARCON, a scheme that alternates between generating semantic and RGB tokens, allowing the LVM to explicitly learn high-level structural video information. We find high consistency in the RGB images and semantic maps generated without special design. Moreover, we employ an optical flow-based texture stitching method to enhance visual quality. Experiments in autonomous driving scenarios show that our model can consistently generate long videos.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2411.05619.pdf' target='_blank'>https://arxiv.org/pdf/2411.05619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilong Zhang, Ruifeng Chen, Junyin Ye, Yihao Sun, Pengyuan Wang, Jingcheng Pang, Kaiyuan Li, Tianshuo Liu, Haoxin Lin, Yang Yu, Zhi-Hua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05619">WHALE: Towards Generalizable and Scalable World Models for Embodied Decision-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models play a crucial role in decision-making within embodied environments, enabling cost-free explorations that would otherwise be expensive in the real world. To facilitate effective decision-making, world models must be equipped with strong generalizability to support faithful imagination in out-of-distribution (OOD) regions and provide reliable uncertainty estimation to assess the credibility of the simulated experiences, both of which present significant challenges for prior scalable approaches. This paper introduces WHALE, a framework for learning generalizable world models, consisting of two key techniques: behavior-conditioning and retracing-rollout. Behavior-conditioning addresses the policy distribution shift, one of the primary sources of the world model generalization error, while retracing-rollout enables efficient uncertainty estimation without the necessity of model ensembles. These techniques are universal and can be combined with any neural network architecture for world model learning. Incorporating these two techniques, we present Whale-ST, a scalable spatial-temporal transformer-based world model with enhanced generalizability. We demonstrate the superiority of Whale-ST in simulation tasks by evaluating both value estimation accuracy and video generation fidelity. Additionally, we examine the effectiveness of our uncertainty estimation technique, which enhances model-based policy optimization in fully offline scenarios. Furthermore, we propose Whale-X, a 414M parameter world model trained on 970K trajectories from Open X-Embodiment datasets. We show that Whale-X exhibits promising scalability and strong generalizability in real-world manipulation scenarios using minimal demonstrations.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2408.01268.pdf' target='_blank'>https://arxiv.org/pdf/2408.01268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Kaufmann, Kostas Lakis, Johannes Lengler, Raghu Raman Ravi, Ulysse Schaller, Konstantin Sturm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01268">Rumour Spreading Depends on the Latent Geometry and Degree Distribution in Social Network Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study push-pull rumour spreading in ultra-small-world models for social networks where the degrees follow a power-law distribution. In a non-geometric setting, Fountoulakis, Panagiotou and Sauerwald have shown that rumours always spread ultra-fast (SODA 2012), i.e. in doubly logarithmic time. On the other hand, Janssen and Mehrabian have found that rumours spread slowly (polynomial time) in a spatial preferential attachment model (SIDMA 2017). We study the question systematically for the model of Geometric Inhomogeneous Random Graphs (GIRGs). Our results are two-fold: first, with Euclidean geometry slow, fast (polylogarithmic) and ultra-fast rumour spreading may occur, depending on the exponent of the power law and the strength of the geometry in the networks, and we fully characterise the phase boundaries in between. The regimes do not coincide with the graph distance regimes, i.e., polylogarithmic or even polynomial rumour spreading may occur even if graph distances are doubly logarithmic. We expect these results to hold with little effort for related models, e.g. Scale-Free Percolation. Second, we show that rumour spreading is always (at least) fast in a non-metric geometry. The considered non-metric geometry allows to model social connections where resemblance of vertices in a single attribute, such as familial kinship, already strongly indicates the presence of an edge. Euclidean geometry fails to capture such ties.
  For some regimes in the Euclidean setting, the efficient pathways for spreading rumours differ from previously identified paths. For example, a vertex of degree $d$ can transmit the rumour to a vertex of larger degree by a chain of length $3$, where one of the two intermediaries has constant degree, and the other has degree $d^{c}$ for some constant $c<1$. Similar but longer chains of vertices, all having non-constant degree, turn out to be useful as well.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2407.10264.pdf' target='_blank'>https://arxiv.org/pdf/2407.10264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip H. S. Torr, Amartya Sanyal, Puneet K. Dokania
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10264">What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., "design") versus the specific concepts the task is asked to be performed upon (e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-known safety fine-tuning methods -- supervised safety fine-tuning, direct preference optimization, and unlearning -- and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. We validate our findings, wherever possible, on real-world models -- specifically, Llama-2 7B and Llama-3 8B.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2406.08862.pdf' target='_blank'>https://arxiv.org/pdf/2406.08862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Aman Chadha, Jundong Li, Tariq Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08862">Cognitively Inspired Energy-Based World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the predominant methods for training world models is autoregressive prediction in the output space of the next element of a sequence. In Natural Language Processing (NLP), this takes the form of Large Language Models (LLMs) predicting the next token; in Computer Vision (CV), this takes the form of autoregressive models predicting the next frame/token/pixel. However, this approach differs from human cognition in several respects. First, human predictions about the future actively influence internal cognitive processes. Second, humans naturally evaluate the plausibility of predictions regarding future states. Based on this capability, and third, by assessing when predictions are sufficient, humans allocate a dynamic amount of time to make a prediction. This adaptive process is analogous to System 2 thinking in psychology. All these capabilities are fundamental to the success of humans at high-level reasoning and planning. Therefore, to address the limitations of traditional autoregressive models lacking these human-like capabilities, we introduce Energy-Based World Models (EBWM). EBWM involves training an Energy-Based Model (EBM) to predict the compatibility of a given context and a predicted future state. In doing so, EBWM enables models to achieve all three facets of human cognition described. Moreover, we developed a variant of the traditional autoregressive transformer tailored for Energy-Based models, termed the Energy-Based Transformer (EBT). Our results demonstrate that EBWM scales better with data and GPU Hours than traditional autoregressive transformers in CV, and that EBWM offers promising early scaling in NLP. Consequently, this approach offers an exciting path toward training future models capable of System 2 thinking and intelligently searching across state spaces.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2402.12275.pdf' target='_blank'>https://arxiv.org/pdf/2402.12275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tang, Darren Key, Kevin Ellis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12275">WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We define this optimism as a logical constraint between a program and a planner. We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2401.14718.pdf' target='_blank'>https://arxiv.org/pdf/2401.14718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Ming, Zhewei Huang, Jingwei Wu, Zhuoxuan Ju, Daxin Jiang, Jianming Hu, Lihui Peng, Shuchang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.14718">A Survey on Future Frame Synthesis: Bridging Deterministic and Generative Approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Future Frame Synthesis (FFS), the task of generating subsequent video frames from context, represents a core challenge in machine intelligence and a cornerstone for developing predictive world models. This survey provides a comprehensive analysis of the FFS landscape, charting its critical evolution from deterministic algorithms focused on pixel-level accuracy to modern generative paradigms that prioritize semantic coherence and dynamic plausibility. We introduce a novel taxonomy organized by algorithmic stochasticity, which not only categorizes existing methods but also reveals the fundamental drivers--advances in architectures, datasets, and computational scale--behind this paradigm shift. Critically, our analysis identifies a bifurcation in the field's trajectory: one path toward efficient, real-time prediction, and another toward large-scale, generative world simulation. By pinpointing key challenges and proposing concrete research questions for both frontiers, this survey serves as an essential guide for researchers aiming to advance the frontiers of visual dynamic modeling.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2401.13795.pdf' target='_blank'>https://arxiv.org/pdf/2401.13795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Saygin Seyfioglu, Karim Bouyarmane, Suren Kumar, Amir Tavanaei, Ismail B. Tutar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13795">Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as "Virtual Try-All"-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present "Diffuse to Choose," a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2310.16029.pdf' target='_blank'>https://arxiv.org/pdf/2310.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16029">Finetuning Offline World Models in the Real World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) is notoriously data-inefficient, which makes training on a real robot difficult. While model-based RL algorithms (world models) improve data-efficiency to some extent, they still require hours or days of interaction to learn skills. Recently, offline RL has been proposed as a framework for training RL policies on pre-existing datasets without any online interaction. However, constraining an algorithm to a fixed dataset induces a state-action distribution shift between training and inference, and limits its applicability to new tasks. In this work, we seek to get the best of both worlds: we consider the problem of pretraining a world model with offline data collected on a real robot, and then finetuning the model on online data collected by planning with the learned model. To mitigate extrapolation errors during online interaction, we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty. We evaluate our method on a variety of visuo-motor control tasks in simulation and on a real robot, and find that our method enables few-shot finetuning to seen and unseen tasks even when offline data is limited. Videos, code, and data are available at https://yunhaifeng.com/FOWM .
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2310.05921.pdf' target='_blank'>https://arxiv.org/pdf/2310.05921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05921">Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2309.00941.pdf' target='_blank'>https://arxiv.org/pdf/2309.00941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neel Nanda, Andrew Lee, Martin Wattenberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2306.11335.pdf' target='_blank'>https://arxiv.org/pdf/2306.11335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11335">Surfer: Progressive Reasoning with World Models for Robotic Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Considering how to make the model accurately understand and follow natural language instructions and perform actions consistent with world knowledge is a key challenge in robot manipulation. This mainly includes human fuzzy instruction reasoning and the following of physical knowledge. Therefore, the embodied intelligence agent must have the ability to model world knowledge from training data. However, most existing vision and language robot manipulation methods mainly operate in less realistic simulator and language settings and lack explicit modeling of world knowledge. To bridge this gap, we introduce a novel and simple robot manipulation framework, called Surfer. It is based on the world model, treats robot manipulation as a state transfer of the visual scene, and decouples it into two parts: action and scene. Then, the generalization ability of the model on new instructions and new scenes is enhanced by explicit modeling of the action and scene prediction in multi-modal information. In addition to the framework, we also built a robot manipulation simulator that supports full physics execution based on the MuJoCo physics engine. It can automatically generate demonstration training data and test data, effectively reducing labor costs. To conduct a comprehensive and systematic evaluation of the robot manipulation model in terms of language understanding and physical execution, we also created a robotic manipulation benchmark with progressive reasoning tasks, called SeaWave. It contains 4 levels of progressive reasoning tasks and can provide a standardized testing platform for embedded AI agents in multi-modal environments. On average, Surfer achieved a success rate of 54.74% on the defined four levels of manipulation tasks, exceeding the best baseline performance of 47.64%.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2305.14984.pdf' target='_blank'>https://arxiv.org/pdf/2305.14984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel GlÃ¶ckler, Michael Deistler, Jakob H. Macke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14984">Adversarial robustness of amortized Bayesian inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2303.10144.pdf' target='_blank'>https://arxiv.org/pdf/2303.10144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolai Dorka, Tim Welschehold, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10144">Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2302.06372.pdf' target='_blank'>https://arxiv.org/pdf/2302.06372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wanyue Xu, Zhongzhi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06372">Optimal Scale-Free Small-World Graphs with Minimum Scaling of Cover Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The cover time of random walks on a graph has found wide practical applications in different fields of computer science, such as crawling and searching on the World Wide Web and query processing in sensor networks, with the application effects dependent on the behavior of cover time: the smaller the cover time, the better the application performance. It was proved that over all graphs with $N$ nodes, complete graphs have the minimum cover time $N\log N$. However, complete graphs cannot mimic real-world networks with small average degree and scale-free small-world properties, for which the cover time has not been examined carefully, and its behavior is still not well understood. In this paper, we first experimentally evaluate the cover time for various real-world networks with scale-free small-world properties, which scales as $N\log N$. To better understand the behavior of the cover time for real-world networks, we then study the cover time of three scale-free small-world model networks by using the connection between cover time and resistance diameter. For all the three networks, their cover time also behaves as $N\log N$. This work indicates that sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of cover time. Our results deepen understanding the behavior of cover time in real-world networks with scale-free small-world structure, and have potential implications in the design of efficient algorithms related to cover time.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2208.06448.pdf' target='_blank'>https://arxiv.org/pdf/2208.06448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Rodriguez-Sanchez, Benjamin A. Spiegel, Jennifer Wang, Roma Patel, Stefanie Tellex, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.06448">RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2010.08869.pdf' target='_blank'>https://arxiv.org/pdf/2010.08869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Fishman, Nishanth Kumar, Cameron Allen, Natasha Danas, Michael Littman, Stefanie Tellex, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2010.08869">Task Scoping: Generating Task-Specific Abstractions for Planning in Open-Scope Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A general-purpose planning agent requires an open-scope world model: one rich enough to tackle any of the wide range of tasks it may be asked to solve over its operational lifetime. This stands in contrast with typical planning approaches, where the scope of a model is limited to a specific family of tasks that share significant structure. Unfortunately, planning to solve any specific task using an open-scope model is computationally intractable - even for state-of-the-art methods - due to the many states and actions that are necessarily present in the model but irrelevant to that problem. We propose task scoping: a method that exploits knowledge of the initial state, goal conditions, and transition system to automatically and efficiently remove provably irrelevant variables and actions from a planning problem. Our approach leverages causal link analysis and backwards reachability over state variables (rather than states) along with operator merging (when effects on relevant variables are identical). Using task scoping as a pre-planning step can shrink the search space by orders of magnitude and dramatically decrease planning time. We empirically demonstrate that these improvements occur across a variety of open-scope domains, including Minecraft, where our approach leads to a 75x reduction in search time with a state-of-the-art numeric planner, even after including the time required for task scoping itself.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2510.08398.pdf' target='_blank'>https://arxiv.org/pdf/2510.08398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08398">VideoVerse: How Far is Your T2V Generator from a World Model?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2510.08398.pdf' target='_blank'>https://arxiv.org/pdf/2510.08398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08398">VideoVerse: How Far is Your T2V Generator from a World Model?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2509.23979.pdf' target='_blank'>https://arxiv.org/pdf/2509.23979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Wang, Junfeng Sun, Xingdi Yuan, Ruoyao Wang, Ziang Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23979">ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating interactive world models remains a core challenge in Large Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a refactored, modular, and extensible implementation of the original ByteSized32 corpus to explore the task of text game generation. We further optimize the code structure of each text game and create the GameBasic.py foundation library, which centralizes common logic across all 32 games by abstracting 7 base classes (GameObject, etc.) into reusable modules, thereby reducing from 20k to 10k total lines of Python code compared to the original Bytesized32. Our refactored implementation enables extendability - with our centralized design, ByteSized32Refactored can be more efficiently extended to include text games of new scenarios and specifications by reusing the shared logic and functionalities. Extensive experiments with GPT-4o demonstrate a mix of performance - with Bytesized32Refactored, the generated text games for unseen scenarios showcase quality improvements on two of the four evaluation dimensions while decreases on the other two, indicating that the hierarchical structure of the refactored code presents new challenges for LLMs. Overall, we highlight that our extensible code structure, centered on the foundation library and the modular optimization, not only facilitates LLM adaptation to environment specifications but also establishes a scalable environment that supports future extensions.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2509.23979.pdf' target='_blank'>https://arxiv.org/pdf/2509.23979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Wang, Junfeng Sun, Xingdi Yuan, Ruoyao Wang, Ziang Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23979">ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulating interactive world models remains a core challenge in Large Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a refactored, modular, and extensible implementation of the original ByteSized32 corpus to explore the task of text game generation. We further optimize the code structure of each text game and create the GameBasic.py foundation library, which centralizes common logic across all 32 games by abstracting 7 base classes (GameObject, etc.) into reusable modules, thereby reducing from 20k to 10k total lines of Python code compared to the original Bytesized32. Our refactored implementation enables extendability - with our centralized design, ByteSized32Refactored can be more efficiently extended to include text games of new scenarios and specifications by reusing the shared logic and functionalities. Extensive experiments with GPT-4o demonstrate a mix of performance - with Bytesized32Refactored, the generated text games for unseen scenarios showcase quality improvements on two of the four evaluation dimensions while decreases on the other two, indicating that the hierarchical structure of the refactored code presents new challenges for LLMs. Overall, we highlight that our extensible code structure, centered on the foundation library and the modular optimization, not only facilitates LLM adaptation to environment specifications but also establishes a scalable environment that supports future extensions.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2509.09737.pdf' target='_blank'>https://arxiv.org/pdf/2509.09737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klemen Kotar, Wanhee Lee, Rahul Venkatesh, Honglin Chen, Daniel Bear, Jared Watrous, Simon Kim, Khai Loong Aw, Lilian Naing Chen, Stefan Stojanov, Kevin Feigelis, Imran Thobani, Alex Durango, Khaled Jedoui, Atlas Kazemian, Dan Yamins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09737">World Modeling with Probabilistic Structure Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful "intermediate structures", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2509.09737.pdf' target='_blank'>https://arxiv.org/pdf/2509.09737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Klemen Kotar, Wanhee Lee, Rahul Venkatesh, Honglin Chen, Daniel Bear, Jared Watrous, Simon Kim, Khai Loong Aw, Lilian Naing Chen, Stefan Stojanov, Kevin Feigelis, Imran Thobani, Alex Durango, Khaled Jedoui, Atlas Kazemian, Dan Yamins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09737">World Modeling with Probabilistic Structure Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful "intermediate structures", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2509.03479.pdf' target='_blank'>https://arxiv.org/pdf/2509.03479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Wang, Mingjia Zhao, Junfeng Sun, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03479">Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI technology advances, research in playing text-based games with agents has becomeprogressively popular. In this paper, a novel approach to agent design and agent learning ispresented with the context of reinforcement learning. A model of deep learning is first applied toprocess game text and build a world model. Next, the agent is learned through a policy gradient-based deep reinforcement learning method to facilitate conversion from state value to optimal policy.The enhanced agent works better in several text-based game experiments and significantlysurpasses previous agents on game completion ratio and win rate. Our study introduces novelunderstanding and empirical ground for using reinforcement learning for text games and sets thestage for developing and optimizing reinforcement learning agents for more general domains andproblems.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2509.03479.pdf' target='_blank'>https://arxiv.org/pdf/2509.03479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Wang, Mingjia Zhao, Junfeng Sun, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03479">Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI technology advances, research in playing text-based games with agents has becomeprogressively popular. In this paper, a novel approach to agent design and agent learning ispresented with the context of reinforcement learning. A model of deep learning is first applied toprocess game text and build a world model. Next, the agent is learned through a policy gradient-based deep reinforcement learning method to facilitate conversion from state value to optimal policy.The enhanced agent works better in several text-based game experiments and significantlysurpasses previous agents on game completion ratio and win rate. Our study introduces novelunderstanding and empirical ground for using reinforcement learning for text games and sets thestage for developing and optimizing reinforcement learning agents for more general domains andproblems.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2509.00074.pdf' target='_blank'>https://arxiv.org/pdf/2509.00074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>CÃ©dric Colas, Tracey Mills, Ben Prystawski, Michael Henry Tessler, Noah Goodman, Jacob Andreas, Joshua Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00074">Language and Experience: A Computational Model of Social Learning in Complex Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2509.00074.pdf' target='_blank'>https://arxiv.org/pdf/2509.00074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>CÃ©dric Colas, Tracey Mills, Ben Prystawski, Michael Henry Tessler, Noah Goodman, Jacob Andreas, Joshua Tenenbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00074">Language and Experience: A Computational Model of Social Learning in Complex Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to combine linguistic guidance from others with direct experience is central to human development, enabling safe and rapid learning in new environments. How do people integrate these two sources of knowledge, and how might AI systems? We present a computational framework that models social learning as joint probabilistic inference over structured, executable world models given sensorimotor and linguistic data. We make this possible by turning a pretrained language model into a probabilistic model of how humans share advice conditioned on their beliefs, allowing our agents both to generate advice for others and to interpret linguistic input as evidence during Bayesian inference. Using behavioral experiments and simulations across 10 video games, we show how linguistic guidance can shape exploration and accelerate learning by reducing risky interactions and speeding up key discoveries in both humans and models. We further explore how knowledge can accumulate across generations through iterated learning experiments and demonstrate successful knowledge transfer between humans and models -- revealing how structured, language-compatible representations might enable human-machine collaborative learning.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2508.01922.pdf' target='_blank'>https://arxiv.org/pdf/2508.01922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hunter Schofield, Mohammed Elmahgiubi, Kasra Rezaee, Jinjun Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01922">Beyond Simulation: Benchmarking World Models for Planning and Causality in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have become increasingly popular in acting as learned traffic simulators. Recent work has explored replacing traditional traffic simulators with world models for policy training. In this work, we explore the robustness of existing metrics to evaluate world models as traffic simulators to see if the same metrics are suitable for evaluating a world model as a pseudo-environment for policy training. Specifically, we analyze the metametric employed by the Waymo Open Sim-Agents Challenge (WOSAC) and compare world model predictions on standard scenarios where the agents are fully or partially controlled by the world model (partial replay). Furthermore, since we are interested in evaluating the ego action-conditioned world model, we extend the standard WOSAC evaluation domain to include agents that are causal to the ego vehicle. Our evaluations reveal a significant number of scenarios where top-ranking models perform well under no perturbation but fail when the ego agent is forced to replay the original trajectory. To address these cases, we propose new metrics to highlight the sensitivity of world models to uncontrollable objects and evaluate the performance of world models as pseudo-environments for policy training and analyze some state-of-the-art world models under these new metrics.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2507.09797.pdf' target='_blank'>https://arxiv.org/pdf/2507.09797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ping Liu, Rajat Arora, Xiao Shi, Benjamin Le, Qianqi Shen, Jianqiang Shen, Chengming Jiang, Nikita Zhiltsov, Priya Bannur, Yidan Zhu, Liming Dong, Haichao Wei, Qi Guo, Luke Simon, Liangjie Hong, Wenjing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09797">A Scalable and Efficient Signal Integration System for Job Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LinkedIn, one of the world's largest platforms for professional networking and job seeking, encounters various modeling challenges in building recommendation systems for its job matching product, including cold-start, filter bubbles, and biases affecting candidate-job matching. To address these, we developed the STAR (Signal Integration for Talent And Recruiters) system, leveraging the combined strengths of Large Language Models (LLMs) and Graph Neural Networks (GNNs). LLMs excel at understanding textual data, such as member profiles and job postings, while GNNs capture intricate relationships and mitigate cold-start issues through network effects. STAR integrates diverse signals by uniting LLM and GNN capabilities with industrial-scale paradigms including adaptive sampling and version management. It provides an end-to-end solution for developing and deploying embeddings in large-scale recommender systems. Our key contributions include a robust methodology for building embeddings in industrial applications, a scalable GNN-LLM integration for high-performing recommendations, and practical insights for real-world model deployment.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2506.17967.pdf' target='_blank'>https://arxiv.org/pdf/2506.17967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariya Hendriksen, Tabish Rashid, David Bignell, Raluca Georgescu, Abdelhak Lemkhenter, Katja Hofmann, Sam Devlin, Sarah Parisot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17967">Adapting Vision-Language Models for Evaluating World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models -- generative models that simulate environment dynamics conditioned on past observations and actions -- are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency -- capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce a evaluation protocol targeting two recognition tasks -- action recognition and character recognition -- each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a method for adapting VLMs to rollout evaluation under data and compute constraints. We conduct a large-scale study comparing full, partial, and parameter-efficient finetuning across task formats, context lengths, sampling strategies, and data compositions. The resulting unified evaluator matches the performance of task-specific baselines using a single checkpoint. Human studies confirm strong alignment with human judgments, establishing UNIVERSE as a scalable, semantics-aware evaluator for world models.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2506.06725.pdf' target='_blank'>https://arxiv.org/pdf/2506.06725.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillaume Levy, Cedric Colas, Pierre-Yves Oudeyer, Thomas Carta, Clement Romac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06725">WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2505.19785.pdf' target='_blank'>https://arxiv.org/pdf/2505.19785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyi Xu, Gousia Habib, Dilruk Perera, Mengling Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19785">medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses can vary significantly and evolve over time. Clinical data used to support these treatment decisions are often irregularly sampled, where missing data frequencies may implicitly convey information about the patient's condition. Existing Reinforcement Learning (RL) based clinical decision support systems often ignore the missing patterns and distort them with coarse discretization and simple imputation. They are also predominantly model-free and largely depend on retrospective data, which could lead to insufficient exploration and bias by historical behaviors. To address these limitations, we propose medDreamer, a novel model-based reinforcement learning framework for personalized treatment recommendation. medDreamer contains a world model with an Adaptive Feature Integration module that simulates latent patient states from irregular data and a two-phase policy trained on a hybrid of real and imagined trajectories. This enables learning optimal policies that go beyond the sub-optimality of historical clinical decisions, while remaining close to real clinical data. We evaluate medDreamer on both sepsis and mechanical ventilation treatment tasks using two large-scale Electronic Health Records (EHRs) datasets. Comprehensive evaluations show that medDreamer significantly outperforms model-free and model-based baselines in both clinical outcomes and off-policy metrics.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2505.19095.pdf' target='_blank'>https://arxiv.org/pdf/2505.19095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runliang Niu, Jinglong Ji, Yi Chang, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19095">ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of large language models (LLMs) has sparked growing interest in building Artificial General Intelligence (AGI) within Graphical User Interface (GUI) environments. However, existing GUI agents based on LLMs or vision-language models (VLMs) often fail to generalize to novel environments and rely heavily on manually curated, diverse datasets. To overcome these limitations, we introduce ScreenExplorer, a VLM trained via Group Relative Policy Optimization(GRPO) in real, dynamic, and open-ended GUI environments. Innovatively, we introduced a world-model-based curiosity reward function to help the agent overcome the cold-start phase of exploration. Additionally, distilling experience streams further enhances the model's exploration capabilities. Our training framework enhances model exploration in open GUI environments, with trained models showing better environmental adaptation and sustained exploration compared to static deployment models. Our findings offer a scalable pathway toward AGI systems with self-improving capabilities in complex interactive settings.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2504.02252.pdf' target='_blank'>https://arxiv.org/pdf/2504.02252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JB Lanier, Kyungmin Kim, Armin Karamzade, Yifei Liu, Ankita Sinha, Kat He, Davide Corsi, Roy Fox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02252">Adapting World Models with Latent-State Dynamics Residuals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation-to-reality reinforcement learning (RL) faces the critical challenge of reconciling discrepancies between simulated and real-world dynamics, which can severely degrade agent performance. A promising approach involves learning corrections to simulator forward dynamics represented as a residual error function, however this operation is impractical with high-dimensional states such as images. To overcome this, we propose ReDRAW, a latent-state autoregressive world model pretrained in simulation and calibrated to target environments through residual corrections of latent-state dynamics rather than of explicit observed states. Using this adapted world model, ReDRAW enables RL agents to be optimized with imagined rollouts under corrected dynamics and then deployed in the real world. In multiple vision-based MuJoCo domains and a physical robot visual lane-following task, ReDRAW effectively models changes to dynamics and avoids overfitting in low data regimes where traditional transfer methods fail.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2503.15168.pdf' target='_blank'>https://arxiv.org/pdf/2503.15168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier Del Ser, Jesus L. Lobo, Heimo MÃ¼ller, Andreas Holzinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15168">World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World Models help Artificial Intelligence (AI) predict outcomes, reason about its environment, and guide decision-making. While widely used in reinforcement learning, they lack the structured, adaptive representations that even young children intuitively develop. Advancing beyond pattern recognition requires dynamic, interpretable frameworks inspired by Piaget's cognitive development theory. We highlight six key research areas -- physics-informed learning, neurosymbolic learning, continual learning, causal inference, human-in-the-loop AI, and responsible AI -- as essential for enabling true reasoning in AI. By integrating statistical learning with advances in these areas, AI can evolve from pattern recognition to genuine understanding, adaptation and reasoning capabilities.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2502.01591.pdf' target='_blank'>https://arxiv.org/pdf/2502.01591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01591">Improving Transformer World Models for Data-Efficient RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present three improvements to the standard model-based RL paradigm based on transformers: (a) "Dyna with warmup", which trains the policy on real and imaginary data, but only starts using imaginary data after the world model has been sufficiently trained; (b) "nearest neighbor tokenizer" for image patches, which improves upon previous tokenization schemes, which are needed when using a transformer world model (TWM), by ensuring the code words are static after creation, thus providing a constant target for TWM learning; and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep, instead of generating them sequentially. We then show that our method significantly improves upon prior methods in various environments. We mostly focus on the challenging Craftax-classic benchmark, where our method achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and exceeding human performance of 65.0% for the first time. We also show preliminary results on Craftax-full, MinAtar, and three different two-player games, to illustrate the generality of the approach.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2502.00708.pdf' target='_blank'>https://arxiv.org/pdf/2502.00708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qixuan Li, Chao Wang, Zongjin He, Yan Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00708">PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-3D asset generation has achieved significant optimization under the supervision of 2D diffusion priors. However, when dealing with compositional scenes, existing methods encounter several challenges: 1). failure to ensure that composite scene layouts comply with physical laws; 2). difficulty in accurately capturing the assets and relationships described in complex scene descriptions; 3). limited autonomous asset generation capabilities among layout approaches leveraging large language models (LLMs). To avoid these compromises, we propose a novel framework for compositional scene generation, PhiP-G, which seamlessly integrates generation techniques with layout guidance based on a world model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene description to generate a scene graph, and integrating a multimodal 2D generation agent and a 3D Gaussian generation method for targeted assets creation. For the stage of layout, PhiP-G employs a physical pool with adhesion capabilities and a visual supervision agent, forming a world model for layout prediction and planning. Extensive experiments demonstrate that PhiP-G significantly enhances the generation quality and physical rationality of the compositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA) performance in CLIP scores, achieves parity with the leading methods in generation quality as measured by the T$^3$Bench, and improves efficiency by 24x.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2411.08027.pdf' target='_blank'>https://arxiv.org/pdf/2411.08027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08027">LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact -- the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present LLMPhy, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, LLMPhy uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of LLMPhy, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2411.07690.pdf' target='_blank'>https://arxiv.org/pdf/2411.07690.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifan Zeng, Chongzhe Zhang, Feng Liu, Joseph Sifakis, Qunli Zhang, Shiming Liu, Peng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07690">World Models: The Safety Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents. It is arguably evolving into an essential foundation for building AI agent systems. A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely. The safety property of WM plays a key role in their effective use in critical applications. In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged. We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2411.04434.pdf' target='_blank'>https://arxiv.org/pdf/2411.04434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Pearce, Tabish Rashid, Dave Bignell, Raluca Georgescu, Sam Devlin, Katja Hofmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04434">Scaling Laws for Pre-training Agents and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of embodied agents has been shown to improve by increasing model parameters, dataset size, and compute. This has been demonstrated in domains from robotics to video games, when generative learning objectives on offline datasets (pre-training) are used to model an agent's behavior (imitation learning) or their environment (world modeling). This paper characterizes the role of scale in these tasks more precisely. Going beyond the simple intuition that `bigger is better', we show that the same types of power laws found in language modeling also arise in world modeling and imitation learning (e.g. between loss and optimal model size). However, the coefficients of these laws are heavily influenced by the tokenizer, task \& architecture -- this has important implications on the optimal sizing of models and data.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2410.21059.pdf' target='_blank'>https://arxiv.org/pdf/2410.21059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxu Feng, Takato Horii, Takayuki Nagai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21059">Predictive Reachability for Embodiment Selection in Mobile Manipulation Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile manipulators require coordinated control between navigation and manipulation to accomplish tasks. Typically, coordinated mobile manipulation behaviors have base navigation to approach the goal followed by arm manipulation to reach the desired pose. Selecting the embodiment between the base and arm can be determined based on reachability. Previous methods evaluate reachability by computing inverse kinematics and activate arm motions once solutions are identified. In this study, we introduce a new approach called predictive reachability that decides reachability based on predicted arm motions. Our model utilizes a hierarchical policy framework built upon a world model. The world model allows the prediction of future trajectories and the evaluation of reachability. The hierarchical policy selects the embodiment based on the predicted reachability and plans accordingly. Unlike methods that require prior knowledge about robots and environments for inverse kinematics, our method only relies on image-based observations. We evaluate our approach through basic reaching tasks across various environments. The results demonstrate that our method outperforms previous model-based approaches in both sample efficiency and performance, while enabling more reasonable embodiment selection based on predictive reachability.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2410.19379.pdf' target='_blank'>https://arxiv.org/pdf/2410.19379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah Mustafa, Ryo Hanai, Ixchel Ramirez, Floris Erich, Ryoichi Nakajo, Yukiyasu Domae, Tetsuya Ogata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19379">Visual Imitation Learning of Non-Prehensile Manipulation Tasks with Dynamics-Supervised Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unlike quasi-static robotic manipulation tasks like pick-and-place, dynamic tasks such as non-prehensile manipulation pose greater challenges, especially for vision-based control. Successful control requires the extraction of features relevant to the target task. In visual imitation learning settings, these features can be learnt by backpropagating the policy loss through the vision backbone. Yet, this approach tends to learn task-specific features with limited generalizability. Alternatively, learning world models can realize more generalizable vision backbones. Utilizing the learnt features, task-specific policies are subsequently trained. Commonly, these models are trained solely to predict the next RGB state from the current state and action taken. But only-RGB prediction might not fully-capture the task-relevant dynamics. In this work, we hypothesize that direct supervision of target dynamic states (Dynamics Mapping) can learn better dynamics-informed world models. Beside the next RGB reconstruction, the world model is also trained to directly predict position, velocity, and acceleration of environment rigid bodies. To verify our hypothesis, we designed a non-prehensile 2D environment tailored to two tasks: "Balance-Reaching" and "Bin-Dropping". When trained on the first task, dynamics mapping enhanced the task performance under different training configurations (Decoupled, Joint, End-to-End) and policy architectures (Feedforward, Recurrent). Notably, its most significant impact was for world model pretraining boosting the success rate from 21% to 85%. Although frozen dynamics-informed world models could generalize well to a task with in-domain dynamics, but poorly to a one with out-of-domain dynamics.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2406.14415.pdf' target='_blank'>https://arxiv.org/pdf/2406.14415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hunter Schofield, Hamidreza Mirkhani, Mohammed Elmahgiubi, Kasra Rezaee, Jinjun Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14415">Vectorized Representation Dreamer (VRD): Dreaming-Assisted Multi-Agent Motion-Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For an autonomous vehicle to plan a path in its environment, it must be able to accurately forecast the trajectory of all dynamic objects in its proximity. While many traditional methods encode observations in the scene to solve this problem, there are few approaches that consider the effect of the ego vehicle's behavior on the future state of the world. In this paper, we introduce VRD, a vectorized world model-inspired approach to the multi-agent motion forecasting problem. Our method combines a traditional open-loop training regime with a novel dreamed closed-loop training pipeline that leverages a kinematic reconstruction task to imagine the trajectory of all agents, conditioned on the action of the ego vehicle. Quantitative and qualitative experiments are conducted on the Argoverse 2 multi-world forecasting evaluation dataset and the intersection drone (inD) dataset to demonstrate the performance of our proposed model. Our model achieves state-of-the-art performance on the single prediction miss rate metric on the Argoverse 2 dataset and performs on par with the leading models for the single prediction displacement metrics.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2406.08756.pdf' target='_blank'>https://arxiv.org/pdf/2406.08756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ping Chen, Wenjie Zhang, Shuibing He, Weijian Chen, Siling Yang, Kexin Huang, Yanlong Yin, Xuan Zhan, Yingjie Gu, Zhuwei Peng, Yi Zheng, Zhefeng Wang, Gang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08756">Optimizing Large Model Training through Overlapped Activation Recomputation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large model training often uses recomputation to alleviate memory pressure and pipelines to exploit the parallelism of data, tensors, and devices. However, existing recomputation approaches may incur high overhead when training real-world models, as they are executed on demand in the critical training path. In this paper, we present Lynx, a new recomputation framework to reduce overhead by overlapping recomputation with communication in training pipelines. To reduce the large search space for recomputation strategies, we propose a heuristic-based recomputation scheduling algorithm, which is based on the observation that there are identical structures in large DNN models so that we can apply the same scheduling policy to all such structures. Additionally, we propose a recomputation-aware model partitioning method to balance each stage's execution time for improved training throughput. Our comprehensive evaluation using GPT models with 1.3B-23B parameters shows that Lynx outperforms existing recomputation approaches by up to 1.37x.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2406.03689.pdf' target='_blank'>https://arxiv.org/pdf/2406.03689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyon Vafa, Justin Y. Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03689">Evaluating the World Model Implicit in a Generative Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2405.03878.pdf' target='_blank'>https://arxiv.org/pdf/2405.03878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya A. Ramesh, Kenny Young, Louis Kirsch, JÃ¼rgen Schmidhuber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03878">Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal credit assignment in reinforcement learning is challenging due to delayed and stochastic outcomes. Monte Carlo targets can bridge long delays between action and consequence but lead to high-variance targets due to stochasticity. Temporal difference (TD) learning uses bootstrapping to overcome variance but introduces a bias that can only be corrected through many iterations. TD($Î»$) provides a mechanism to navigate this bias-variance tradeoff smoothly. Appropriately selecting $Î»$ can significantly improve performance. Here, we propose Chunked-TD, which uses predicted probabilities of transitions from a model for computing $Î»$-return targets. Unlike other model-based solutions to credit assignment, Chunked-TD is less vulnerable to model inaccuracies. Our approach is motivated by the principle of history compression and 'chunks' trajectories for conventional TD learning. Chunking with learned world models compresses near-deterministic regions of the environment-policy interaction to speed up credit assignment while still bootstrapping when necessary. We propose algorithms that can be implemented online and show that they solve some problems much faster than conventional TD($Î»$).
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2403.12309.pdf' target='_blank'>https://arxiv.org/pdf/2403.12309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Armin Karamzade, Kyungmin Kim, Montek Kalsi, Roy Fox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12309">Reinforcement Learning from Delayed Observations via World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In standard reinforcement learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of learning algorithms. In this paper, we address observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to 250%. Moreover, we evaluate our methods on visual delayed environments, for the first time showcasing delay-aware reinforcement learning continuous control with visual observations.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2402.06665.pdf' target='_blank'>https://arxiv.org/pdf/2402.06665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Marc Rigter, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard SchÃ¶lkopf, Cheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06665">The Essential Role of Causality in Foundation World Models for Embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2402.05290.pdf' target='_blank'>https://arxiv.org/pdf/2402.05290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michel Ma, Tianwei Ni, Clement Gehring, Pierluca D'Oro, Pierre-Luc Bacon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05290">Do Transformer World Models Give Better Policy Gradients?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients over long horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimization landscapes that are easier to navigate even when compared to those from the simulator itself. This property allows transformer AWMs to produce better policies than competitive baselines in realistic long-horizon tasks.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2401.05946.pdf' target='_blank'>https://arxiv.org/pdf/2401.05946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antoine Dedieu, Wolfgang Lehrach, Guangyao Zhou, Dileep George, Miguel LÃ¡zaro-Gredilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05946">Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2312.05044.pdf' target='_blank'>https://arxiv.org/pdf/2312.05044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc HÃ¶ftmann, Jan Robine, Stefan Harmeling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05044">Backward Learning for Goal-Conditioned Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Can we learn policies in reinforcement learning without rewards? Can we learn a policy just by trying to reach a goal state? We answer these questions positively by proposing a multi-step procedure that first learns a world model that goes backward in time, secondly generates goal-reaching backward trajectories, thirdly improves those sequences using shortest path finding algorithms, and finally trains a neural network policy by imitation learning. We evaluate our method on a deterministic maze environment where the observations are $64\times 64$ pixel bird's eye images and can show that it consistently reaches several goals.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2310.18847.pdf' target='_blank'>https://arxiv.org/pdf/2310.18847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiran Lekkala, Chen Liu, Laurent Itti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18847">Bird's Eye View Based Pretrained World model for Visual Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sim2Real transfer has gained popularity because it helps transfer from inexpensive simulators to real world. This paper presents a novel system that fuses components in a traditional World Model into a robust system, trained entirely within a simulator, that Zero-Shot transfers to the real world. To facilitate transfer, we use an intermediary representation that is based on \textit{Bird's Eye View (BEV)} images. Thus, our robot learns to navigate in a simulator by first learning to translate from complex \textit{First-Person View (FPV)} based RGB images to BEV representations, then learning to navigate using those representations. Later, when tested in the real world, the robot uses the perception model that translates FPV-based RGB images to embeddings that were learned by the FPV to BEV translator and that can be used by the downstream policy. The incorporation of state-checking modules using \textit{Anchor images} and Mixture Density LSTM not only interpolates uncertain and missing observations but also enhances the robustness of the model in the real-world. We trained the model using data from a Differential drive robot in the CARLA simulator. Our methodology's effectiveness is shown through the deployment of trained models onto a real-world Differential drive robot. Lastly we release a comprehensive codebase, dataset and models for training and deployment (\url{https://sites.google.com/view/value-explicit-pretraining}).
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2309.03886.pdf' target='_blank'>https://arxiv.org/pdf/2309.03886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03886">FIND: A Function Description Benchmark for Evaluating Interpretability Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions span textual and numeric domains, and involve a range of real-world complexities. We evaluate methods that use pretrained language models (LMs) to produce descriptions of function behavior in natural language and code. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built from an LM with black-box access to functions, can infer function structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, AIA descriptions tend to capture global function behavior and miss local details. These results suggest that FIND will be useful for evaluating more sophisticated interpretability methods before they are applied to real-world models.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2308.11093.pdf' target='_blank'>https://arxiv.org/pdf/2308.11093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario LuÄiÄ, Fisher Yu, Thomas Kipf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11093">Video OWL-ViT: Temporally-consistent open-world localization in video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pre-training, can be transferred successfully to open-world localization across diverse videos.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2308.01399.pdf' target='_blank'>https://arxiv.org/pdf/2308.01399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01399">Learning to Model the World with Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language -- language like "this button turns on the TV" or "I put the bowls away" -- that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2305.10626.pdf' target='_blank'>https://arxiv.org/pdf/2305.10626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10626">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2302.10320.pdf' target='_blank'>https://arxiv.org/pdf/2302.10320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suzan Ece Ada, Emre Ugur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10320">Meta-World Conditional Neural Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Meta-World Conditional Neural Processes (MW-CNP), a conditional world model generator that leverages sample efficiency and scalability of Conditional Neural Processes to enable an agent to sample from its own "hallucination". We intend to reduce the agent's interaction with the target environment at test time as much as possible. To reduce the number of samples required at test time, we first obtain a latent representation of the transition dynamics from a single rollout from the test environment with hidden parameters. Then, we obtain rollouts for few-shot learning by interacting with the "hallucination" generated by the meta-world model. Using the world model representation from MW-CNP, the meta-RL agent can adapt to an unseen target environment with significantly fewer samples collected from the target environment compared to the baselines. We emphasize that the agent does not have access to the task parameters throughout training and testing, and MW-CNP is trained on offline interaction data logged during meta-training.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2212.01354.pdf' target='_blank'>https://arxiv.org/pdf/2212.01354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karl J Friston, Maxwell J D Ramstead, Alex B Kiefer, Alexander Tschantz, Christopher L Buckley, Mahault Albarracin, Riddhi J Pitliya, Conor Heins, Brennan Klein, Beren Millidge, Dalton A R Sakthivadivel, Toby St Clere Smithe, Magnus Koudahl, Safae Essafi Tremblay, Capm Petersen, Kaiser Fung, Jason G Fox, Steven Swanson, Dan Mapes, Gabriel RenÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01354">Designing Ecosystems of Intelligence from First Principles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants -- what we call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. In this context, we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world -- also known as self-evidencing. Formally, this corresponds to maximizing (Bayesian) model evidence, via belief updating over several scales: i.e., inference, learning, and model selection. Operationally, this self-evidencing can be realized via (variational) message passing or belief propagation on a factor graph. Crucially, active inference foregrounds an existential imperative of intelligent systems; namely, curiosity or the resolution of uncertainty. This same imperative underwrites belief sharing in ensembles of agents, in which certain aspects (i.e., factors) of each agent's generative world model provide a common ground or frame of reference. Active inference plays a foundational role in this ecology of belief sharing -- leading to a formal account of collective intelligence that rests on shared narratives and goals. We also consider the kinds of communication protocols that must be developed to enable such an ecosystem of intelligences and motivate the development of a shared hyper-spatial modeling language and transaction protocol, as a first -- and key -- step towards such an ecology.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2210.05861.pdf' target='_blank'>https://arxiv.org/pdf/2210.05861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05861">SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer's dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2202.09481.pdf' target='_blank'>https://arxiv.org/pdf/2202.09481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.09481">TransDreamer: Reinforcement Learning with Transformer World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2510.07417.pdf' target='_blank'>https://arxiv.org/pdf/2510.07417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Corban Rivera, Grayson Byrd, Meghan Booker, Bethany Kemp, Allison Gaines, Emma Holmes, James Uplinger, Celso M de Melo, David Handelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07417">FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating heterogeneous robot teams from free-form natural-language instructions is hard. Language-only planners struggle with long-horizon coordination and hallucination, while purely formal methods require closed-world models. We present FLEET, a hybrid decentralized framework that turns language into optimized multi-robot schedules. An LLM front-end produces (i) a task graph with durations and precedence and (ii) a capability-aware robot--task fitness matrix; a formal back-end solves a makespan-minimization problem while the underlying robots execute their free-form subtasks with agentic closed-loop control. Across multiple free-form language-guided autonomy coordination benchmarks, FLEET improves success over state of the art generative planners on two-agent teams across heterogeneous tasks. Ablations show that mixed integer linear programming (MILP) primarily improves temporal structure, while LLM-derived fitness is decisive for capability-coupled tasks; together they deliver the highest overall performance. We demonstrate the translation to real world challenges with hardware trials using a pair of quadruped robots with disjoint capabilities.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2510.07417.pdf' target='_blank'>https://arxiv.org/pdf/2510.07417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Corban Rivera, Grayson Byrd, Meghan Booker, Bethany Kemp, Allison Gaines, Emma Holmes, James Uplinger, Celso M de Melo, David Handelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07417">FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordinating heterogeneous robot teams from free-form natural-language instructions is hard. Language-only planners struggle with long-horizon coordination and hallucination, while purely formal methods require closed-world models. We present FLEET, a hybrid decentralized framework that turns language into optimized multi-robot schedules. An LLM front-end produces (i) a task graph with durations and precedence and (ii) a capability-aware robot--task fitness matrix; a formal back-end solves a makespan-minimization problem while the underlying robots execute their free-form subtasks with agentic closed-loop control. Across multiple free-form language-guided autonomy coordination benchmarks, FLEET improves success over state of the art generative planners on two-agent teams across heterogeneous tasks. Ablations show that mixed integer linear programming (MILP) primarily improves temporal structure, while LLM-derived fitness is decisive for capability-coupled tasks; together they deliver the highest overall performance. We demonstrate the translation to real world challenges with hardware trials using a pair of quadruped robots with disjoint capabilities.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2509.26339.pdf' target='_blank'>https://arxiv.org/pdf/2509.26339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric R. Damm, Thomas M. Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26339">Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2509.26339.pdf' target='_blank'>https://arxiv.org/pdf/2509.26339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric R. Damm, Thomas M. Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26339">Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2509.26339.pdf' target='_blank'>https://arxiv.org/pdf/2509.26339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric R. Damm, Thomas M. Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26339">Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2509.26339.pdf' target='_blank'>https://arxiv.org/pdf/2509.26339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric R. Damm, Thomas M. Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26339">Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2509.25518.pdf' target='_blank'>https://arxiv.org/pdf/2509.25518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Robertshaw, Han-Ru Wu, Alejandro Granados, Thomas C Booth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25518">World Model for AI Autonomous Navigation in Mechanical Thrombectomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2509.25518.pdf' target='_blank'>https://arxiv.org/pdf/2509.25518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Robertshaw, Han-Ru Wu, Alejandro Granados, Thomas C Booth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25518">World Model for AI Autonomous Navigation in Mechanical Thrombectomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2509.25373.pdf' target='_blank'>https://arxiv.org/pdf/2509.25373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25373">From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2509.25373.pdf' target='_blank'>https://arxiv.org/pdf/2509.25373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25373">From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2509.25282.pdf' target='_blank'>https://arxiv.org/pdf/2509.25282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexi Xu, Jiaqi Liu, Lanruo Wang, Su Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25282">Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple "world model" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2509.25282.pdf' target='_blank'>https://arxiv.org/pdf/2509.25282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexi Xu, Jiaqi Liu, Lanruo Wang, Su Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25282">Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple "world model" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2508.10770.pdf' target='_blank'>https://arxiv.org/pdf/2508.10770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Han, Yunfei Gao, Yong Li, Wuzhou Yu, Qiaosheng Zhang, Wenqi Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10770">From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatio-physical reasoning, a foundation capability for understanding the real physics world, is a critical step towards building robust world models. While recent vision language models (VLMs) have shown remarkable progress in specialized domains like multimodal mathematics and pure spatial understanding, their capability for spatio-physical reasoning remains largely unexplored. This paper provides a comprehensive diagnostic analysis of mainstream VLMs, revealing that current models perform inadequately on this crucial task. Further detailed analysis shows that this underperformance is largely attributable to biases caused by human-like prior and a lack of deep reasoning. To address these challenges, we apply supervised fine-tuning followed by rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant improvements in spatio-physical reasoning capabilities and surpassing leading proprietary models. Nevertheless, despite this success, the model's generalization to new physics scenarios remains limited -- underscoring the pressing need for new approaches in spatio-physical reasoning.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2508.06659.pdf' target='_blank'>https://arxiv.org/pdf/2508.06659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Martinez-Lopez, Tao Li, Yingdong Lu, Juntao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06659">In-Context Reinforcement Learning via Communicative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents often struggle to generalize to new tasks and contexts without updating their parameters, mainly because their learned representations and policies are overfit to the specifics of their training environments. To boost agents' in-context RL (ICRL) ability, this work formulates ICRL as a two-agent emergent communication problem and introduces CORAL (Communicative Representation for Adaptive RL), a framework that learns a transferable communicative context by decoupling latent representation learning from control. In CORAL, an Information Agent (IA) is pre-trained as a world model on a diverse distribution of tasks. Its objective is not to maximize task reward, but to build a world model and distill its understanding into concise messages. The emergent communication protocol is shaped by a novel Causal Influence Loss, which measures the effect that the message has on the next action. During deployment, the previously trained IA serves as a fixed contextualizer for a new Control Agent (CA), which learns to solve tasks by interpreting the provided communicative context. Our experiments demonstrate that this approach enables the CA to achieve significant gains in sample efficiency and successfully perform zero-shot adaptation with the help of pre-trained IA in entirely unseen sparse-reward environments, validating the efficacy of learning a transferable communicative representation.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2507.06952.pdf' target='_blank'>https://arxiv.org/pdf/2507.06952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyon Vafa, Peter G. Chang, Ashesh Rambachan, Sendhil Mullainathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06952">What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2506.16584.pdf' target='_blank'>https://arxiv.org/pdf/2506.16584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadav Kunievsky, James A. Evans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16584">Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2505.19698.pdf' target='_blank'>https://arxiv.org/pdf/2505.19698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19698">JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2505.16474.pdf' target='_blank'>https://arxiv.org/pdf/2505.16474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Xingzhuo Guo, Haoran Xu, Mingsheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16474">Consistent World Models via Foresight Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion and flow-based models have enabled significant progress in generation tasks across various modalities and have recently found applications in world modeling. However, unlike typical generation tasks that encourage sample diversity, world models entail different sources of uncertainty and require consistent samples aligned with the ground-truth trajectory, which is a limitation we empirically observe in diffusion models. We argue that a key bottleneck in learning consistent diffusion-based world models lies in the suboptimal predictive ability, which we attribute to the entanglement of condition understanding and target denoising within shared architectures and co-training schemes. To address this, we propose Foresight Diffusion (ForeDiff), a diffusion-based world modeling framework that enhances consistency by decoupling condition understanding from target denoising. ForeDiff incorporates a separate deterministic predictive stream to process conditioning inputs independently of the denoising stream, and further leverages a pretrained predictor to extract informative representations that guide generation. Extensive experiments on robot video prediction and scientific spatiotemporal forecasting show that ForeDiff improves both predictive accuracy and sample consistency over strong baselines, offering a promising direction for diffusion-based world models.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2504.12299.pdf' target='_blank'>https://arxiv.org/pdf/2504.12299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marko Tot, Shu Ishida, Abdelhak Lemkhenter, David Bignell, Pallavi Choudhury, Chris Lovett, Luis FranÃ§a, Matheus Ribeiro Furtado de MendonÃ§a, Tarun Gupta, Darren Gehring, Sam Devlin, Sergio Valcarcel Macua, Raluca Georgescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12299">Adapting a World Model for Trajectory Following in a 3D Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2503.16034.pdf' target='_blank'>https://arxiv.org/pdf/2503.16034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Radu Calinescu, Sinem Getir Yaman, Simos Gerasimou, Gricel VÃ¡zquez, Micah Bassett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16034">Verification and External Parameter Inference for Stochastic World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given its ability to analyse stochastic models ranging from discrete and continuous-time Markov chains to Markov decision processes and stochastic games, probabilistic model checking (PMC) is widely used to verify system dependability and performance properties. However, modelling the behaviour of, and verifying these properties for many software-intensive systems requires the joint analysis of multiple interdependent stochastic models of different types, which existing PMC techniques and tools cannot handle. To address this limitation, we introduce a tool-supported UniversaL stochasTIc Modelling, verificAtion and synThEsis (ULTIMATE) framework that supports the representation, verification and synthesis of heterogeneous multi-model stochastic systems with complex model interdependencies. Through its unique integration of multiple PMC paradigms, and underpinned by a novel verification method for handling model interdependencies, ULTIMATE unifies-for the first time-the modelling of probabilistic and nondeterministic uncertainty, discrete and continuous time, partial observability, and the use of both Bayesian and frequentist inference to exploit domain knowledge and data about the modelled system and its context. A comprehensive suite of case studies and experiments confirm the generality and effectiveness of our novel verification framework.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2503.09911.pdf' target='_blank'>https://arxiv.org/pdf/2503.09911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kohei Hayashi, Masanori Koyama, Julian Jorge Andrade Guerreiro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09911">Inter-environmental world modeling for continuous and compositional dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various world model frameworks are being developed today based on autoregressive frameworks that rely on discrete representations of actions and observations, and these frameworks are succeeding in constructing interactive generative models for the target environment of interest. Meanwhile, humans demonstrate remarkable generalization abilities to combine experiences in multiple environments to mentally simulate and learn to control agents in diverse environments. Inspired by this human capability, we introduce World modeling through Lie Action (WLA), an unsupervised framework that learns continuous latent action representations to simulate across environments. WLA learns a control interface with high controllability and predictive ability by simultaneously modeling the dynamics of multiple environments using Lie group theory and object-centric autoencoder. On synthetic benchmark and real-world datasets, we demonstrate that WLA can be trained using only video frames and, with minimal or no action labels, can quickly adapt to new environments with novel action sets.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2503.04256.pdf' target='_blank'>https://arxiv.org/pdf/2503.04256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiang Sun, Haotian Fu, Michael Littman, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04256">Knowledge Retention for Continual Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose DRAGO, a novel approach for continual model-based reinforcement learning aimed at improving the incremental development of world models across a sequence of tasks that differ in their reward functions but not the state space or dynamics. DRAGO comprises two key components: Synthetic Experience Rehearsal, which leverages generative models to create synthetic experiences from past tasks, allowing the agent to reinforce previously learned dynamics without storing data, and Regaining Memories Through Exploration, which introduces an intrinsic reward mechanism to guide the agent toward revisiting relevant states from prior tasks. Together, these components enable the agent to maintain a comprehensive and continually developing world model, facilitating more effective learning and adaptation across diverse environments. Empirical evaluations demonstrate that DRAGO is able to preserve knowledge across tasks, achieving superior performance in various continual learning scenarios.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2503.01411.pdf' target='_blank'>https://arxiv.org/pdf/2503.01411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Yan, Ahmed Abdulkadir, Gerrit A. Schatte, Giulia Aguzzi, Joonsu Gha, Nikola Pascher, Matthias Rosenthal, Yunlong Gao, Benjamin F. Grewe, Thilo Stadelmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01411">Learning Actionable World Models for Industrial Process Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To go from (passive) process monitoring to active process control, an effective AI system must learn about the behavior of the complex system from very limited training data, forming an ad-hoc digital twin with respect to process inputs and outputs that captures the consequences of actions on the process's world. We propose a novel methodology based on learning world models that disentangles process parameters in the learned latent representation, allowing for fine-grained control. Representation learning is driven by the latent factors influencing the processes through contrastive learning within a joint embedding predictive architecture. This makes changes in representations predictable from changes in inputs and vice versa, facilitating interpretability of key factors responsible for process variations, paving the way for effective control actions to keep the process within operational bounds. The effectiveness of our method is validated on the example of plastic injection molding, demonstrating practical relevance in proposing specific control actions for a notoriously unstable process.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2502.15657.pdf' target='_blank'>https://arxiv.org/pdf/2502.15657.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, SÃ¶ren Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.15657">Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2501.16443.pdf' target='_blank'>https://arxiv.org/pdf/2501.16443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16443">Objects matter: object-centric world models improve reinforcement learning in visually complex environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning has achieved remarkable success in learning control policies from pixels across a wide range of tasks, yet its application remains hindered by low sample efficiency, requiring significantly more environment interactions than humans to reach comparable performance. Model-based reinforcement learning (MBRL) offers a solution by leveraging learnt world models to generate simulated experience, thereby improving sample efficiency. However, in visually complex environments, small or dynamic elements can be critical for decision-making. Yet, traditional MBRL methods in pixel-based environments typically rely on auto-encoding with an $L_2$ loss, which is dominated by large areas and often fails to capture decision-relevant details. To address these limitations, we propose an object-centric MBRL pipeline, which integrates recent advances in computer vision to allow agents to focus on key decision-related elements. Our approach consists of four main steps: (1) annotating key objects related to rewards and goals with segmentation masks, (2) extracting object features using a pre-trained, frozen foundation vision model, (3) incorporating these object features with the raw observations to predict environmental dynamics, and (4) training the policy using imagined trajectories generated by this object-centric world model. Building on the efficient MBRL algorithm STORM, we call this pipeline OC-STORM. We demonstrate OC-STORM's practical value in overcoming the limitations of conventional MBRL approaches on both Atari games and the visually complex game Hollow Knight.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2501.14174.pdf' target='_blank'>https://arxiv.org/pdf/2501.14174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyeob Baek, Yi-Fu Wu, Gautam Singh, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14174">Dreamweaver: Learning Compositional World Models from Pixels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans have an innate ability to decompose their perceptions of the world into objects and their attributes, such as colors, shapes, and movement patterns. This cognitive process enables us to imagine novel futures by recombining familiar concepts. However, replicating this ability in artificial intelligence systems has proven challenging, particularly when it comes to modeling videos into compositional concepts and generating unseen, recomposed futures without relying on auxiliary data, such as text, masks, or bounding boxes. In this paper, we propose Dreamweaver, a neural architecture designed to discover hierarchical and compositional representations from raw videos and generate compositional future simulations. Our approach leverages a novel Recurrent Block-Slot Unit (RBSU) to decompose videos into their constituent objects and attributes. In addition, Dreamweaver uses a multi-future-frame prediction objective to capture disentangled representations for dynamic concepts more effectively as well as static concepts. In experiments, we demonstrate our model outperforms current state-of-the-art baselines for world modeling when evaluated under the DCI framework across multiple datasets. Furthermore, we show how the modularized concept representations of our model enable compositional imagination, allowing the generation of novel videos by recombining attributes from previously seen objects. cun-bjy.github.io/dreamweaver-website
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2501.11949.pdf' target='_blank'>https://arxiv.org/pdf/2501.11949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian He, Wenqi Liang, Chunhui Hao, Gan Sun, Jiandong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11949">GLAM: Global-Local Variation Awareness in Mamba-based World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mimicking the real interaction trajectory in the inference of the world model has been shown to improve the sample efficiency of model-based reinforcement learning (MBRL) algorithms. Many methods directly use known state sequences for reasoning. However, this approach fails to enhance the quality of reasoning by capturing the subtle variation between states. Much like how humans infer trends in event development from this variation, in this work, we introduce Global-Local variation Awareness Mamba-based world model (GLAM) that improves reasoning quality by perceiving and predicting variation between states. GLAM comprises two Mambabased parallel reasoning modules, GMamba and LMamba, which focus on perceiving variation from global and local perspectives, respectively, during the reasoning process. GMamba focuses on identifying patterns of variation between states in the input sequence and leverages these patterns to enhance the prediction of future state variation. LMamba emphasizes reasoning about unknown information, such as rewards, termination signals, and visual representations, by perceiving variation in adjacent states. By integrating the strengths of the two modules, GLAM accounts for highervalue variation in environmental changes, providing the agent with more efficient imagination-based training. We demonstrate that our method outperforms existing methods in normalized human scores on the Atari 100k benchmark.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2411.14991.pdf' target='_blank'>https://arxiv.org/pdf/2411.14991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JosÃ©phine Pazem, Marius Krumm, Alexander Q. Vining, Lukas J. Fiderer, Hans J. Briegel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14991">Free Energy Projective Simulation (FEPS): Active inference with interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last decade, the free energy principle (FEP) and active inference (AIF) have achieved many successes connecting conceptual models of learning and cognition to mathematical models of perception and action. This effort is driven by a multidisciplinary interest in understanding aspects of self-organizing complex adaptive systems, including elements of agency. Various reinforcement learning (RL) models performing active inference have been proposed and trained on standard RL tasks using deep neural networks. Recent work has focused on improving such agents' performance in complex environments by incorporating the latest machine learning techniques. In this paper, we take an alternative approach. Within the constraints imposed by the FEP and AIF, we attempt to model agents in an interpretable way without deep neural networks by introducing Free Energy Projective Simulation (FEPS). Using internal rewards only, FEPS agents build a representation of their partially observable environments with which they interact. Following AIF, the policy to achieve a given task is derived from this world model by minimizing the expected free energy. Leveraging the interpretability of the model, techniques are introduced to deal with long-term goals and reduce prediction errors caused by erroneous hidden state estimation. We test the FEPS model on two RL environments inspired from behavioral biology: a timed response task and a navigation task in a partially observable grid. Our results show that FEPS agents fully resolve the ambiguity of both environments by appropriately contextualizing their observations based on prediction accuracy only. In addition, they infer optimal policies flexibly for any target observation in the environment.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2411.14322.pdf' target='_blank'>https://arxiv.org/pdf/2411.14322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun P S, Andrew Melnik, Gora Chand Nandi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14322">SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting and Dense Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2410.12822.pdf' target='_blank'>https://arxiv.org/pdf/2410.12822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Rigter, Tarun Gupta, Agrin Hilmkil, Chao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.12822">AVID: Adapting Video Diffusion Models to World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed-source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learned mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation.1 Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2409.16824.pdf' target='_blank'>https://arxiv.org/pdf/2409.16824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16824">Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimal decision-making under partial observability requires reasoning about the uncertainty of the environment's hidden state. However, most reinforcement learning architectures handle partial observability with sequence models that have no internal mechanism to incorporate uncertainty in their hidden state representation, such as recurrent neural networks, deterministic state-space models and transformers. Inspired by advances in probabilistic world models for reinforcement learning, we propose a standalone Kalman filter layer that performs closed-form Gaussian inference in linear state-space models and train it end-to-end within a model-free architecture to maximize returns. Similar to efficient linear recurrent layers, the Kalman filter layer processes sequential data using a parallel scan, which scales logarithmically with the sequence length. By design, Kalman filter layers are a drop-in replacement for other recurrent layers in standard model-free architectures, but importantly they include an explicit mechanism for probabilistic filtering of the latent state representation. Experiments in a wide variety of tasks with partial observability show that Kalman filter layers excel in problems where uncertainty reasoning is key for decision-making, outperforming other stateful models.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2409.14084.pdf' target='_blank'>https://arxiv.org/pdf/2409.14084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Ferreira, Moreno Schlageter, Raghu Rajan, Andre Biedenkapp, Frank Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14084">One-shot World Models Using a Transformer Trained on a Synthetic Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods. However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments. We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution. Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment. We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more complex environments remains a challenge, currently. Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2409.13228.pdf' target='_blank'>https://arxiv.org/pdf/2409.13228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Baumeister, Lukas Mack, Joerg Stueckler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13228">Incremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot adaptation is an important capability for intelligent robots that perform tasks in open-world settings such as everyday environments or flexible production. In this paper, we propose a novel approach for non-prehensile manipulation which incrementally adapts a physics-based dynamics model for model-predictive control (MPC). The model prediction is aligned with a few examples of robot-object interactions collected with the MPC. This is achieved by using a parallelizable rigid-body physics simulation as dynamic world model and sampling-based optimization of the model parameters. In turn, the optimized dynamics model can be used for MPC using efficient sampling-based optimization. We evaluate our few-shot adaptation approach in object pushing experiments in simulation and with a real robot.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2408.11816.pdf' target='_blank'>https://arxiv.org/pdf/2408.11816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony GX-Chen, Kenneth Marino, Rob Fergus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11816">Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher level of state abstraction to pixels, and attribute change at a higher level of temporal abstraction to primitive actions. This abstraction simplifies the transition dynamic by making specific future states easier to predict. We make use of this to propose a fully model-based algorithm that learns a discriminative world model, plans to explore efficiently with only a count-based intrinsic reward, and can subsequently plan to reach any discovered (abstract) states.
  We demonstrate the model's ability to (i) efficiently solve single tasks, (ii) transfer zero-shot and few-shot across item types and environments, and (iii) plan across long horizons. Across a suite of 2D crafting and MiniHack environments, we empirically show our model significantly out-performs state-of-the-art low-level methods (without abstraction), as well as performant model-free and model-based methods using the same abstraction. Finally, we show how to learn low level object-perturbing policies via reinforcement learning, and the object mapping itself by supervised learning.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2407.20506.pdf' target='_blank'>https://arxiv.org/pdf/2407.20506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupei Yang, Biwei Huang, Shikui Tu, Lei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20506">Boosting Efficiency in Task-Agnostic Exploration through Causal Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effectiveness of model training heavily relies on the quality of available training resources. However, budget constraints often impose limitations on data collection efforts. To tackle this challenge, we introduce causal exploration in this paper, a strategy that leverages the underlying causal knowledge for both data collection and model training. We, in particular, focus on enhancing the sample efficiency and reliability of the world model learning within the domain of task-agnostic reinforcement learning. During the exploration phase, the agent actively selects actions expected to yield causal insights most beneficial for world model training. Concurrently, the causal knowledge is acquired and incrementally refined with the ongoing collection of data. We demonstrate that causal exploration aids in learning accurate world models using fewer data and provide theoretical guarantees for its convergence. Empirical experiments, on both synthetic data and real-world applications, further validate the benefits of causal exploration.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2407.13466.pdf' target='_blank'>https://arxiv.org/pdf/2407.13466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elie Aljalbout, Nikolaos Sotirakis, Patrick van der Smagt, Maximilian Karl, Nutan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13466">LIMT: Language-Informed Multi-Task Visual World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most recent successes in robot reinforcement learning involve learning a specialized single-task agent.
  However, robots capable of performing multiple tasks can be much more valuable in real-world applications.
  Multi-task reinforcement learning can be very challenging due to the increased sample complexity and the potentially conflicting task objectives.
  Previous work on this topic is dominated by model-free approaches.
  The latter can be very sample inefficient even when learning specialized single-task agents.
  In this work, we focus on model-based multi-task reinforcement learning.
  We propose a method for learning multi-task visual world models, leveraging pre-trained language models to extract semantically meaningful task representations.
  These representations are used by the world model and policy to reason about task similarity in dynamics and behavior.
  Our results highlight the benefits of using language-driven task representations for world models and a clear advantage of model-based multi-task learning over the more common model-free paradigm.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2405.15383.pdf' target='_blank'>https://arxiv.org/pdf/2405.15383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicola Dainese, Matteo Merler, Minttu Alakuijala, Pekka Marttinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15383">Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has potential to be more precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2405.12399.pdf' target='_blank'>https://arxiv.org/pdf/2405.12399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, FranÃ§ois Fleuret
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12399">Diffusion for World Modeling: Visual Details Matter in Atari</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. We further demonstrate that DIAMOND's diffusion world model can stand alone as an interactive neural game engine by training on static Counter-Strike: Global Offensive gameplay. To foster future research on diffusion for world modeling, we release our code, agents, videos and playable world models at https://diamond-wm.github.io.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2405.02288.pdf' target='_blank'>https://arxiv.org/pdf/2405.02288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhua Wu, Bingzhao Gao, Jincheng Gao, Jianhao Yu, Hongqing Chu, Qiankun Yu, Xun Gong, Yi Chang, H. Eric Tseng, Hong Chen, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02288">Prospective Role of Foundation Models in Advancing Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of artificial intelligence and breakthroughs in deep learning, large-scale Foundation Models (FMs), such as GPT, Sora, etc., have achieved remarkable results in many fields including natural language processing and computer vision. The application of FMs in autonomous driving holds considerable promise. For example, they can contribute to enhancing scene understanding and reasoning. By pre-training on rich linguistic and visual data, FMs can understand and interpret various elements in a driving scene, and provide cognitive reasoning to give linguistic and action instructions for driving decisions and planning. Furthermore, FMs can augment data based on the understanding of driving scenarios to provide feasible scenes of those rare occurrences in the long tail distribution that are unlikely to be encountered during routine driving and data collection. The enhancement can subsequently lead to improvement in the accuracy and reliability of autonomous driving systems. Another testament to the potential of FMs' applications lies in World Models, exemplified by the DREAMER series, which showcases the ability to comprehend physical laws and dynamics. Learning from massive data under the paradigm of self-supervised learning, World Model can generate unseen yet plausible driving environments, facilitating the enhancement in the prediction of road users' behaviors and the off-line training of driving strategies. In this paper, we synthesize the applications and future trends of FMs in autonomous driving. By utilizing the powerful capabilities of FMs, we strive to tackle the potential issues stemming from the long-tail distribution in autonomous driving, consequently advancing overall safety in this domain.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2404.06356.pdf' target='_blank'>https://arxiv.org/pdf/2404.06356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, Jakob Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06356">Policy-Guided Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2402.02675.pdf' target='_blank'>https://arxiv.org/pdf/2402.02675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobin South, Alexander Camuto, Shrey Jain, Shayla Nguyen, Robert Mahari, Christian Paquin, Jason Morton, Alex 'Sandy' Pentland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02675">Verifiable evaluations of machine learning models using zkSNARKs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results-whether over task accuracy, bias evaluations, or safety checks-are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. We present a flexible proving system that enables verifiable attestations to be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a new transparency paradigm in the verifiable evaluation of private models.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2312.17227.pdf' target='_blank'>https://arxiv.org/pdf/2312.17227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jyothir S, Siddhartha Jalagam, Yann LeCun, Vlad Sobal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17227">Gradient-based Planning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours. While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations. Consequently, these models must be learned from data using neural networks. Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning. However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model. In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms. In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks. Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2311.09064.pdf' target='_blank'>https://arxiv.org/pdf/2311.09064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09064">Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2310.17330.pdf' target='_blank'>https://arxiv.org/pdf/2310.17330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjae Lee, Daesol Cho, Jonghae Park, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17330">CQM: Curriculum Reinforcement Learning with a Quantized World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the proposed method allows efficient explorations in an uninformed environment with raw goal examples only. Also, ours outperforms the state-of-the-art curriculum RL methods on data efficiency and performance, in various goal-reaching tasks even with ego-centric visual inputs.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2310.08287.pdf' target='_blank'>https://arxiv.org/pdf/2310.08287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Laurent, Emanuel Aldea, Gianni Franchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08287">A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The distribution of the weights of modern deep neural networks (DNNs) - crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions. Finally, to help the community improve our understanding of the Bayesian posterior, we will shortly release the first large-scale checkpoint dataset, including thousands of real-world models and our codes.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2306.01157.pdf' target='_blank'>https://arxiv.org/pdf/2306.01157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AlizÃ©e Pace, Hugo YÃ¨che, Bernhard SchÃ¶lkopf, Gunnar RÃ¤tsch, Guy Tennenholtz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01157">Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2305.16852.pdf' target='_blank'>https://arxiv.org/pdf/2305.16852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Towle, Ke Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16852">Model-Based Simulation for Optimising Smart Reply</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of them conveys the user's desired response. This is a significant challenge, due to the lack of datasets containing sets of responses to learn from. Resultantly, previous work has focused largely on post-hoc diversification, rather than explicitly learning to predict sets of responses. Motivated by this problem, we present a novel method SimSR, that employs model-based simulation to discover high-value response sets, through simulating possible user responses with a learned world model. Unlike previous approaches, this allows our method to directly optimise the end-goal of SR--maximising the relevance of at least one of the predicted replies. Empirically on two public datasets, when compared to SoTA baselines, our method achieves up to 21% and 18% improvement in ROUGE score and Self-ROUGE score respectively.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2305.14879.pdf' target='_blank'>https://arxiv.org/pdf/2305.14879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre CÃ´tÃ©, Peter Jansen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14879">ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2304.07219.pdf' target='_blank'>https://arxiv.org/pdf/2304.07219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Matthies, Muhammad Burhan Hafez, Mostafa Kotb, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07219">Model Predictive Control with Self-supervised Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC framework, so that the agent can reconstruct the original observation given the internal state representation. This allows our agent to have a more stable learning signal during training and also improves sample efficiency. Our proposed addition of another loss term leads to improved performance on both state- and image-based tasks from the DeepMind-Control suite.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2301.05832.pdf' target='_blank'>https://arxiv.org/pdf/2301.05832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tadahiro Taniguchi, Shingo Murata, Masahiro Suzuki, Dimitri Ognibene, Pablo Lanillos, Emre Ugur, Lorenzo Jamone, Tomoaki Nakamura, Alejandra Ciria, Bruno Lara, Giovanni Pezzulo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05832">World Models and Predictive Coding for Cognitive and Developmental Robotics: Frontiers and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e., controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2212.08712.pdf' target='_blank'>https://arxiv.org/pdf/2212.08712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Kazemi, Nicola Paoletti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08712">Causal Temporal Reasoning for Markov Decision Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce $\textit{PCFTL (Probabilistic CounterFactual Temporal Logic)}$, a new probabilistic temporal logic for the verification of Markov Decision Processes (MDP). PCFTL is the first to include operators for causal reasoning, allowing us to express interventional and counterfactual queries. Given a path formula $Ï$, an interventional property is concerned with the satisfaction probability of $Ï$ if we apply a particular change $I$ to the MDP (e.g., switching to a different policy); a counterfactual allows us to compute, given an observed MDP path $Ï$, what the outcome of $Ï$ would have been had we applied $I$ in the past. For its ability to reason about \textit{what-if} scenarios involving different configurations of the MDP, our approach represents a departure from existing probabilistic temporal logics that can only reason about a fixed system configuration. From a syntactic viewpoint, we introduce a generalized counterfactual operator that subsumes both interventional and counterfactual probabilities as well as the traditional probabilistic operator found in e.g., PCTL. From a semantics viewpoint, our logic is interpreted over a structural causal model translation of the MDP, which gives us a representation amenable to counterfactual reasoning. We evaluate PCFTL in the context of safe reinforcement learning using a benchmark of grid-world models.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2210.01240.pdf' target='_blank'>https://arxiv.org/pdf/2210.01240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abulhair Saparov, He He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.01240">Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2510.07092.pdf' target='_blank'>https://arxiv.org/pdf/2510.07092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Mereu, Aidan Scannell, Yuxin Hou, Yi Zhao, Aditya Jitta, Antonio Dominguez, Luigi Acerbi, Amos Storkey, Paul Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07092">Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are a powerful paradigm in AI and robotics, enabling agents to reason about the future by predicting visual observations or compact latent states. The 1X World Model Challenge introduces an open-source benchmark of real-world humanoid interaction, with two complementary tracks: sampling, focused on forecasting future image frames, and compression, focused on predicting future discrete latent codes. For the sampling track, we adapt the video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned future frame prediction. We condition the video generation on robot states using AdaLN-Zero, and further post-train the model using LoRA. For the compression track, we train a Spatio-Temporal Transformer model from scratch. Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386 in the compression task, securing 1st place in both challenges.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2510.07092.pdf' target='_blank'>https://arxiv.org/pdf/2510.07092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Mereu, Aidan Scannell, Yuxin Hou, Yi Zhao, Aditya Jitta, Antonio Dominguez, Luigi Acerbi, Amos Storkey, Paul Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07092">Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are a powerful paradigm in AI and robotics, enabling agents to reason about the future by predicting visual observations or compact latent states. The 1X World Model Challenge introduces an open-source benchmark of real-world humanoid interaction, with two complementary tracks: sampling, focused on forecasting future image frames, and compression, focused on predicting future discrete latent codes. For the sampling track, we adapt the video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned future frame prediction. We condition the video generation on robot states using AdaLN-Zero, and further post-train the model using LoRA. For the compression track, we train a Spatio-Temporal Transformer model from scratch. Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386 in the compression task, securing 1st place in both challenges.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2509.14758.pdf' target='_blank'>https://arxiv.org/pdf/2509.14758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ihab Tabbara, Yuxuan Yang, Ahmad Hamzeh, Maxwell Astafyev, Hussein Sibai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14758">Designing Latent Safety Filters using Pre-Trained Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety of vision-based control systems remains a major challenge hindering their deployment in critical settings. Safety filters have gained increased interest as effective tools for ensuring the safety of classical control systems, but their applications in vision-based control settings have so far been limited. Pre-trained vision models (PVRs) have been shown to be effective perception backbones for control in various robotics domains. In this paper, we are interested in examining their effectiveness when used for designing vision-based safety filters. We use them as backbones for classifiers defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety filters, and for latent world models. We discuss the trade-offs between training from scratch, fine-tuning, and freezing the PVRs when training the models they are backbones for. We also evaluate whether one of the PVRs is superior across all tasks, evaluate whether learned world models or Q-functions are better for switching decisions to safe policies, and discuss practical considerations for deploying these PVRs on resource-constrained devices.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2509.14758.pdf' target='_blank'>https://arxiv.org/pdf/2509.14758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ihab Tabbara, Yuxuan Yang, Ahmad Hamzeh, Maxwell Astafyev, Hussein Sibai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14758">Designing Latent Safety Filters using Pre-Trained Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety of vision-based control systems remains a major challenge hindering their deployment in critical settings. Safety filters have gained increased interest as effective tools for ensuring the safety of classical control systems, but their applications in vision-based control settings have so far been limited. Pre-trained vision models (PVRs) have been shown to be effective perception backbones for control in various robotics domains. In this paper, we are interested in examining their effectiveness when used for designing vision-based safety filters. We use them as backbones for classifiers defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety filters, and for latent world models. We discuss the trade-offs between training from scratch, fine-tuning, and freezing the PVRs when training the models they are backbones for. We also evaluate whether one of the PVRs is superior across all tasks, evaluate whether learned world models or Q-functions are better for switching decisions to safe policies, and discuss practical considerations for deploying these PVRs on resource-constrained devices.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2509.00210.pdf' target='_blank'>https://arxiv.org/pdf/2509.00210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00210">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.00210.pdf' target='_blank'>https://arxiv.org/pdf/2509.00210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00210">Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2508.11200.pdf' target='_blank'>https://arxiv.org/pdf/2508.11200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbin Lin, Bin Li, Kwok Wai Samuel Au
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11200">Visuomotor Grasping with World Models for Surgical Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grasping is a fundamental task in robot-assisted surgery (RAS), and automating it can reduce surgeon workload while enhancing efficiency, safety, and consistency beyond teleoperated systems. Most prior approaches rely on explicit object pose tracking or handcrafted visual features, limiting their generalization to novel objects, robustness to visual disturbances, and the ability to handle deformable objects. Visuomotor learning offers a promising alternative, but deploying it in RAS presents unique challenges, such as low signal-to-noise ratio in visual observations, demands for high safety and millimeter-level precision, as well as the complex surgical environment. This paper addresses three key challenges: (i) sim-to-real transfer of visuomotor policies to ex vivo surgical scenes, (ii) visuomotor learning using only a single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic grasping with a single policy that generalizes to diverse, unseen surgical objects without retraining or task-specific models. We introduce Grasp Anything for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping. GASv2 leverages a world-model-based architecture and a surgical perception pipeline for visual observations, combined with a hybrid control system for safe execution. We train the policy in simulation using domain randomization for sim-to-real transfer and deploy it on a real robot in both phantom-based and ex vivo surgical settings, using only a single pair of endoscopic cameras. Extensive experiments show our policy achieves a 65% success rate in both settings, generalizes to unseen objects and grippers, and adapts to diverse disturbances, demonstrating strong performance, generality, and robustness.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2508.10489.pdf' target='_blank'>https://arxiv.org/pdf/2508.10489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Ulmen, Ganesh Sundaram, Daniel GÃ¶rges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10489">Learning State-Space Models of Dynamic Systems from Arbitrary Data using Joint Embedding Predictive Architectures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of Joint Embedding Predictive Architectures (JEPAs), which appear to be more capable than reconstruction-based methods, this paper introduces a novel technique for creating world models using continuous-time dynamic systems from arbitrary observation data. The proposed method integrates sequence embeddings with neural ordinary differential equations (neural ODEs). It employs loss functions that enforce contractive embeddings and Lipschitz constants in state transitions to construct a well-organized latent state space. The approach's effectiveness is demonstrated through the generation of structured latent state-space models for a simple pendulum system using only image data. This opens up a new technique for developing more general control algorithms and estimation techniques with broad applications in robotics.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2507.08210.pdf' target='_blank'>https://arxiv.org/pdf/2507.08210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fryderyk Mantiuk, Hanqi Zhou, Charley M. Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08210">From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What drives an agent to explore the world while also maintaining control over the environment? From a child at play to scientists in the lab, intelligent agents must balance curiosity (the drive to seek knowledge) with competence (the drive to master and control the environment). Bridging cognitive theories of intrinsic motivation with reinforcement learning, we ask how evolving internal representations mediate the trade-off between curiosity (novelty or information gain) and competence (empowerment). We compare two model-based agents using handcrafted state abstractions (Tabular) or learning an internal world model (Dreamer). The Tabular agent shows curiosity and competence guide exploration in distinct patterns, while prioritizing both improves exploration. The Dreamer agent reveals a two-way interaction between exploration and representation learning, mirroring the developmental co-evolution of curiosity and competence. Our findings formalize adaptive exploration as a balance between pursuing the unknown and the controllable, offering insights for cognitive theories and efficient reinforcement learning.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2507.05169.pdf' target='_blank'>https://arxiv.org/pdf/2507.05169.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Xing, Mingkai Deng, Jinyu Hou, Zhiting Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05169">Critiques of World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2507.04370.pdf' target='_blank'>https://arxiv.org/pdf/2507.04370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Gao, Junhong Ye, Jiaqi Wang, Jitao Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04370">WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents. However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution. Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions. Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses. To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training. WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning. This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy. Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2507.04075.pdf' target='_blank'>https://arxiv.org/pdf/2507.04075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Burchi, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04075">Accurate and Efficient World Modeling with Masked Latent Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Dreamer algorithm has recently obtained remarkable performance across diverse environment domains by training powerful agents with simulated trajectories. However, the compressed nature of its world model's latent space can result in the loss of crucial information, negatively affecting the agent's performance. Recent approaches, such as $Î$-IRIS and DIAMOND, address this limitation by training more accurate world models. However, these methods require training agents directly from pixels, which reduces training efficiency and prevents the agent from benefiting from the inner representations learned by the world model. In this work, we propose an alternative approach to world modeling that is both accurate and efficient. We introduce EMERALD (Efficient MaskEd latent tRAnsformer worLD model), a world model using a spatial latent state with MaskGIT predictions to generate accurate trajectories in latent space and improve the agent performance. On the Crafter benchmark, EMERALD achieves new state-of-the-art performance, becoming the first method to surpass human experts performance within 10M environment steps. Our method also succeeds to unlock all 22 Crafter achievements at least once during evaluation.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2506.23032.pdf' target='_blank'>https://arxiv.org/pdf/2506.23032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bradly Alicea, Morgan Hough, Amanda Nelson, Jesse Parent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23032">A "Good" Regulator May Provide a World Model for Intelligent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One classic idea from the cybernetics literature is the Every Good Regulator Theorem (EGRT). The EGRT provides a means to identify good regulation, or the conditions under which an agent (regulator) can match the dynamical behavior of a system. We reevaluate and recast the EGRT in a modern context to provide insight into how intelligent autonomous learning systems might utilize a compressed global representation (world model). One-to-one mappings between a regulator (R) and the corresponding system (S) provide a reduced representation that preserves useful variety to match all possible outcomes of a system. Secondarily, we question the role of purpose or autonomy in this process, demonstrating how physical paradigms such as temporal criticality, non-normal denoising, and alternating procedural acquisition can recast behavior as statistical mechanics and yield regulatory relationships. These diverse physical systems challenge the notion of tightly-coupled good regulation when applied to non-uniform and out-of-distribution phenomena. Modern definitions of intelligence are found to be inadequate, and can be improved upon by viewing intelligence as embodied non-purposeful good regulation. Overall, we aim to recast the EGRT as a tool for contemporary Artificial Intelligence (AI) architectures by considering the role of good regulation in the implementation of world models.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2506.23032.pdf' target='_blank'>https://arxiv.org/pdf/2506.23032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bradly Alicea, Morgan Hough, Amanda Nelson, Jesse Parent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23032">A "Good" Regulator May Provide a World Model for Intelligent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One classic idea from the cybernetics literature is the Every Good Regulator Theorem (EGRT). The EGRT provides a means to identify good regulation, or the conditions under which an agent (regulator) can match the dynamical behavior of a system. We reevaluate and recast the EGRT in a modern context to provide insight into how intelligent autonomous learning systems might utilize a compressed global representation (world model). One-to-one mappings between a regulator (R) and the corresponding system (S) provide a reduced representation that preserves useful variety to match all possible outcomes of a system. The EGRT also extends to second-order cybernetics, where an internal model (M) observes the behavior of S and supervises a S-R closed loop mapping. Secondarily, we demonstrate how physical phenomena such as temporal criticality, non-normal denoising, and alternating procedural acquisition can recast behavior as statistical mechanics and yield regulatory relationships. These diverse physical systems challenge the notion of tightly-coupled good regulation when applied to non-uniform and out-of-distribution phenomena. Overall, we aim to recast the EGRT as a potential approach for developing world models that adapt and respond to a wide range of task environments.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2506.23032.pdf' target='_blank'>https://arxiv.org/pdf/2506.23032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bradly Alicea, Morgan Hough, Amanda Nelson, Jesse Parent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23032">A "Good" Regulator May Provide a World Model for Intelligent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One classic idea from the cybernetics literature is the Every Good Regulator Theorem (EGRT). The EGRT provides a means to identify good regulation, or the conditions under which an agent (regulator) can match the dynamical behavior of a system. We reevaluate and recast the EGRT in a modern context to provide insight into how intelligent autonomous learning systems might utilize a compressed global representation (world model). One-to-one mappings between a regulator (R) and the corresponding system (S) provide a reduced representation that preserves useful variety to match all possible outcomes of a system. The EGRT also extends to second-order cybernetics, where an internal model (M) observes the behavior of S and supervises a S-R closed loop mapping. Secondarily, we demonstrate how physical phenomena such as temporal criticality, non-normal denoising, and alternating procedural acquisition can recast behavior as statistical mechanics and yield regulatory relationships. These diverse physical systems challenge the notion of tightly-coupled good regulation when applied to non-uniform and out-of-distribution phenomena. Overall, we aim to recast the EGRT as a potential approach for developing world models that adapt and respond to a wide range of task environments.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2506.11773.pdf' target='_blank'>https://arxiv.org/pdf/2506.11773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikang Leng, Megha Thukral, Yaqi Liu, Hrudhai Rajasekhar, Shruthi K. Hiremath, Jiaman He, Thomas PlÃ¶tz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11773">AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major challenge in developing robust and generalizable Human Activity Recognition (HAR) systems for smart homes is the lack of large and diverse labeled datasets. Variations in home layouts, sensor configurations, and individual behaviors further exacerbate this issue. To address this, we leverage the idea of embodied AI agents-virtual agents that perceive and act within simulated environments guided by internal world models. We introduce AgentSense, a virtual data generation pipeline in which agents live out daily routines in simulated smart homes, with behavior guided by Large Language Models (LLMs). The LLM generates diverse synthetic personas and realistic routines grounded in the environment, which are then decomposed into fine-grained actions. These actions are executed in an extended version of the VirtualHome simulator, which we augment with virtual ambient sensors that record the agents' activities. Our approach produces rich, privacy-preserving sensor data that reflects real-world diversity. We evaluate AgentSense on five real HAR datasets. Models pretrained on the generated data consistently outperform baselines, especially in low-resource settings. Furthermore, combining the generated virtual sensor data with a small amount of real data achieves performance comparable to training on full real-world datasets. These results highlight the potential of using LLM-guided embodied agents for scalable and cost-effective sensor data generation in HAR.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2506.11112.pdf' target='_blank'>https://arxiv.org/pdf/2506.11112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christine Bauer, Li Chen, Nicola Ferro, Norbert Fuhr, Avishek Anand, Timo Breuer, Guglielmo Faggioli, Ophir Frieder, Hideo Joho, Jussi Karlgren, Johannes Kiesel, Bart P. Knijnenburg, Aldo Lipani, Lien Michiels, Andrea Papenmeier, Maria Soledad Pera, Mark Sanderson, Scott Sanner, Benno Stein, Johanne R. Trippas, Karin Verspoor, Martijn C Willemsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11112">Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2506.02923.pdf' target='_blank'>https://arxiv.org/pdf/2506.02923.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexis Bellot, Jonathan Richens, Tom Everitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02923">The Limits of Predicting Agents from Behaviour</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the complexity of AI systems and their interactions with the world increases, generating explanations for their behaviour is important for safely deploying AI. For agents, the most natural abstractions for predicting behaviour attribute beliefs, intentions and goals to the system. If an agent behaves as if it has a certain goal or belief, then we can make reasonable predictions about how it will behave in novel situations, including those where comprehensive safety evaluations are untenable. How well can we infer an agent's beliefs from their behaviour, and how reliably can these inferred beliefs predict the agent's behaviour in novel situations? We provide a precise answer to this question under the assumption that the agent's behaviour is guided by a world model. Our contribution is the derivation of novel bounds on the agent's behaviour in new (unseen) deployment environments, which represent a theoretical limit for predicting intentional agents from behavioural data alone. We discuss the implications of these results for several research areas including fairness and safety.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2506.01622.pdf' target='_blank'>https://arxiv.org/pdf/2506.01622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01622">General agents contain world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2506.01622.pdf' target='_blank'>https://arxiv.org/pdf/2506.01622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01622">General agents contain world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2506.00819.pdf' target='_blank'>https://arxiv.org/pdf/2506.00819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawood Wasif, Terrence J Moore, Chandan K Reddy, Jin-Hee Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00819">DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems map sensor data directly to control commands, but remain opaque, lack interpretability, and offer no formal safety guarantees. While recent vision-language-guided reinforcement learning (RL) methods introduce semantic feedback, they often rely on static prompts and fixed objectives, limiting adaptability to dynamic driving scenes. We present DriveMind, a unified semantic reward framework that integrates: (i) a contrastive Vision-Language Model (VLM) encoder for stepwise semantic anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via chain-of-thought (CoT) distillation, for dynamic prompt generation upon semantic drift; (iii) a hierarchical safety module enforcing kinematic constraints (e.g., speed, lane centering, stability); and (iv) a compact predictive world model to reward alignment with anticipated ideal states. DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route completion, and near-zero collisions in CARLA Town 2, outperforming baselines by over 4% in success rate. Its semantic reward generalizes zero-shot to real dash-cam data with minimal distributional shift, demonstrating robust cross-domain alignment and potential for real-world deployment.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2505.21872.pdf' target='_blank'>https://arxiv.org/pdf/2505.21872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George R. Nahass, Zhu Wang, Homa Rashidisabet, Won Hwa Kim, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi, Sathya N. Ravi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21872">Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine unlearning aims to remove the influence of specific training samples from a trained model without full retraining. While prior work has largely focused on privacy-motivated settings, we recast unlearning as a general-purpose tool for post-deployment model revision. Specifically, we focus on utilizing unlearning in clinical contexts where data shifts, device deprecation, and policy changes are common. To this end, we propose a bilevel optimization formulation of boundary-based unlearning that can be solved using iterative algorithms. We provide convergence guarantees when first-order algorithms are used to unlearn. Our method introduces tunable loss design for controlling the forgetting-retention tradeoff and supports novel model composition strategies that merge the strengths of distinct unlearning runs. Across benchmark and real-world clinical imaging datasets, our approach outperforms baselines on both forgetting and retention metrics, including scenarios involving imaging devices and anatomical outliers. This work establishes machine unlearning as a modular, practical alternative to retraining for real-world model maintenance in clinical applications.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2504.18576.pdf' target='_blank'>https://arxiv.org/pdf/2504.18576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofan Li, Chenming Wu, Zhao Yang, Zhihao Xu, Dingkang Liang, Yumeng Zhang, Ji Wan, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18576">DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents DriVerse, a generative model for simulating navigation-driven driving scenes from a single image and a future trajectory. Previous autonomous driving world models either directly feed the trajectory or discrete control signals into the generation pipeline, leading to poor alignment between the control inputs and the implicit features of the 2D base generative model, which results in low-fidelity video outputs. Some methods use coarse textual commands or discrete vehicle control signals, which lack the precision to guide fine-grained, trajectory-specific video generation, making them unsuitable for evaluating actual autonomous driving algorithms. DriVerse introduces explicit trajectory guidance in two complementary forms: it tokenizes trajectories into textual prompts using a predefined trend vocabulary for seamless language integration, and converts 3D trajectories into 2D spatial motion priors to enhance control over static content within the driving scene. To better handle dynamic objects, we further introduce a lightweight motion alignment module, which focuses on the inter-frame consistency of dynamic pixels, significantly enhancing the temporal coherence of moving elements over long sequences. With minimal training and no need for additional data, DriVerse outperforms specialized models on future video generation tasks across both the nuScenes and Waymo datasets. The code and models will be released to the public.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2503.21668.pdf' target='_blank'>https://arxiv.org/pdf/2503.21668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danaja Rutar, Alva Markelius, Konstantinos Voudouris, JosÃ© HernÃ¡ndez-Orallo, Lucy Cheke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21668">Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality. This capability enables us to predict events, plan action and navigate environments, all of which rely on a composite sense of objecthood. Despite its importance, there is no single, unified account of objecthood, though multiple theoretical frameworks provide insights. In the first part of this paper, we present a comprehensive overview of the main theoretical frameworks in objecthood research - Gestalt psychology, enactive cognition, and developmental psychology - and identify the core capabilities each framework attributes to object understanding, as well as what functional roles they play in shaping world models in biological agents. Given the foundational role of objecthood in world modelling, understanding objecthood is also essential in AI. In the second part of the paper, we evaluate how current AI paradigms approach and test objecthood capabilities compared to those in cognitive science. We define an AI paradigm as a combination of how objecthood is conceptualised, the methods used for studying objecthood, the data utilised, and the evaluation techniques. We find that, whilst benchmarks can detect that AI systems model isolated aspects of objecthood, the benchmarks cannot detect when AI systems lack functional integration across these capabilities, not solving the objecthood challenge fully. Finally, we explore novel evaluation approaches that align with the integrated vision of objecthood outlined in this paper. These methods are promising candidates for advancing from isolated object capabilities toward general-purpose AI with genuine object understanding in real-world contexts.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2503.10370.pdf' target='_blank'>https://arxiv.org/pdf/2503.10370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iman Nematollahi, Branton DeMoss, Akshay L Chandra, Nick Hawes, Wolfram Burgard, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10370">LUMOS: Language-Conditioned Imitation Learning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce LUMOS, a language-conditioned multi-task imitation learning framework for robotics. LUMOS learns skills by practicing them over many long-horizon rollouts in the latent space of a learned world model and transfers these skills zero-shot to a real robot. By learning on-policy in the latent space of the learned world model, our algorithm mitigates policy-induced distribution shift which most offline imitation learning methods suffer from. LUMOS learns from unstructured play data with fewer than 1% hindsight language annotations but is steerable with language commands at test time. We achieve this coherent long-horizon performance by combining latent planning with both image- and language-based hindsight goal relabeling during training, and by optimizing an intrinsic reward defined in the latent space of the world model over multiple time steps, effectively reducing covariate shift. In experiments on the difficult long-horizon CALVIN benchmark, LUMOS outperforms prior learning-based methods with comparable approaches on chained multi-task evaluations. To the best of our knowledge, we are the first to learn a language-conditioned continuous visuomotor control for a real-world robot within an offline world model. Videos, dataset and code are available at http://lumos.cs.uni-freiburg.de.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2503.04931.pdf' target='_blank'>https://arxiv.org/pdf/2503.04931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierrick Lorang, Hong Lu, Matthias Scheutz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04931">Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting quickly to dynamic, uncertain environments-often called "open worlds"-remains a major challenge in robotics. Traditional Task and Motion Planning (TAMP) approaches struggle to cope with unforeseen changes, are data-inefficient when adapting, and do not leverage world models during learning. We address this issue with a hybrid planning and learning system that integrates two models: a low level neural network based model that learns stochastic transitions and drives exploration via an Intrinsic Curiosity Module (ICM), and a high level symbolic planning model that captures abstract transitions using operators, enabling the agent to plan in an "imaginary" space and generate reward machines. Our evaluation in a robotic manipulation domain with sequential novelty injections demonstrates that our approach converges faster and outperforms state-of-the-art hybrid methods.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2503.04421.pdf' target='_blank'>https://arxiv.org/pdf/2503.04421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Yuan, Anders SÃ¸gaard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04421">Revisiting the Othello World Model Hypothesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Li et al. (2023) used the Othello board game as a test case for the ability of GPT-2 to induce world models, and were followed up by Nanda et al. (2023b). We briefly discuss the original experiments, expanding them to include more language models with more comprehensive probing. Specifically, we analyze sequences of Othello board states and train the model to predict the next move based on previous moves. We evaluate seven language models (GPT-2, T5, Bart, Flan-T5, Mistral, LLaMA-2, and Qwen2.5) on the Othello task and conclude that these models not only learn to play Othello, but also induce the Othello board layout. We find that all models achieve up to 99% accuracy in unsupervised grounding and exhibit high similarity in the board features they learned. This provides considerably stronger evidence for the Othello World Model Hypothesis than previous works.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2503.04416.pdf' target='_blank'>https://arxiv.org/pdf/2503.04416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Burchi, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04416">Learning Transformer-based World Models with Contrastive Predictive Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2502.11663.pdf' target='_blank'>https://arxiv.org/pdf/2502.11663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, Zehuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11663">MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2502.00622.pdf' target='_blank'>https://arxiv.org/pdf/2502.00622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Qi, Haocheng Yin, Aris Zhu, Yilun Du, Heng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00622">Strengthening Generative Robot Policies through Predictive World Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present generative predictive control (GPC), a learning control framework that (i) clones a generative diffusion-based policy from expert demonstrations, (ii) trains a predictive action-conditioned world model from both expert demonstrations and random explorations, and (iii) synthesizes an online planner that ranks and optimizes the action proposals from (i) by looking ahead into the future using the world model from (ii). Across a variety of robotic manipulation tasks, we demonstrate that GPC consistently outperforms behavior cloning in both state-based and vision-based settings, in simulation and in the real world.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2501.05610.pdf' target='_blank'>https://arxiv.org/pdf/2501.05610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshan Zhou, Carol M. Menassa, Vineet R. Kamat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05610">Towards Probabilistic Inference of Human Motor Intentions by Assistive Mobile Robots Controlled via a Brain-Computer Interface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assistive mobile robots are a transformative technology that helps persons with disabilities regain the ability to move freely. Although autonomous wheelchairs significantly reduce user effort, they still require human input to allow users to maintain control and adapt to changing environments. Brain Computer Interface (BCI) stands out as a highly user-friendly option that does not require physical movement. Current BCI systems can understand whether users want to accelerate or decelerate, but they implement these changes in discrete speed steps rather than allowing for smooth, continuous velocity adjustments. This limitation prevents the systems from mimicking the natural, fluid speed changes seen in human self-paced motion. The authors aim to address this limitation by redesigning the perception-action cycle in a BCI controlled robotic system: improving how the robotic agent interprets the user's motion intentions (world state) and implementing these actions in a way that better reflects natural physical properties of motion, such as inertia and damping. The scope of this paper focuses on the perception aspect. We asked and answered a normative question "what computation should the robotic agent carry out to optimally perceive incomplete or noisy sensory observations?" Empirical EEG data were collected, and probabilistic representation that served as world state distributions were learned and evaluated in a Generative Adversarial Network framework. The ROS framework was established that connected with a Gazebo environment containing a digital twin of an indoor space and a virtual model of a robotic wheelchair. Signal processing and statistical analyses were implemented to identity the most discriminative features in the spatial-spectral-temporal dimensions, which are then used to construct the world model for the robotic agent to interpret user motion intentions as a Bayesian observer.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2412.06139.pdf' target='_blank'>https://arxiv.org/pdf/2412.06139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Qiao, Henry Williams, David Valencia, Bruce MacDonald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06139">Bounded Exploration with World Model Uncertainty in Soft Actor-Critic Reinforcement Learning Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the bottlenecks preventing Deep Reinforcement Learning algorithms (DRL) from real-world applications is how to explore the environment and collect informative transitions efficiently. The present paper describes bounded exploration, a novel exploration method that integrates both 'soft' and intrinsic motivation exploration. Bounded exploration notably improved the Soft Actor-Critic algorithm's performance and its model-based extension's converging speed. It achieved the highest score in 6 out of 8 experiments. Bounded exploration presents an alternative method to introduce intrinsic motivations to exploration when the original reward function has strict meanings.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2411.12671.pdf' target='_blank'>https://arxiv.org/pdf/2411.12671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano De Giorgis, Aldo Gangemi, Alessandro Russo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12671">Neurosymbolic Graph Enrichment for Grounded World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of artificial intelligence systems capable of understanding and reasoning about complex real-world scenarios is a significant challenge. In this work we present a novel approach to enhance and exploit LLM reactive capability to address complex problems and interpret deeply contextual real-world meaning. We introduce a method and a tool for creating a multimodal, knowledge-augmented formal representation of meaning that combines the strengths of large language models with structured semantic representations. Our method begins with an image input, utilizing state-of-the-art large language models to generate a natural language description. This description is then transformed into an Abstract Meaning Representation (AMR) graph, which is formalized and enriched with logical design patterns, and layered semantics derived from linguistic and factual knowledge bases. The resulting graph is then fed back into the LLM to be extended with implicit knowledge activated by complex heuristic learning, including semantic implicatures, moral values, embodied cognition, and metaphorical representations. By bridging the gap between unstructured language models and formal semantic structures, our method opens new avenues for tackling intricate problems in natural language understanding and reasoning.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2408.08900.pdf' target='_blank'>https://arxiv.org/pdf/2408.08900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostafa Rahgouy, Hamed Babaei Giglou, Mehnaz Tabassum, Dongji Feng, Amit Das, Taher Rahgooy, Gerry Dozier, Cheryl D. Seals
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08900">Towards Effective Authorship Attribution: Integrating Class-Incremental Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AA is the process of attributing an unidentified document to its true author from a predefined group of known candidates, each possessing multiple samples. The nature of AA necessitates accommodating emerging new authors, as each individual must be considered unique. This uniqueness can be attributed to various factors, including their stylistic preferences, areas of expertise, gender, cultural background, and other personal characteristics that influence their writing. These diverse attributes contribute to the distinctiveness of each author, making it essential for AA systems to recognize and account for these variations. However, current AA benchmarks commonly overlook this uniqueness and frame the problem as a closed-world classification, assuming a fixed number of authors throughout the system's lifespan and neglecting the inclusion of emerging new authors. This oversight renders the majority of existing approaches ineffective for real-world applications of AA, where continuous learning is essential. These inefficiencies manifest as current models either resist learning new authors or experience catastrophic forgetting, where the introduction of new data causes the models to lose previously acquired knowledge. To address these inefficiencies, we propose redefining AA as CIL, where new authors are introduced incrementally after the initial training phase, allowing the system to adapt and learn continuously. To achieve this, we briefly examine subsequent CIL approaches introduced in other domains. Moreover, we have adopted several well-known CIL methods, along with an examination of their strengths and weaknesses in the context of AA. Additionally, we outline potential future directions for advancing CIL AA systems. As a result, our paper can serve as a starting point for evolving AA systems from closed-world models to continual learning through CIL paradigms.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2407.13518.pdf' target='_blank'>https://arxiv.org/pdf/2407.13518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrey Gorodetskiy, Konstantin Mironov, Aleksandr Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13518">Model-based Policy Optimization using Symbolic World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of learning-based control methods in robotics presents significant challenges. One is that model-free reinforcement learning algorithms use observation data with low sample efficiency. To address this challenge, a prevalent approach is model-based reinforcement learning, which involves employing an environment dynamics model. We suggest approximating transition dynamics with symbolic expressions, which are generated via symbolic regression. Approximation of a mechanical system with a symbolic model has fewer parameters than approximation with neural networks, which can potentially lead to higher accuracy and quality of extrapolation. We use a symbolic dynamics model to generate trajectories in model-based policy optimization to improve the sample efficiency of the learning algorithm. We evaluate our approach across various tasks within simulated environments. Our method demonstrates superior sample efficiency in these tasks compared to model-free and model-based baseline methods.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2407.12197.pdf' target='_blank'>https://arxiv.org/pdf/2407.12197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Donato, Thomas George Thuruthel, Egidio Falotico
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12197">Towards Interpretable Visuo-Tactile Predictive Models for Soft Robot Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous systems face the intricate challenge of navigating unpredictable environments and interacting with external objects. The successful integration of robotic agents into real-world situations hinges on their perception capabilities, which involve amalgamating world models and predictive skills. Effective perception models build upon the fusion of various sensory modalities to probe the surroundings. Deep learning applied to raw sensory modalities offers a viable option. However, learning-based perceptive representations become difficult to interpret. This challenge is particularly pronounced in soft robots, where the compliance of structures and materials makes prediction even harder. Our work addresses this complexity by harnessing a generative model to construct a multi-modal perception model for soft robots and to leverage proprioceptive and visual information to anticipate and interpret contact interactions with external objects. A suite of tools to interpret the perception model is furnished, shedding light on the fusion and prediction processes across multiple sensory inputs after the learning phase. We will delve into the outlooks of the perception model and its implications for control purposes.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2407.02508.pdf' target='_blank'>https://arxiv.org/pdf/2407.02508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Zhou, Yihao Qin, Dan Xu, Yiding Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02508">Physics-informed Imitative Reinforcement Learning for Real-world Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in imitative reinforcement learning (IRL) have considerably enhanced the ability of autonomous agents to assimilate expert demonstrations, leading to rapid skill acquisition in a range of demanding tasks. However, such learning-based agents face significant challenges when transferring knowledge to highly dynamic closed-loop environments. Their performance is significantly impacted by the conflicting optimization objectives of imitation learning (IL) and reinforcement learning (RL), sample inefficiency, and the complexity of uncovering the hidden world model and physics. To address this challenge, we propose a physics-informed IRL that is entirely data-driven. It leverages both expert demonstration data and exploratory data with a joint optimization objective, allowing the underlying physical principles of vehicle dynamics to emerge naturally from the training process. The performance is evaluated through empirical experiments and results exceed popular IL, RL and IRL algorithms in closed-loop settings on Waymax benchmark. Our approach exhibits 37.8% reduction in collision rate and 22.2% reduction in off-road rate compared to the baseline method.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2406.06835.pdf' target='_blank'>https://arxiv.org/pdf/2406.06835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangeetha Sivasothy, Scott Barnett, Rena Logothetis, Mohamed Abdelrazek, Zafaryab Rasool, Srikanth Thudumu, Zac Brannelly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06835">Large language models for generating rules, yay or nay?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Engineering safety-critical systems such as medical devices and digital health intervention systems is complex, where long-term engagement with subject-matter experts (SMEs) is needed to capture the systems' expected behaviour. In this paper, we present a novel approach that leverages Large Language Models (LLMs), such as GPT-3.5 and GPT-4, as a potential world model to accelerate the engineering of software systems. This approach involves using LLMs to generate logic rules, which can then be reviewed and informed by SMEs before deployment. We evaluate our approach using a medical rule set, created from the pandemic intervention monitoring system in collaboration with medical professionals during COVID-19. Our experiments show that 1) LLMs have a world model that bootstraps implementation, 2) LLMs generated less number of rules compared to experts, and 3) LLMs do not have the capacity to generate thresholds for each rule. Our work shows how LLMs augment the requirements' elicitation process by providing access to a world model for domains.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2405.15083.pdf' target='_blank'>https://arxiv.org/pdf/2405.15083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Burchi, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15083">MuDreamer: Learning Predictive World Models without Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using a pixel reconstruction loss. However, while the reconstruction loss is essential to Dreamer's performance, it also necessitates modeling unnecessary information. Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential. In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world model without the need for reconstructing input signals. Rather than relying on pixel reconstruction, hidden representations are instead learned by predicting the environment value function and previously selected actions. Similar to predictive self-supervised methods for images, we find that the use of batch normalization is crucial to prevent learning collapse. We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability. We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches, replacing the environment background with task-irrelevant real-world videos. Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2402.15283.pdf' target='_blank'>https://arxiv.org/pdf/2402.15283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Benfeghoul, Umais Zahid, Qinghai Guo, Zafeirios Fountas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15283">When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2402.10877.pdf' target='_blank'>https://arxiv.org/pdf/2402.10877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Richens, Tom Everitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10877">Robust agents learn causal world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2402.03570.pdf' target='_blank'>https://arxiv.org/pdf/2402.03570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03570">Diffusion World Model: Future Modeling Beyond Step-by-Step Rollout for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive queries. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and is comparable to or slightly surpassing their model-free counterparts.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2401.01021.pdf' target='_blank'>https://arxiv.org/pdf/2401.01021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Butian Xiong, Liguang Zhou, Tin Lun Lam, Yangsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01021">Class Relevance Learning For Out-of-distribution Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image classification plays a pivotal role across diverse applications, yet challenges persist when models are deployed in real-world scenarios. Notably, these models falter in detecting unfamiliar classes that were not incorporated during classifier training, a formidable hurdle for safe and effective real-world model deployment, commonly known as out-of-distribution (OOD) detection. While existing techniques, like max logits, aim to leverage logits for OOD identification, they often disregard the intricate interclass relationships that underlie effective detection. This paper presents an innovative class relevance learning method tailored for OOD detection. Our method establishes a comprehensive class relevance learning framework, strategically harnessing interclass relationships within the OOD pipeline. This framework significantly augments OOD detection capabilities. Extensive experimentation on diverse datasets, encompassing generic image classification datasets (Near OOD and Far OOD datasets), demonstrates the superiority of our method over state-of-the-art alternatives for OOD detection.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2312.09844.pdf' target='_blank'>https://arxiv.org/pdf/2312.09844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09844">Small Dataset, Big Gains: Enhancing Reinforcement Learning by Offline Pre-Training with Model Based Augmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning leverages pre-collected datasets of transitions to train policies. It can serve as effective initialization for online algorithms, enhancing sample efficiency and speeding up convergence. However, when such datasets are limited in size and quality, offline pre-training can produce sub-optimal policies and lead to degraded online reinforcement learning performance. In this paper we propose a model-based data augmentation strategy to maximize the benefits of offline reinforcement learning pre-training and reduce the scale of data needed to be effective. Our approach leverages a world model of the environment trained on the offline dataset to augment states during offline pre-training. We evaluate our approach on a variety of MuJoCo robotic tasks and our results show it can jump-start online fine-tuning and substantially reduce - in some cases by an order of magnitude - the required number of environment interactions.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2311.17406.pdf' target='_blank'>https://arxiv.org/pdf/2311.17406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwei Chen, Anxing Xiao, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17406">LLM-State: Open World State Representation for Long-horizon Task Planning with Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the problem of long-horizon task planning with the Large Language Model (LLM) in an open-world household environment. Existing works fail to explicitly track key objects and attributes, leading to erroneous decisions in long-horizon tasks, or rely on highly engineered state features and feedback, which is not generalizable. We propose an open state representation that provides continuous expansion and updating of object attributes from the LLM's inherent capabilities for context understanding and historical action reasoning. Our proposed representation maintains a comprehensive record of an object's attributes and changes, enabling robust retrospective summary of the sequence of actions leading to the current state. This allows continuously updating world model to enhance context understanding for decision-making in task planning. We validate our model through experiments across simulated and real-world task planning scenarios, demonstrating significant improvements over baseline methods in a variety of tasks requiring long-horizon state tracking and reasoning. (Video\footnote{Video demonstration: \url{https://youtu.be/QkN-8pxV3Mo}.})
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2311.15930.pdf' target='_blank'>https://arxiv.org/pdf/2311.15930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, GrÃ©goire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, Pascal Vincent
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15930">WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose WorldSense, a benchmark designed to assess the extent to which LLMs are consistently able to sustain tacit world models, by testing how they draw simple inferences from descriptions of simple arrangements of entities. Worldsense is a synthetic benchmark with three problem types, each with their own trivial control, which explicitly avoids bias by decorrelating the abstract structure of problems from the vocabulary and expressions, and by decorrelating all problem subparts with the correct response. We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly, we show that while finetuning on similar problems does result in substantial improvements -- within- and out-of-distribution -- the finetuned models do not generalise beyond a constraint problem space.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2311.08345.pdf' target='_blank'>https://arxiv.org/pdf/2311.08345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Tenhumberg, Darius Burschka, Berthold BÃ¤uml
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08345">Speeding Up Optimization-based Motion Planning through Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning collision-free motions for robots with many degrees of freedom is challenging in environments with complex obstacle geometries. Recent work introduced the idea of speeding up the planning by encoding prior experience of successful motion plans in a neural network. However, this "neural motion planning" did not scale to complex robots in unseen 3D environments as needed for real-world applications. Here, we introduce "basis point set", well-known in computer vision, to neural motion planning as a modern compact environment encoding enabling efficient supervised training networks that generalize well over diverse 3D worlds. Combined with a new elaborate training scheme, we reach a planning success rate of 100%. We use the network to predict an educated initial guess for an optimization-based planner (OMP), which quickly converges to a feasible solution, massively outperforming random multi-starts when tested on previously unseen environments. For the DLR humanoid Agile Justin with 19DoF and in challenging obstacle environments, optimal paths can be generated in 200ms using only a single CPU core. We also show a first successful real-world experiment based on a high-resolution world model from an integrated 3D sensor.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2311.05938.pdf' target='_blank'>https://arxiv.org/pdf/2311.05938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Tenhumberg, Arman Mielke, Berthold BÃ¤uml
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05938">Efficient Learning of Fast Inverse Kinematics with Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast inverse kinematics (IK) is a central component in robotic motion planning. For complex robots, IK methods are often based on root search and non-linear optimization algorithms. These algorithms can be massively sped up using a neural network to predict a good initial guess, which can then be refined in a few numerical iterations. Besides previous work on learning-based IK, we present a learning approach for the fundamentally more complex problem of IK with collision avoidance. We do this in diverse and previously unseen environments. From a detailed analysis of the IK learning problem, we derive a network and unsupervised learning architecture that removes the need for a sample data generation step. Using the trained network's prediction as an initial guess for a two-stage Jacobian-based solver allows for fast and accurate computation of the collision-free IK. For the humanoid robot, Agile Justin (19 DoF), the collision-free IK is solved in less than 10 milliseconds (on a single CPU core) and with an accuracy of 10^-4 m and 10^-3 rad based on a high-resolution world model generated from the robot's integrated 3D sensor. Our method massively outperforms a random multi-start baseline in a benchmark with the 19 DoF humanoid and challenging 3D environments. It requires ten times less training time than a supervised training method while achieving comparable results.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2309.00638.pdf' target='_blank'>https://arxiv.org/pdf/2309.00638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peer Nagy, Sascha Frey, Silvia Sapora, Kang Li, Anisoara Calinescu, Stefan Zohren, Jakob Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00638">Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing a generative model of realistic order flow in financial markets is a challenging open problem, with numerous applications for market participants. Addressing this, we propose the first end-to-end autoregressive generative model that generates tokenized limit order book (LOB) messages. These messages are interpreted by a Jax-LOB simulator, which updates the LOB state. To handle long sequences efficiently, the model employs simplified structured state-space layers to process sequences of order book states and tokenized messages. Using LOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message data, converting groups of successive digits to tokens, similar to tokenization in large language models. Out-of-sample results show promising performance in approximating the data distribution, as evidenced by low model perplexity. Furthermore, the mid-price returns calculated from the generated order flow exhibit a significant correlation with the data, indicating impressive conditional forecast performance. Due to the granularity of generated data, and the accuracy of the model, it offers new application areas for future work beyond forecasting, e.g. acting as a world model in high-frequency financial reinforcement learning applications. Overall, our results invite the use and extension of the model in the direction of autoregressive large financial models for the generation of high-frequency financial data and we commit to open-sourcing our code to facilitate future research.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2307.08947.pdf' target='_blank'>https://arxiv.org/pdf/2307.08947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Wardat, Breno Dantas Cruz, Wei Le, Hridesh Rajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08947">An Effective Data-Driven Approach for Localizing Deep Learning Faults</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) applications are being used to solve problems in critical domains (e.g., autonomous driving or medical diagnosis systems). Thus, developers need to debug their systems to ensure that the expected behavior is delivered. However, it is hard and expensive to debug DNNs. When the failure symptoms or unsatisfied accuracies are reported after training, we lose the traceability as to which part of the DNN program is responsible for the failure. Even worse, sometimes, a deep learning program has different types of bugs. To address the challenges of debugging DNN models, we propose a novel data-driven approach that leverages model features to learn problem patterns. Our approach extracts these features, which represent semantic information of faults during DNN training. Our technique uses these features as a training dataset to learn and infer DNN fault patterns. Also, our methodology automatically links bug symptoms to their root causes, without the need for manually crafted mappings, so that developers can take the necessary steps to fix faults. We evaluate our approach using real-world and mutated models. Our results demonstrate that our technique can effectively detect and diagnose different bug types. Finally, our technique achieved better accuracy, precision, and recall than prior work for mutated models. Also, our approach achieved comparable results for real-world models in terms of accuracy and performance to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2306.02021.pdf' target='_blank'>https://arxiv.org/pdf/2306.02021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Gao, Zhiyu Lin, Yunfan Yang, Jitao Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02021">Towards Black-box Adversarial Example Detection: A Data Reconstruction-based Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial example detection is known to be an effective adversarial defense method. Black-box attack, which is a more realistic threat and has led to various black-box adversarial training-based defense methods, however, does not attract considerable attention in adversarial example detection. In this paper, we fill this gap by positioning the problem of black-box adversarial example detection (BAD). Data analysis under the introduced BAD settings demonstrates (1) the incapability of existing detectors in addressing the black-box scenario and (2) the potential of exploring BAD solutions from a data perspective. To tackle the BAD problem, we propose a data reconstruction-based adversarial example detection method. Specifically, we use variational auto-encoder (VAE) to capture both pixel and frequency representations of normal examples. Then we use reconstruction error to detect adversarial examples. Compared with existing detection methods, the proposed method achieves substantially better detection performance in BAD, which helps promote the deployment of adversarial example detection-based defense solutions in real-world models.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2304.06011.pdf' target='_blank'>https://arxiv.org/pdf/2304.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aravind Venugopal, Stephanie Milani, Fei Fang, Balaraman Ravindran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06011">MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods often suffer from high sample complexity, limiting their use in real-world problems where data is sparse or expensive to collect. Although latent-variable world models have been employed to address this issue by generating abundant synthetic data for MARL training, most of these models cannot encode vital global information available during training into their latent states, which hampers learning efficiency. The few exceptions that incorporate global information assume centralized execution of their learned policies, which is impractical in many applications with partial observability.
  We propose a novel model-based MARL algorithm, MABL (Multi-Agent Bi-Level world model), that learns a bi-level latent-variable world model from high-dimensional inputs. Unlike existing models, MABL is capable of encoding essential global information into the latent states during training while guaranteeing the decentralized execution of learned policies. For each agent, MABL learns a global latent state at the upper level, which is used to inform the learning of an agent latent state at the lower level. During execution, agents exclusively use lower-level latent states and act independently. Crucially, MABL can be combined with any model-free MARL algorithm for policy learning. In our empirical evaluation with complex discrete and continuous multi-agent tasks including SMAC, Flatland, and MAMuJoCo, MABL surpasses SOTA multi-agent latent-variable world models in both sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2302.08007.pdf' target='_blank'>https://arxiv.org/pdf/2302.08007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bita Rouhani, Ritchie Zhao, Venmugil Elango, Rasoul Shafipour, Mathew Hall, Maral Mesmakhosroshahi, Ankit More, Levi Melnick, Maximilian Golub, Girish Varatkar, Lei Shao, Gaurav Kolhe, Dimitry Melts, Jasmine Klar, Renee L'Heureux, Matt Perry, Doug Burger, Eric Chung, Zhaoxia Deng, Sam Naghshineh, Jongsoo Park, Maxim Naumov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08007">With Shared Microexponents, A Little Shifting Goes a Long Way</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2302.04732.pdf' target='_blank'>https://arxiv.org/pdf/2302.04732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ãngel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth Holstein, Ameet Talwalkar, Jason I. Hong, Adam Perer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04732">Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models with high accuracy on test data can still produce systematic failures, such as harmful biases and safety issues, when deployed in the real world. To detect and mitigate such failures, practitioners run behavioral evaluation of their models, checking model outputs for specific types of inputs. Behavioral evaluation is important but challenging, requiring that practitioners discover real-world patterns and validate systematic failures. We conducted 18 semi-structured interviews with ML practitioners to better understand the challenges of behavioral evaluation and found that it is a collaborative, use-case-first process that is not adequately supported by existing task- and domain-specific tools. Using these findings, we designed Zeno, a general-purpose framework for visualizing and testing AI systems across diverse use cases. In four case studies with participants using Zeno on real-world models, we found that practitioners were able to reproduce previous manual analyses and discover new systematic failures.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2302.03086.pdf' target='_blank'>https://arxiv.org/pdf/2302.03086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Branton DeMoss, Paul Duckworth, Jakob Foerster, Nick Hawes, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03086">DITTO: Offline Imitation Learning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For imitation learning algorithms to scale to real-world challenges, they must handle high-dimensional observations, offline learning, and policy-induced covariate-shift. We propose DITTO, an offline imitation learning algorithm which addresses all three of these problems. DITTO optimizes a novel distance metric in the latent space of a learned world model: First, we train a world model on all available trajectory data, then, the imitation agent is unrolled from expert start states in the learned model, and penalized for its latent divergence from the expert dataset over multiple time steps. We optimize this multi-step latent divergence using standard reinforcement learning algorithms, which provably induces imitation learning, and empirically achieves state-of-the art performance and sample efficiency on a range of Atari environments from pixels, without any online environment access. We also adapt other standard imitation learning algorithms to the world model setting, and show that this considerably improves their performance. Our results show how creative use of world models can lead to a simple, robust, and highly-performant policy-learning framework.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2301.05746.pdf' target='_blank'>https://arxiv.org/pdf/2301.05746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Gurung, Mojtaba Komeili, Arthur Szlam, Jason Weston, Jack Urbanek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05746">Infusing Commonsense World Models with Graph Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While language models have become more capable of producing compelling language, we find there are still gaps in maintaining consistency, especially when describing events in a dynamically changing world. We study the setting of generating narratives in an open world text adventure game, where a graph representation of the underlying game state can be used to train models that consume and output both grounded graph representations and natural language descriptions and actions. We build a large set of tasks by combining crowdsourced and simulated gameplays with a novel dataset of complex actions in order to to construct such models. We find it is possible to improve the consistency of action narration models by training on graph contexts and targets, even if graphs are not present at test time. This is shown both in automatic metrics and human evaluations. We plan to release our code, the new set of tasks, and best performing models.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2211.15944.pdf' target='_blank'>https://arxiv.org/pdf/2211.15944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Kessler, Mateusz Ostaszewski, MichaÅ Bortkiewicz, Mateusz Å»arski, Maciej WoÅczyk, Jack Parker-Holder, Stephen J. Roberts, Piotr MiÅoÅ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15944">The Effectiveness of World Models for Continual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2509.24527.pdf' target='_blank'>https://arxiv.org/pdf/2509.24527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danijar Hafner, Wilson Yan, Timothy Lillicrap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24527">Training Agents Inside of Scalable World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2509.24527.pdf' target='_blank'>https://arxiv.org/pdf/2509.24527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danijar Hafner, Wilson Yan, Timothy Lillicrap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24527">Training Agents Inside of Scalable World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2508.16512.pdf' target='_blank'>https://arxiv.org/pdf/2508.16512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chun-Peng Chang, Chen-Yu Wang, Julian Schmidt, Holger Caesar, Alain Pagani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16512">Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called "world models". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2508.12087.pdf' target='_blank'>https://arxiv.org/pdf/2508.12087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanjiang Yang, Yang Shen, Yueming Li, Meng Li, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12087">MAPF-World: Action World Model for Multi-Agent Path Finding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2508.12087.pdf' target='_blank'>https://arxiv.org/pdf/2508.12087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanjiang Yang, Yang Shen, Yueming Li, Meng Li, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12087">MAPF-World: Action World Model for Multi-Agent Path Finding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2507.04898.pdf' target='_blank'>https://arxiv.org/pdf/2507.04898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edmund Ross, Claudia Drygala, Leonhard Schwarz, Samir Kaiser, Francesca di Mare, Tobias Breiten, Hanno Gottschalk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04898">When do World Models Successfully Learn Dynamical Systems?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we explore the use of compact latent representations with learned time dynamics ('World Models') to simulate physical systems. Drawing on concepts from control theory, we propose a theoretical framework that explains why projecting time slices into a low-dimensional space and then concatenating to form a history ('Tokenization') is so effective at learning physics datasets, and characterise when exactly the underlying dynamics admit a reconstruction mapping from the history of previous tokenized frames to the next. To validate these claims, we develop a sequence of models with increasing complexity, starting with least-squares regression and progressing through simple linear layers, shallow adversarial learners, and ultimately full-scale generative adversarial networks (GANs). We evaluate these models on a variety of datasets, including modified forms of the heat and wave equations, the chaotic regime 2D Kuramoto-Sivashinsky equation, and a challenging computational fluid dynamics (CFD) dataset of a 2D KÃ¡rmÃ¡n vortex street around a fixed cylinder, where our model is successfully able to recreate the flow.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2506.21782.pdf' target='_blank'>https://arxiv.org/pdf/2506.21782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Narendra, Dmitry Makarov, Aleksandr Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21782">M3PO: Massively Multi-Task Model-Based Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a scalable model-based reinforcement learning (MBRL) framework designed to address sample inefficiency in single-task settings and poor generalization in multi-task domains. Existing model-based approaches like DreamerV3 rely on pixel-level generative models that neglect control-centric representations, while model-free methods such as PPO suffer from high sample complexity and weak exploration. M3PO integrates an implicit world model, trained to predict task outcomes without observation reconstruction, with a hybrid exploration strategy that combines model-based planning and model-free uncertainty-driven bonuses. This eliminates the bias-variance trade-off in prior methods by using discrepancies between model-based and model-free value estimates to guide exploration, while maintaining stable policy updates through a trust-region optimizer. M3PO provides an efficient and robust alternative to existing model-based policy optimization approaches and achieves state-of-the-art performance across multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2506.13833.pdf' target='_blank'>https://arxiv.org/pdf/2506.13833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoliang Chen, Le Chang, Xin Yu, Yunhe Huang, Xianling Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13833">A Survey on World Models Grounded in Acoustic Physical Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2506.11302.pdf' target='_blank'>https://arxiv.org/pdf/2506.11302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>HÃ©ctor CarriÃ³n, Yutong Bai, VÃ­ctor A. HernÃ¡ndez Castro, Kishan Panaganti, Ayush Zenith, Matthew Trang, Tony Zhang, Pietro Perona, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11302">TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2503.06170.pdf' target='_blank'>https://arxiv.org/pdf/2503.06170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjoon Jeong, Junha Chun, Soonwoo Cha, Taesup Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06170">Object-Centric World Model for Language-Guided Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A world model is essential for an agent to predict the future and plan in domains such as autonomous driving and robotics. To achieve this, recent advancements have focused on video generation, which has gained significant attention due to the impressive success of diffusion models. However, these models require substantial computational resources. To address these challenges, we propose a world model leveraging object-centric representation space using slot attention, guided by language instructions. Our model perceives the current state as an object-centric representation and predicts future states in this representation space conditioned on natural language instructions. This approach results in a more compact and computationally efficient model compared to diffusion-based generative alternatives. Furthermore, it flexibly predicts future states based on language instructions, and offers a significant advantage in manipulation tasks where object recognition is crucial. In this paper, we demonstrate that our latent predictive world model surpasses generative world models in visuo-linguo-motor control tasks, achieving superior sample and computation efficiency. We also investigate the generalization performance of the proposed method and explore various strategies for predicting actions using object-centric representations.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2503.05573.pdf' target='_blank'>https://arxiv.org/pdf/2503.05573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feeza Khan Khanzada, Jaerock Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05573">InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm for autonomous driving, where data efficiency and robustness are critical. Yet, existing solutions often rely on carefully crafted, task specific extrinsic rewards, limiting generalization to new tasks or environments. In this paper, we propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle Exploration), a method that leverages purely intrinsic, disagreement based rewards within a Dreamer based MBRL framework. By training an ensemble of world models, the agent actively explores high uncertainty regions of environments without any task specific feedback. This approach yields a task agnostic latent representation, allowing for rapid zero shot or few shot fine tuning on downstream driving tasks such as lane following and collision avoidance. Experimental results in both seen and unseen environments demonstrate that InDRiVE achieves higher success rates and fewer infractions compared to DreamerV2 and DreamerV3 baselines despite using significantly fewer training steps. Our findings highlight the effectiveness of purely intrinsic exploration for learning robust vehicle control behaviors, paving the way for more scalable and adaptable autonomous driving systems.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2502.16372.pdf' target='_blank'>https://arxiv.org/pdf/2502.16372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Liu, Huihua Zhao, Chenran Li, Joydeep Biswas, Soha Pouya, Yan Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16372">COMPASS: Cross-embodiment Mobility Policy via Residual RL and Skill Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As robots are increasingly deployed in diverse application domains, generalizable cross-embodiment mobility policies are increasingly essential. While classical mobility stacks have proven effective on specific robot platforms, they pose significant challenges when scaling to new embodiments. Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), offer alternative solutions but suffer from covariate shift, sparse sampling in large environments, and embodiment-specific constraints.
  This paper introduces COMPASS, a novel workflow for developing cross-embodiment mobility policies by integrating IL, residual RL, and policy distillation. We begin with IL on a mobile robot, leveraging easily accessible teacher policies to train a foundational model that combines a world model with a mobility policy. Building on this base, we employ residual RL to fine-tune embodiment-specific policies, exploiting pre-trained representations to improve sampling efficiency in handling various physical constraints and sensor modalities. Finally, policy distillation merges these embodiment-specialist policies into a single robust cross-embodiment policy.
  We empirically demonstrate that COMPASS scales effectively across diverse robot platforms while maintaining adaptability to various environment configurations, achieving a generalist policy with a success rate approximately 5X higher than the pre-trained IL policy. The resulting framework offers an efficient, scalable solution for cross-embodiment mobility, enabling robots with different designs to navigate safely and efficiently in complex scenarios.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2501.10809.pdf' target='_blank'>https://arxiv.org/pdf/2501.10809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramesh Bahadur Bist, Lilong Chai, Shawna Weimer, Hannah Atungulua, Chantel Pennicott, Xiao Yang, Sachin Subedi, Chaitanya Pallerla, Yang Tian, Dongyi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10809">Efficient auto-labeling of large-scale poultry datasets (ALPD) using an ensemble model with self- and active-learning approaches</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of artificial intelligence in poultry farming has highlighted the challenge of efficiently labeling large, diverse datasets. Manual annotation is time-consuming and costly, making it impractical for modern systems that continuously generate data. This study addresses this challenge by exploring semi-supervised auto-labeling methods, integrating self and active learning approaches to develop an efficient, label-scarce framework for auto-labeling large poultry datasets (ALPD). For this study, video data were collected from broilers and laying hens housed. Various machine learning models, including zero-shot models and supervised models, were utilized for broilers and hens detection. The results showed that YOLOv8s-World and YOLOv9s performed better when compared performance metrics for broiler and hen detection under supervised learning, while among the semi-supervised model, YOLOv8s-ALPD achieved the highest precision (96.1%) and recall (99%) with an RMSE of 1.87. The hybrid YOLO-World model, incorporating the optimal YOLOv8s backbone with zero-shot models, demonstrated the highest overall performance. It achieved a precision of 99.2%, recall of 99.4%, and an F1 score of 98.7% for detection. In addition, the semi-supervised models with minimal human intervention (active learning) reduced annotation time by over 80% compared to full manual labeling. Moreover, integrating zero-shot models with the best models enhanced broiler and hen detection, achieving comparable results to supervised models while significantly increasing speed. In conclusion, integrating semi-supervised auto-labeling and zero-shot models significantly improves detection accuracy. It reduces manual annotation efforts, offering a promising solution to optimize AI-driven systems in poultry farming, advancing precision livestock management, and promoting more sustainable practices.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2501.05329.pdf' target='_blank'>https://arxiv.org/pdf/2501.05329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05329">Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an efficient knowledge transfer approach for model-based reinforcement learning, addressing the challenge of deploying large world models in resource-constrained environments. Our method distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance on the MT30 benchmark with a normalized score of 28.45, a substantial improvement over the original 1M parameter model's score of 18.93. This demonstrates the ability of our distillation technique to consolidate complex multi-task knowledge effectively. Additionally, we apply FP16 post-training quantization, reducing the model size by 50% while maintaining performance. Our work bridges the gap between the power of large models and practical deployment constraints, offering a scalable solution for efficient and accessible multi-task reinforcement learning in robotics and other resource-limited domains.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2410.02742.pdf' target='_blank'>https://arxiv.org/pdf/2410.02742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolan Liu, Jishen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02742">Grounding Large Language Models In Embodied Environment With Imperfect World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite a widespread success in various applications, large language models (LLMs) often stumble when tackling basic physical reasoning or executing robotics tasks, due to a lack of direct experience with the physical nuances of the real world. To address these issues, we propose a Grounding Large language model with Imperfect world MOdel (GLIMO), which utilizes proxy world models such as simulators to collect and synthesize trining data. GLIMO incorporates an LLM agent-based data generator to automatically create high-quality and diverse instruction datasets. The generator includes an iterative self-refining module for temporally consistent experience sampling, a diverse set of question-answering instruction seeds, and a retrieval-augmented generation module for reflecting on prior experiences. Comprehensive experiments show that our approach improve the performance of strong open-source LLMs like LLaMA-3 with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$ across three different benchmarks, respectively. The performance is able to compete with or surpass their larger counterparts such as GPT-4.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2409.19455.pdf' target='_blank'>https://arxiv.org/pdf/2409.19455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Lamarre, Jonathan Kelly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19455">The Importance of Adaptive Decision-Making for Autonomous Long-Range Planetary Surface Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-distance driving is an important component of planetary surface exploration. Unforeseen events often require human operators to adjust mobility plans, but this approach does not scale and will be insufficient for future missions. Interest in self-reliant rovers is increasing, however the research community has not yet given significant attention to autonomous, adaptive decision-making. In this paper, we look back at specific planetary mobility operations where human-guided adaptive planning played an important role in mission safety and productivity. Inspired by the abilities of human experts, we identify shortcomings of existing autonomous mobility algorithms for robots operating in off-road environments like planetary surfaces. We advocate for adaptive decision-making capabilities such as unassisted learning from past experiences and more reliance on stochastic world models. The aim of this work is to highlight promising research avenues to enhance ground planning tools and, ultimately, long-range autonomy algorithms on board planetary rovers.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2407.15845.pdf' target='_blank'>https://arxiv.org/pdf/2407.15845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yakir Oz, Gilad Yehudai, Gal Vardi, Itai Antebi, Michal Irani, Niv Haim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15845">Reconstructing Training Data From Real World Models Trained with Transfer Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current methods for reconstructing training data from trained classifiers are restricted to very small models, limited training set sizes, and low-resolution images. Such restrictions hinder their applicability to real-world scenarios. In this paper, we present a novel approach enabling data reconstruction in realistic settings for models trained on high-resolution images. Our method adapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios -- specifically, targeting models trained via transfer learning over image embeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs data reconstruction in the embedding space rather than in the image space, showcasing its applicability beyond visual data. Moreover, we introduce a novel clustering-based method to identify good reconstructions from thousands of candidates. This significantly improves on previous works that relied on knowledge of the training set to identify good reconstructed images. Our findings shed light on a potential privacy risk for data leakage from models trained using transfer learning.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2406.15850.pdf' target='_blank'>https://arxiv.org/pdf/2406.15850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Rodriguez-Sanchez, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15850">Learning Abstract World Model for Value-preserving Planning with Options</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally-extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP. We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2403.15908.pdf' target='_blank'>https://arxiv.org/pdf/2403.15908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15908">Deep Gaussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and probabilistic models. During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2403.09859.pdf' target='_blank'>https://arxiv.org/pdf/2403.09859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, Aviv Tamar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09859">MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to $15\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2402.11871.pdf' target='_blank'>https://arxiv.org/pdf/2402.11871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naman Shah, Jayesh Nagpal, Siddharth Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11871">From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. We propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into logic-based world models, facilitating efficient zero-shot generalization to significantly more complex tasks. Empirical results demonstrate that our approach achieves performance comparable to hand-crafted models, successfully scaling execution horizons and handling up to 18 times more objects than seen in training, providing the first autonomous framework for learning transferable symbolic abstractions from raw robot trajectories.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2312.05230.pdf' target='_blank'>https://arxiv.org/pdf/2312.05230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiting Hu, Tianmin Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05230">Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2310.09997.pdf' target='_blank'>https://arxiv.org/pdf/2310.09997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09997">Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to plan at many different levels of abstraction enables agents to envision the long-term repercussions of their decisions and thus enables sample-efficient learning. This becomes particularly beneficial in complex environments from high-dimensional state space such as pixels, where the goal is distant and the reward sparse. We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model. Forecaster learns an abstract model of its environment by modelling the transitions dynamics at an abstract level and training a world model on such transition. It then uses this world model to choose optimal high-level goals through a tree-search planning procedure. It additionally trains a low-level policy that learns to reach those goals. Our method not only captures building world models with longer horizons, but also, planning with such models in downstream tasks. We empirically demonstrate Forecaster's potential in both single-task learning and generalization to new tasks in the AntMaze domain.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2310.08731.pdf' target='_blank'>https://arxiv.org/pdf/2310.08731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Wei Zhou, Robert Wright, Mark O. Riedl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08731">Novelty Detection in Reinforcement Learning with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as novelties. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL focused novelty detection algorithms.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2307.03762.pdf' target='_blank'>https://arxiv.org/pdf/2307.03762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Ma, Chi Zhang, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03762">Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, knowledge acquisition isn't solely reliant on passive input but requires repeated trials and errors. We conclude by outlining promising future research directions in the field of artificial general intelligence.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2305.13396.pdf' target='_blank'>https://arxiv.org/pdf/2305.13396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chris Doyle, Sarah Shader, Michelle Lau, Megumi Sano, Daniel L. K. Yamins, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13396">Developmental Curiosity and Social Interaction in Virtual Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2305.02749.pdf' target='_blank'>https://arxiv.org/pdf/2305.02749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongwei Yu, Jingqing Ruan, Dengpeng Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02749">Explainable Reinforcement Learning via a Causal World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2304.11104.pdf' target='_blank'>https://arxiv.org/pdf/2304.11104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander W. Goodall, Francesco Belardinelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11104">Approximate Shielding of Atari Agents for Safe Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Balancing exploration and conservatism in the constrained setting is an important problem if we are to use reinforcement learning for meaningful tasks in the real world. In this paper, we propose a principled algorithm for safe exploration based on the concept of shielding. Previous approaches to shielding assume access to a safety-relevant abstraction of the environment or a high-fidelity simulator. Instead, our work is based on latent shielding - another approach that leverages world models to verify policy roll-outs in the latent space of a learned dynamics model. Our novel algorithm builds on this previous work, using safety critics and other additional features to improve the stability and farsightedness of the algorithm. We demonstrate the effectiveness of our approach by running experiments on a small set of Atari games with state dependent safety labels. We present preliminary results that show our approximate shielding algorithm effectively reduces the rate of safety violations, and in some cases improves the speed of convergence and quality of the final agent.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2304.02868.pdf' target='_blank'>https://arxiv.org/pdf/2304.02868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng Tsai, Xiaochen Zhou, Sierra S. Liu, Jing Li, Mo Yu, Hongyuan Mei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02868">Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2302.13051.pdf' target='_blank'>https://arxiv.org/pdf/2302.13051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel LundÃ©n, Lars Hummelgren, Jan Kudlicka, Oscar Eriksson, David Broman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.13051">Suspension Analysis and Selective Continuation-Passing Style for Universal Probabilistic Programming Languages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Universal probabilistic programming languages (PPLs) make it relatively easy to encode and automatically solve statistical inference problems. To solve inference problems, PPL implementations often apply Monte Carlo inference algorithms that rely on execution suspension. State-of-the-art solutions enable execution suspension either through (i) continuation-passing style (CPS) transformations or (ii) efficient, but comparatively complex, low-level solutions that are often not available in high-level languages. CPS transformations introduce overhead due to unnecessary closure allocations -- a problem the PPL community has generally overlooked. To reduce overhead, we develop a new efficient selective CPS approach for PPLs. Specifically, we design a novel static suspension analysis technique that determines parts of programs that require suspension, given a particular inference algorithm. The analysis allows selectively CPS transforming the program only where necessary. We formally prove the correctness of the analysis and implement the analysis and transformation in the Miking CorePPL compiler. We evaluate the implementation for a large number of Monte Carlo inference algorithms on real-world models from phylogenetics, epidemiology, and topic modeling. The evaluation results demonstrate significant improvements across all models and inference algorithms.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2301.06294.pdf' target='_blank'>https://arxiv.org/pdf/2301.06294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Balloch, Zhiyu Lin, Robert Wright, Xiangyu Peng, Mustafa Hussain, Aarun Srinivas, Julia Kim, Mark O. Riedl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06294">Neuro-Symbolic World Models for Adapting to Open World Novelty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-world novelty--a sudden change in the mechanics or properties of an environment--is a common occurrence in the real world. Novelty adaptation is an agent's ability to improve its policy performance post-novelty. Most reinforcement learning (RL) methods assume that the world is a closed, fixed process. Consequentially, RL policies adapt inefficiently to novelties. To address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic world model for rapid novelty adaptation. WorldCloner learns an efficient symbolic representation of the pre-novelty environment transitions, and uses this transition model to detect novelty and efficiently adapt to novelty in a single-shot fashion. Additionally, WorldCloner augments the policy learning process using imagination-based adaptation, where the world model simulates transitions of the post-novelty environment to help the policy adapt. By blending ''imagined'' transitions with interactions in the post-novelty environment, performance can be recovered with fewer total environment interactions. Using environments designed for studying novelty in sequential decision-making problems, we show that the symbolic world model helps its neural policy adapt more efficiently than model-based and model-based neural-only reinforcement learning methods.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2203.15945.pdf' target='_blank'>https://arxiv.org/pdf/2203.15945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.15945">A Framework for Improving the Reliability of Black-box Variational Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2203.15945.pdf' target='_blank'>https://arxiv.org/pdf/2203.15945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.15945">A Framework for Improving the Reliability of Black-box Variational Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2112.02538.pdf' target='_blank'>https://arxiv.org/pdf/2112.02538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng-Cheng Kuo, Yu-Peng Hsieh, Huan-Hsin Tseng, Chi-Te Wang, Shih-Hau Fang, Yu Tsao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.02538">Toward Real-World Voice Disorder Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objective: Voice disorders significantly compromise individuals' ability to speak in their daily lives. Without early diagnosis and treatment, these disorders may deteriorate drastically. Thus, automatic classification systems at home are desirable for people who are inaccessible to clinical disease assessments. However, the performance of such systems may be weakened due to the constrained resources and domain mismatch between the clinical data and noisy real-world data. Methods: This study develops a compact and domain-robust voice disorder classification system to identify the utterances of health, neoplasm, and benign structural diseases. Our proposed system utilizes a feature extractor model composed of factorized convolutional neural networks and subsequently deploys domain adversarial training to reconcile the domain mismatch by extracting domain invariant features. Results: The results show that the unweighted average recall in the noisy real-world domain improved by 13% and remained at 80% in the clinic domain with only slight degradation. The domain mismatch was effectively eliminated. Moreover, the proposed system reduced the usage of both memory and computation by over 73.9%. Conclusion: By deploying factorized convolutional neural networks and domain adversarial training, domain-invariant features can be derived for voice disorder classification with limited resources. The promising results confirm that the proposed system can significantly reduce resource consumption and improve classification accuracy by considering the domain mismatch. Significance: To the best of our knowledge, this is the first study that jointly considers real-world model compression and noise-robustness issues in voice disorder classification. The proposed system is intended for application to embedded systems with limited resources.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2510.02287.pdf' target='_blank'>https://arxiv.org/pdf/2510.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Li, Antonio Torralba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02287">MultiModal Action Conditioned Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2510.02287.pdf' target='_blank'>https://arxiv.org/pdf/2510.02287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Li, Antonio Torralba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02287">MultiModal Action Conditioned Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2509.19538.pdf' target='_blank'>https://arxiv.org/pdf/2509.19538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19538">DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2509.19538.pdf' target='_blank'>https://arxiv.org/pdf/2509.19538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19538">DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2509.15479.pdf' target='_blank'>https://arxiv.org/pdf/2509.15479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BjÃ¶rn MÃ¶ller, Zhengyang Li, Malte Stelzer, Thomas Graave, Fabian Bettels, Muaaz Ataya, Tim Fingscheidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15479">OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2509.15479.pdf' target='_blank'>https://arxiv.org/pdf/2509.15479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BjÃ¶rn MÃ¶ller, Zhengyang Li, Malte Stelzer, Thomas Graave, Fabian Bettels, Muaaz Ataya, Tim Fingscheidt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15479">OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2509.04600.pdf' target='_blank'>https://arxiv.org/pdf/2509.04600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Ying, Zhongyuan Hu, Rui Zhang, Ronghui Li, Yu Lu, Zijiao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04600">WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global human motion reconstruction from in-the-wild monocular videos is increasingly demanded across VR, graphics, and robotics applications, yet requires accurate mapping of human poses from camera to world coordinates-a task challenged by depth ambiguity, motion ambiguity, and the entanglement between camera and human movements. While human-motion-centric approaches excel in preserving motion details and physical plausibility, they suffer from two critical limitations: insufficient exploitation of camera orientation information and ineffective integration of camera translation cues. We present WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and Human), a unified framework addressing both challenges. Our approach introduces an analytical heading angle decomposition technique that offers superior efficiency and extensibility compared to existing geometric methods. Additionally, we design a camera trajectory integration mechanism inspired by world models, providing an effective pathway for leveraging camera translation information beyond naive hard-decoding approaches. Through experiments on in-the-wild benchmarks, WATCH achieves state-of-the-art performance in end-to-end trajectory reconstruction. Our work demonstrates the effectiveness of jointly modeling camera-human motion relationships and offers new insights for addressing the long-standing challenge of camera translation integration in global human motion reconstruction. The code will be available publicly.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2509.04600.pdf' target='_blank'>https://arxiv.org/pdf/2509.04600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Ying, Zhongyuan Hu, Rui Zhang, Ronghui Li, Yu Lu, Zijiao Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04600">WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global human motion reconstruction from in-the-wild monocular videos is increasingly demanded across VR, graphics, and robotics applications, yet requires accurate mapping of human poses from camera to world coordinates-a task challenged by depth ambiguity, motion ambiguity, and the entanglement between camera and human movements. While human-motion-centric approaches excel in preserving motion details and physical plausibility, they suffer from two critical limitations: insufficient exploitation of camera orientation information and ineffective integration of camera translation cues. We present WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and Human), a unified framework addressing both challenges. Our approach introduces an analytical heading angle decomposition technique that offers superior efficiency and extensibility compared to existing geometric methods. Additionally, we design a camera trajectory integration mechanism inspired by world models, providing an effective pathway for leveraging camera translation information beyond naive hard-decoding approaches. Through experiments on in-the-wild benchmarks, WATCH achieves state-of-the-art performance in end-to-end trajectory reconstruction. Our work demonstrates the effectiveness of jointly modeling camera-human motion relationships and offers new insights for addressing the long-standing challenge of camera translation integration in global human motion reconstruction. The code will be available publicly.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2507.22776.pdf' target='_blank'>https://arxiv.org/pdf/2507.22776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim FlÃ¼hmann, Alceu Bissoto, Trung-Dung Hoang, Lisa M. Koch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22776">Label-free estimation of clinically relevant performance metrics under distribution shifts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performance monitoring is essential for safe clinical deployment of image classification models. However, because ground-truth labels are typically unavailable in the target dataset, direct assessment of real-world model performance is infeasible. State-of-the-art performance estimation methods address this by leveraging confidence scores to estimate the target accuracy. Despite being a promising direction, the established methods mainly estimate the model's accuracy and are rarely evaluated in a clinical domain, where strong class imbalances and dataset shifts are common. Our contributions are twofold: First, we introduce generalisations of existing performance prediction methods that directly estimate the full confusion matrix. Then, we benchmark their performance on chest x-ray data in real-world distribution shifts as well as simulated covariate and prevalence shifts. The proposed confusion matrix estimation methods reliably predicted clinically relevant counting metrics on medical images under distribution shifts. However, our simulated shift scenarios exposed important failure modes of current performance estimation techniques, calling for a better understanding of real-world deployment contexts when implementing these performance monitoring techniques for postmarket surveillance of medical AI models.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2507.07432.pdf' target='_blank'>https://arxiv.org/pdf/2507.07432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul M. Riechers, Thomas J. Elliott, Adam S. Shai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07432">Neural networks leverage nominally quantum and post-quantum representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that deep neural networks, including transformers and RNNs, pretrained as usual on next-token prediction, intrinsically discover and represent beliefs over 'quantum' and 'post-quantum' low-dimensional generative models of their training data -- as if performing iterative Bayesian updates over the latent state of this world model during inference as they observe more context. Notably, neural nets easily find these representation whereas there is no finite classical circuit that would do the job. The corresponding geometric relationships among neural activations induced by different input sequences are found to be largely independent of neural-network architecture. Each point in this geometry corresponds to a history-induced probability density over all possible futures, and the relative displacement of these points reflects the difference in mechanism and magnitude for how these distinct pasts affect the future.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2506.06981.pdf' target='_blank'>https://arxiv.org/pdf/2506.06981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riley Simmons-Edler, Ryan P. Badman, Felix Baastad Berg, Raymond Chua, John J. Vastola, Joshua Lunger, William Qian, Kanaka Rajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06981">Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the behavior of deep reinforcement learning (DRL) agents -- particularly as task and agent sophistication increase -- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging -- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics -- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -- analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics -- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential -- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2506.03173.pdf' target='_blank'>https://arxiv.org/pdf/2506.03173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyi Liu, Hao Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03173">FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical intelligence -- anticipating and shaping the world from partial, multisensory observations -- is critical for next-generation world models. We propose FOLIAGE, a physics-informed multimodal world model for unbounded accretive surface growth. In its Action-Perception loop, a unified context encoder maps images, mesh connectivity, and point clouds to a shared latent state. A physics-aware predictor, conditioned on physical control actions, advances this latent state in time to align with the target latent of the surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network (AGN) captures dynamic connectivity through Age Positional Encoding and Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances global context with local dynamics. We create SURF-GARDEN, a world model learning platform comprising a Counterfactual Physics Simulator, a Multimodal Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation suite, evaluates six core tasks -- topology recognition, inverse material estimation, growth-stage classification, latent roll-out, cross-modal retrieval, and dense correspondence -- and four stress tests -- sensor dropout, zero-shot modality transfer, long-horizon prediction, and physics ablation -- to probe resilience. FOLIAGE outperforms specialized baselines while remaining robust across dynamic environments, establishing a new world-model based, multimodal pathway to physical intelligence.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2506.02459.pdf' target='_blank'>https://arxiv.org/pdf/2506.02459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin JJ. Bucher, Iro Armeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02459">ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. In contrast, LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture') but do not support editing, remain limited to rectangular layouts or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a novel voxelization-based evaluation that captures fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on object addition while maintaining competitive results on full scene synthesis.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2506.02459.pdf' target='_blank'>https://arxiv.org/pdf/2506.02459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin JJ. Bucher, Iro Armeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02459">ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture'), but lack editing functionality, are limited to rectangular layouts, or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that enables asset-agnostic deployment and frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a voxelization-based evaluation capturing fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on addition and achieve superior human-perceived quality on full scene synthesis.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2506.02459.pdf' target='_blank'>https://arxiv.org/pdf/2506.02459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin JJ. Bucher, Iro Armeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02459">ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture'), but lack editing functionality, are limited to rectangular layouts, or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that enables asset-agnostic deployment and frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a voxelization-based evaluation capturing fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on addition and achieve superior human-perceived quality on full scene synthesis.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2505.19867.pdf' target='_blank'>https://arxiv.org/pdf/2505.19867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19867">Deep Active Inference Agents for Delayed and Long-Horizon Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the recent success of world-model agents, which extend the core idea of model-based reinforcement learning by learning a differentiable model for sample-efficient control across diverse tasks, active inference (AIF) offers a complementary, neuroscience-grounded paradigm that unifies perception, learning, and action within a single probabilistic framework powered by a generative model. Despite this promise, practical AIF agents still rely on accurate immediate predictions and exhaustive planning, a limitation that is exacerbated in delayed environments requiring plans over long horizons, tens to hundreds of steps. Moreover, most existing agents are evaluated on robotic or vision benchmarks which, while natural for biological agents, fall short of real-world industrial complexity. We address these limitations with a generative-policy architecture featuring (i) a multi-step latent transition that lets the generative model predict an entire horizon in a single look-ahead, (ii) an integrated policy network that enables the transition and receives gradients of the expected free energy, (iii) an alternating optimization scheme that updates model and policy from a replay buffer, and (iv) a single gradient step that plans over long horizons, eliminating exhaustive planning from the control loop. We evaluate our agent in an environment that mimics a realistic industrial scenario with delayed and long-horizon settings. The empirical results confirm the effectiveness of the proposed approach, demonstrating the coupled world-model with the AIF formalism yields an end-to-end probabilistic controller capable of effective decision making in delayed, long-horizon settings without handcrafted rewards or expensive planning.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2505.19860.pdf' target='_blank'>https://arxiv.org/pdf/2505.19860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Roman Gansch, Lina Putze, Tjark Koopmann, Jan Reich, Christian Neurohr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19860">Causal Bayesian Networks for Data-driven Safety Analysis of Complex Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safe operation of safety-critical complex systems interacting with their environment poses significant challenges, particularly when the system's world model relies on machine learning algorithms to process the perception input. A comprehensive safety argumentation requires knowledge of how faults or functional insufficiencies propagate through the system and interact with external factors, to manage their safety impact. While statistical analysis approaches can support the safety assessment, associative reasoning alone is neither sufficient for the safety argumentation nor for the identification and investigation of safety measures. A causal understanding of the system and its interaction with the environment is crucial for safeguarding safety-critical complex systems. It allows to transfer and generalize knowledge, such as insights gained from testing, and facilitates the identification of potential improvements. This work explores using causal Bayesian networks to model the system's causalities for safety analysis, and proposes measures to assess causal influences based on Pearl's framework of causal inference. We compare the approach of causal Bayesian networks to the well-established fault tree analysis, outlining advantages and limitations. In particular, we examine importance metrics typically employed in fault tree analysis as foundation to discuss suitable causal metrics. An evaluation is performed on the example of a perception system for automated driving. Overall, this work presents an approach for causal reasoning in safety analysis that enables the integration of data-driven and expert-based knowledge to account for uncertainties arising from complex systems operating in open environments.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2505.18650.pdf' target='_blank'>https://arxiv.org/pdf/2505.18650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Wang, Peixi Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18650">ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world driving requires people to observe the current environment, anticipate the future, and make appropriate driving decisions. This requirement is aligned well with the capabilities of world models, which understand the environment and predict the future. However, recent world models in autonomous driving are built explicitly, where they could predict the future by controllable driving video generation. We argue that driving world models should have two additional abilities: action control and action prediction. Following this line, previous methods are limited because they predict the video requires given actions of the same length as the video and ignore the dynamical action laws. To address these issues, we propose ProphetDWM, a novel end-to-end driving world model that jointly predicts future videos and actions. Our world model has an action module to learn latent action from the present to the future period by giving the action sequence and observations. And a diffusion-model-based transition module to learn the state distribution. The model is jointly trained by learning latent actions given finite states and predicting action and video. The joint learning connects the action dynamics and states and enables long-term future prediction. We evaluate our method in video generation and action prediction tasks on the Nuscenes dataset. Compared to the state-of-the-art methods, our method achieves the best video consistency and best action prediction accuracy, while also enabling high-quality long-term video and action generation.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2505.13709.pdf' target='_blank'>https://arxiv.org/pdf/2505.13709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Aravind Venugopal, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13709">Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2505.02074.pdf' target='_blank'>https://arxiv.org/pdf/2505.02074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Petri, Luigi Asprino, Aldo Gangemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02074">Learning Local Causal World Models with State Space Models and Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2505.01479.pdf' target='_blank'>https://arxiv.org/pdf/2505.01479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Xiong, Zhangding Liu, Jieyu Zhou, Yusen Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01479">Deliberate Planning in Language Models with Symbolic Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2505.01479.pdf' target='_blank'>https://arxiv.org/pdf/2505.01479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Xiong, Zhangding Liu, Jieyu Zhou, Yusen Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01479">Deliberate Planning in Language Models with Symbolic Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning remains a core challenge for large language models (LLMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LLMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. Conceptually, SymPlanner operationalizes two cognitive faculties: (i) error monitoring and repair via externalized feedback (IC) and (ii) preference formation among alternatives via pairwise comparison (CR), advancing cognitively plausible, symbol-grounded planning aligned with the rich structure in intelligent systems. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2505.01479.pdf' target='_blank'>https://arxiv.org/pdf/2505.01479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Xiong, Zhangding Liu, Jieyu Zhou, Yusen Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01479">Deliberate Planning in Language Models with Symbolic Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning remains a core challenge for large language models (LLMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LLMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. Conceptually, SymPlanner operationalizes two cognitive faculties: (i) error monitoring and repair via externalized feedback (IC) and (ii) preference formation among alternatives via pairwise comparison (CR), advancing cognitively plausible, symbol-grounded planning aligned with the rich structure in intelligent systems. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2504.02918.pdf' target='_blank'>https://arxiv.org/pdf/2504.02918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck W. E. Prinzhorn, Mark Bodracska, Nicu Sebe, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02918">Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in image and video generation raise hopes that these models possess world modeling capabilities, the ability to generate realistic, physically plausible videos. This could revolutionize applications in robotics, autonomous driving, and scientific simulation. However, before treating these models as world models, we must ask: Do they adhere to physical conservation laws? To answer this, we introduce Morpheus, a benchmark for evaluating video generation models on physical reasoning. It features 80 real-world videos capturing physical phenomena, guided by conservation laws. Since artificial generations lack ground truth, we assess physical plausibility using physics-informed metrics evaluated with respect to infallible conservation laws known per physical setting, leveraging advances in physics-informed neural networks and vision-language foundation models. Our findings reveal that even with advanced prompting and video conditioning, current models struggle to encode physical principles despite generating aesthetically pleasing videos. All data, leaderboard, and code are open-sourced at our project page.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2503.20523.pdf' target='_blank'>https://arxiv.org/pdf/2503.20523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, Gianluca Corrado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20523">GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models offer a scalable and flexible paradigm for simulating complex environments, yet current approaches fall short in addressing the domain-specific requirements of autonomous driving - such as multi-agent interactions, fine-grained control, and multi-camera consistency. We introduce GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies these capabilities within a single generative framework. GAIA-2 supports controllable video generation conditioned on a rich set of structured inputs: ego-vehicle dynamics, agent configurations, environmental factors, and road semantics. It generates high-resolution, spatiotemporally consistent multi-camera videos across geographically diverse driving environments (UK, US, Germany). The model integrates both structured conditioning and external latent embeddings (e.g., from a proprietary driving model) to facilitate flexible and semantically grounded scene synthesis. Through this integration, GAIA-2 enables scalable simulation of both common and rare driving scenarios, advancing the use of generative world models as a core tool in the development of autonomous systems. Videos are available at https://wayve.ai/thinking/gaia-2.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2503.20124.pdf' target='_blank'>https://arxiv.org/pdf/2503.20124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zergham Ahmed, Joshua B. Tenenbaum, Christopher J. Bates, Samuel J. Gershman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20124">Synthesizing world models for bilevel planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern reinforcement learning (RL) systems have demonstrated remarkable capabilities in complex environments, such as video games. However, they still fall short of achieving human-like sample efficiency and adaptability when learning new domains. Theory-based reinforcement learning (TBRL) is an algorithmic framework specifically designed to address this gap. Modeled on cognitive theories, TBRL leverages structured, causal world models - "theories" - as forward simulators for use in planning, generalization and exploration. Although current TBRL systems provide compelling explanations of how humans learn to play video games, they face several technical limitations: their theory languages are restrictive, and their planning algorithms are not scalable. To address these challenges, we introduce TheoryCoder, an instantiation of TBRL that exploits hierarchical representations of theories and efficient program synthesis methods for more powerful learning and planning. TheoryCoder equips agents with general-purpose abstractions (e.g., "move to"), which are then grounded in a particular environment by learning a low-level transition model (a Python program synthesized from observations by a large language model). A bilevel planning algorithm can exploit this hierarchical structure to solve large domains. We demonstrate that this approach can be successfully applied to diverse and challenging grid-world games, where approaches based on directly synthesizing a policy perform poorly. Ablation studies demonstrate the benefits of using hierarchical abstractions.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2502.21142.pdf' target='_blank'>https://arxiv.org/pdf/2502.21142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LÃ©opold MaytiÃ©, Roland Bertin Johannet, Rufin VanRullen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.21142">Multimodal Dreaming: A Global Workspace Approach to World Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans leverage rich internal models of the world to reason about the future, imagine counterfactuals, and adapt flexibly to new situations. In Reinforcement Learning (RL), world models aim to capture how the environment evolves in response to the agent's actions, facilitating planning and generalization. However, typical world models directly operate on the environment variables (e.g. pixels, physical attributes), which can make their training slow and cumbersome; instead, it may be advantageous to rely on high-level latent dimensions that capture relevant multimodal variables. Global Workspace (GW) Theory offers a cognitive framework for multimodal integration and information broadcasting in the brain, and recent studies have begun to introduce efficient deep learning implementations of GW. Here, we evaluate the capabilities of an RL system combining GW with a world model. We compare our GW-Dreamer with various versions of the standard PPO and the original Dreamer algorithms. We show that performing the dreaming process (i.e., mental simulation) inside the GW latent space allows for training with fewer environment steps. As an additional emergent property, the resulting model (but not its comparison baselines) displays strong robustness to the absence of one of its observation modalities (images or simulation attributes). We conclude that the combination of GW with World Models holds great potential for improving decision-making in RL agents.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2501.06583.pdf' target='_blank'>https://arxiv.org/pdf/2501.06583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koji Aoshima, Eddie Wadbro, Martin Servin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06583">Optimizing wheel loader performance -- an end-to-end approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. Automating this task presents a challenging planning problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2412.11867.pdf' target='_blank'>https://arxiv.org/pdf/2412.11867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex F. Spies, William Edwards, Michael I. Ivanitskiy, Adrians Skapars, Tilman RÃ¤uker, Katsumi Inoue, Alessandra Russo, Murray Shanahan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11867">Transformers Use Causal World Models in Maze-Solving Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies in interpretability have explored the inner workings of transformer models trained on tasks across various domains, often discovering that these networks naturally develop highly structured representations. When such representations comprehensively reflect the task domain's structure, they are commonly referred to as "World Models" (WMs). In this work, we identify WMs in transformers trained on maze-solving tasks. By using Sparse Autoencoders (SAEs) and analyzing attention patterns, we examine the construction of WMs and demonstrate consistency between SAE feature-based and circuit-based analyses. By subsequently intervening on isolated features to confirm their causal role, we find that it is easier to activate features than to suppress them. Furthermore, we find that models can reason about mazes involving more simultaneously active features than they encountered during training; however, when these same mazes (with greater numbers of connections) are provided to models via input tokens instead, the models fail. Finally, we demonstrate that positional encoding schemes appear to influence how World Models are structured within the model's residual stream.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2412.05766.pdf' target='_blank'>https://arxiv.org/pdf/2412.05766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miles Hutson, Isaac Kauvar, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05766">Policy-shaped prediction: avoiding distractions in model-based reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods -- including DreamerV3 and DreamerPro -- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2412.05337.pdf' target='_blank'>https://arxiv.org/pdf/2412.05337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hidehisa Arai, Keishi Ishihara, Tsubasa Takahashi, Yu Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05337">ACT-Bench: Towards Action Controllable World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have emerged as promising neural simulators for autonomous driving, with the potential to supplement scarce real-world data and enable closed-loop evaluations. However, current research primarily evaluates these models based on visual realism or downstream task performance, with limited focus on fidelity to specific action instructions - a crucial property for generating targeted simulation scenes. Although some studies address action fidelity, their evaluations rely on closed-source mechanisms, limiting reproducibility. To address this gap, we develop an open-access evaluation framework, ACT-Bench, for quantifying action fidelity, along with a baseline world model, Terra. Our benchmarking framework includes a large-scale dataset pairing short context videos from nuScenes with corresponding future trajectory data, which provides conditional input for generating future video frames and enables evaluation of action fidelity for executed motions. Furthermore, Terra is trained on multiple large-scale trajectory-annotated datasets to enhance action fidelity. Leveraging this framework, we demonstrate that the state-of-the-art model does not fully adhere to given instructions, while Terra achieves improved action fidelity. All components of our benchmark framework will be made publicly available to support future research.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2411.06890.pdf' target='_blank'>https://arxiv.org/pdf/2411.06890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anson Lei, Bernhard SchÃ¶lkopf, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06890">SPARTAN: A Sparse Transformer Learning Local Causation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal structures play a central role in world models that flexibly adapt to changes in the environment. While recent works motivate the benefits of discovering local causal graphs for dynamics modelling, in this work we demonstrate that accurately capturing these relationships in complex settings remains challenging for the current state-of-the-art. To remedy this shortcoming, we postulate that sparsity is a critical ingredient for the discovery of such local causal structures. To this end we present the SPARse TrANsformer World model (SPARTAN), a Transformer-based world model that learns local causal structures between entities in a scene. By applying sparsity regularisation on the attention pattern between object-factored tokens, SPARTAN identifies sparse local causal models that accurately predict future object states. Furthermore, we extend our model to capture sparse interventions with unknown targets on the dynamics of the environment. This results in a highly interpretable world model that can efficiently adapt to changes. Empirically, we evaluate SPARTAN against the current state-of-the-art in object-centric world models on observation-based environments and demonstrate that our model can learn accurate local causal graphs and achieve significantly improved few-shot adaptation to changes in the dynamics of the environment as well as robustness against removing irrelevant distractors.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2411.04983.pdf' target='_blank'>https://arxiv.org/pdf/2411.04983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoyue Zhou, Hengkai Pan, Yann LeCun, Lerrel Pinto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04983">DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remains challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2410.22459.pdf' target='_blank'>https://arxiv.org/pdf/2410.22459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Chung, Scott Niekum, David Krueger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22459">Predicting Future Actions of Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2410.16028.pdf' target='_blank'>https://arxiv.org/pdf/2410.16028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Crulis, Barthelemy Serres, Cyril De Runz, Gilles Venturini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16028">Few-shot target-driven instance detection based on open-vocabulary object detection models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current large open vision models could be useful for one and few-shot object recognition. Nevertheless, gradient-based re-training solutions are costly. On the other hand, open-vocabulary object detection models bring closer visual and textual concepts in the same latent space, allowing zero-shot detection via prompting at small computational cost. We propose a lightweight method to turn the latter into a one-shot or few-shot object recognition models without requiring textual descriptions. Our experiments on the TEgO dataset using the YOLO-World model as a base show that performance increases with the model size, the number of examples and the use of image augmentation.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2409.12278.pdf' target='_blank'>https://arxiv.org/pdf/2409.12278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaige Xie, Ian Yang, John Gunerli, Mark Riedl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12278">Making Large Language Models into World Models with Precondition and Effect Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models, which encapsulate the dynamics of how actions affect environments, are foundational to the functioning of intelligent agents. In this work, we explore the potential of Large Language Models (LLMs) to operate as world models. Although LLMs are not inherently designed to model real-world dynamics, we show that they can be induced to perform two critical world model functions: determining the applicability of an action based on a given world state, and predicting the resulting world state upon action execution. This is achieved by fine-tuning two separate LLMs-one for precondition prediction and another for effect prediction-while leveraging synthetic data generation techniques. Through human-participant studies, we validate that the precondition and effect knowledge generated by our models aligns with human understanding of world dynamics. We also analyze the extent to which the world model trained on our synthetic data results in an inferred state space that supports the creation of action chains, a necessary property for planning.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2407.02466.pdf' target='_blank'>https://arxiv.org/pdf/2407.02466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02466">PWM: Policy Learning with Multi-Task World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) has made significant strides in complex tasks but struggles in multi-task settings with different embodiments. World model methods offer scalability by learning a simulation of the environment but often rely on inefficient gradient-free optimization methods for policy extraction. In contrast, gradient-based methods exhibit lower variance but fail to handle discontinuities. Our work reveals that well-regularized world models can generate smoother optimization landscapes than the actual dynamics, facilitating more effective first-order optimization. We introduce Policy learning with multi-task World Models (PWM), a novel model-based RL algorithm for continuous control. Initially, the world model is pre-trained on offline data, and then policies are extracted from it using first-order optimization in less than 10 minutes per task. PWM effectively solves tasks with up to 152 action dimensions and outperforms methods that use ground-truth dynamics. Additionally, PWM scales to an 80-task setting, achieving up to 27% higher rewards than existing baselines without relying on costly online planning. Visualizations and code are available at https://www.imgeorgiev.com/pwm/.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2407.00889.pdf' target='_blank'>https://arxiv.org/pdf/2407.00889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cora A. Dimmig, Marin Kobilarov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00889">Non-Prehensile Aerial Manipulation using Model-Based Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the continual adoption of Uncrewed Aerial Vehicles (UAVs) across a wide-variety of application spaces, robust aerial manipulation remains a key research challenge. Aerial manipulation tasks require interacting with objects in the environment, often without knowing their dynamical properties like mass and friction a priori. Additionally, interacting with these objects can have a significant impact on the control and stability of the vehicle. We investigated an approach for robust control and non-prehensile aerial manipulation in unknown environments. In particular, we use model-based Deep Reinforcement Learning (DRL) to learn a world model of the environment while simultaneously learning a policy for interaction with the environment. We evaluated our approach on a series of push tasks by moving an object between goal locations and demonstrated repeatable behaviors across a range of friction values.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2406.19501.pdf' target='_blank'>https://arxiv.org/pdf/2406.19501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahai Feng, Stuart Russell, Jacob Steinhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19501">Monitoring Latent World States in Language Models with Propositional Probes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of language models could help monitor and correct unfaithful behavior. We hypothesize that language models represent their input contexts in a latent world model, and seek to extract this latent world state from the activations. We do so with 'propositional probes', which compositionally probe tokens for lexical information and bind them into logical propositions representing the world state. For example, given the input context ''Greg is a nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg, nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to this is identifying a 'binding subspace' in which bound tokens have high similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and ''physicist''). We validate propositional probes in a closed-world setting with finitely many predicates and properties. Despite being trained on simple templated contexts, propositional probes generalize to contexts rewritten as short stories and translated to Spanish. Moreover, we find that in three settings where language models respond unfaithfully to the input context -- prompt injections, backdoor attacks, and gender bias -- the decoded propositions remain faithful. This suggests that language models often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2405.15616.pdf' target='_blank'>https://arxiv.org/pdf/2405.15616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Giacomo Indiveri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15616">Neuromorphic dreaming: A pathway to efficient learning in artificial agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving energy efficiency in learning is a key challenge for artificial intelligence (AI) computing platforms. Biological systems demonstrate remarkable abilities to learn complex skills quickly and efficiently. Inspired by this, we present a hardware implementation of model-based reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal analog/digital neuromorphic hardware. This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the "awake" phase, and offline learning, known as the "dreaming" phase. The model proposed includes two symbiotic networks: an agent network that learns by combining real and simulated experiences, and a learned world model network that generates the simulated experiences. We validate the model by training the hardware implementation to play the Atari game Pong. We start from a baseline consisting of an agent network learning without a world model and dreaming, which successfully learns to play the game. By incorporating dreaming, the number of required real game experiences are reduced significantly compared to the baseline. The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed. These results pave the way toward energy-efficient neuromorphic learning systems capable of rapid learning in real world applications and use-cases.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2404.15109.pdf' target='_blank'>https://arxiv.org/pdf/2404.15109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anson Lei, Frederik Nolte, Bernhard SchÃ¶lkopf, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15109">Compete and Compose: Learning Independent Mechanisms for Modular World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modular world model which leverages reusable, independent mechanisms across different environments. COMET is trained on multiple environments with varying dynamics via a two-step process: competition and composition. This enables the model to recognise and learn transferable mechanisms. Specifically, in the competition phase, COMET is trained with a winner-takes-all gradient allocation, encouraging the emergence of independent mechanisms. These are then re-used in the composition phase, where COMET learns to re-compose learnt mechanisms in ways that capture the dynamics of intervened environments. In so doing, COMET explicitly reuses prior knowledge, enabling efficient and interpretable adaptation. We evaluate COMET on environments with image-based observations. In contrast to competitive baselines, we demonstrate that COMET captures recognisable mechanisms without supervision. Moreover, we show that COMET is able to adapt to new environments with varying numbers of objects with improved sample efficiency compared to more conventional finetuning approaches.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2404.02903.pdf' target='_blank'>https://arxiv.org/pdf/2404.02903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vlas Zyrianov, Henry Che, Zhijian Liu, Shenlong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02903">LidarDM: Generative LiDAR Simulation in a Generated World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present LidarDM, a novel LiDAR generative model capable of producing realistic, layout-aware, physically plausible, and temporally coherent LiDAR videos. LidarDM stands out with two unprecedented capabilities in LiDAR generative modeling: (i) LiDAR generation guided by driving scenarios, offering significant potential for autonomous driving simulations, and (ii) 4D LiDAR point cloud generation, enabling the creation of realistic and temporally coherent sequences. At the heart of our model is a novel integrated 4D world generation framework. Specifically, we employ latent diffusion models to generate the 3D scene, combine it with dynamic actors to form the underlying 4D world, and subsequently produce realistic sensory observations within this virtual environment. Our experiments indicate that our approach outperforms competing algorithms in realism, temporal coherency, and layout consistency. We additionally show that LidarDM can be used as a generative world model simulator for training and testing perception models.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2403.00807.pdf' target='_blank'>https://arxiv.org/pdf/2403.00807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhe Ni, Jiang Wu, Hongbo Wang, Wenran Lu, Chenwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00807">Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are a class of generative AI models built using the Transformer network, capable of leveraging vast datasets to identify, summarize, translate, predict, and generate language. LLMs promise to revolutionize society, yet training these foundational models poses immense challenges. Semantic vector search within large language models is a potent technique that can significantly enhance search result accuracy and relevance. Unlike traditional keyword-based search methods, semantic search utilizes the meaning and context of words to grasp the intent behind queries and deliver more precise outcomes. Elasticsearch emerges as one of the most popular tools for implementing semantic search an exceptionally scalable and robust search engine designed for indexing and searching extensive datasets. In this article, we delve into the fundamentals of semantic search and explore how to harness Elasticsearch and Transformer models to bolster large language model processing paradigms. We gain a comprehensive understanding of semantic search principles and acquire practical skills for implementing semantic search in real-world model application scenarios.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2402.18174.pdf' target='_blank'>https://arxiv.org/pdf/2402.18174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koen de Vos, Gijs van den Brandt, Jordy Senden, Pieter Pauwels, Rene van de Molengraft, Elena Torta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18174">Generation of skill-specific maps from graph world models for robotic systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increase in the availability of Building Information Models (BIM) and (semi-) automatic tools to generate BIM from point clouds, we propose a world model architecture and algorithms to allow the use of the semantic and geometric knowledge encoded within these models to generate maps for robot localization and navigation. When heterogeneous robots are deployed within an environment, maps obtained from classical SLAM approaches might not be shared between all agents within a team of robots, e.g. due to a mismatch in sensor type, or a difference in physical robot dimensions. Our approach extracts the 3D geometry and semantic description of building elements (e.g. material, element type, color) from BIM, and represents this knowledge in a graph. Based on queries on the graph and knowledge of the skills of the robot, we can generate skill-specific maps that can be used during the execution of localization or navigation tasks. The approach is validated with data from complex build environments and integrated into existing navigation frameworks.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2401.16650.pdf' target='_blank'>https://arxiv.org/pdf/2401.16650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Yang, Levin Kuhlmann, Gideon Kowadlo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16650">Augmenting Replay in World Models for Continual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual RL requires an agent to learn new tasks without forgetting previous ones, while improving on both past and future tasks. The most common approaches use model-free algorithms and replay buffers can help to mitigate catastrophic forgetting, but often struggle with scalability due to large memory requirements. Biologically inspired replay suggests replay to a world model, aligning with model-based RL; as opposed to the common setting of replay in model-free algorithms. Model-based RL offers benefits for continual RL by leveraging knowledge of the environment, independent of policy. We introduce WMAR (World Models with Augmented Replay), a model-based RL algorithm with a memory-efficient distribution-matching replay buffer. WMAR extends the well known DreamerV3 algorithm, which employs a simple FIFO buffer and was not tested in continual RL. We evaluated WMAR and DreamerV3, with the same-size replay buffers. They were tested on two scenarios: tasks with shared structure using OpenAI Procgen and tasks without shared structure using the Atari benchmark. WMAR demonstrated favourable properties for continual RL considering metrics for forgetting as well as skill transfer on past and future tasks. Compared to DreamerV3, WMAR showed slight benefits in tasks with shared structure and substantially better forgetting characteristics on tasks without shared structure. Our results suggest that model-based RL with a memory-efficient replay buffer can be an effective approach to continual RL, justifying further research.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2401.12917.pdf' target='_blank'>https://arxiv.org/pdf/2401.12917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lancelot Da Costa, Samuel Tenka, Dominic Zhao, Noor Sajid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12917">Active Inference as a Model of Agency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness of active inference for RL is three-fold. \emph{a}) Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency. \emph{b}) It provides an explainable recipe to simulate behaviour, whence behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit in differences in world model. \emph{c}) This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm. Thus, active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2401.11660.pdf' target='_blank'>https://arxiv.org/pdf/2401.11660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dixant Mittal, Wee Sun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11660">Differentiable Tree Search Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learned world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic inductive biases into the neural network architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. D-TSN employs a learned world model to conduct a fully differentiable online search. The world model is jointly optimized with the search algorithm, enabling the learning of a robust world model and mitigating the effect of prediction inaccuracies. Further, we note that a naive incorporation of best-first search could lead to a discontinuous loss function in the parameter space. We address this issue by adopting a stochastic tree expansion policy, formulating search tree expansion as another decision-making task, and introducing an effective variance reduction technique for the gradient computation. We evaluate D-TSN in an offline-RL setting with a limited training data scenario on Procgen games and grid navigation task, and demonstrate that D-TSN outperforms popular model-free and model-based baselines.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2311.12144.pdf' target='_blank'>https://arxiv.org/pdf/2311.12144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Huang, Yue Chen, Zhu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12144">Applications of Large Scale Foundation Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007, autonomous driving has been the most active field of AI applications. Recently powered by large language models (LLMs), chat systems, such as chatGPT and PaLM, emerge and rapidly become a promising direction to achieve artificial general intelligence (AGI) in natural language processing (NLP). There comes a natural thinking that we could employ these abilities to reformulate autonomous driving. By combining LLM with foundation models, it is possible to utilize the human knowledge, commonsense and reasoning to rebuild autonomous driving systems from the current long-tailed AI dilemma. In this paper, we investigate the techniques of foundation models and LLMs applied for autonomous driving, categorized as simulation, world model, data annotation and planning or E2E solutions etc.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2310.02207.pdf' target='_blank'>https://arxiv.org/pdf/2310.02207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wes Gurnee, Max Tegmark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02207">Language Models Represent Space and Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2309.17080.pdf' target='_blank'>https://arxiv.org/pdf/2309.17080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17080">GAIA-1: A Generative World Model for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves.
  To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2309.14236.pdf' target='_blank'>https://arxiv.org/pdf/2309.14236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, Vikash Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14236">MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills directly in the real world. We identify key ingredients for leveraging demonstrations in model learning while respecting real-world safety considerations -- exploration centering, agency handover, and actor-critic ensembles. We empirically demonstrate the contribution of these ingredients in four complex visuo-motor manipulation problems in both simulation and the real world. To the best of our knowledge, our work presents the first successful system for demonstration-augmented visual MBRL trained directly in the real world. Visit https://sites.google.com/view/modem-v2 for videos and more details.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2309.12016.pdf' target='_blank'>https://arxiv.org/pdf/2309.12016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koji Aoshima, Arvid FÃ¤lldin, Eddie Wadbro, Martin Servin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12016">World Modeling for Autonomous Wheel Loaders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method for learning world models for wheel loaders performing automatic loading actions on a pile of soil. Data-driven models were learned to output the resulting pile state, loaded mass, time, and work for a single loading cycle given inputs that include a heightmap of the initial pile shape and action parameters for an automatic bucket-filling controller. Long-horizon planning of sequential loading in a dynamically changing environment is thus enabled as repeated model inference. The models, consisting of deep neural networks, were trained on data from 3D multibody dynamics simulation of over 10,000 random loading actions in gravel piles of different shapes. The accuracy and inference time for predicting the loading performance and the resulting pile state were, on average, 95% in 1.2 ms and 97% in 4.5 ms, respectively. Long-horizon predictions were found feasible over 40 sequential loading actions.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the complicated environment dynamics. Our model produces imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. Experimental results on StarCraft II micro-management, Multi-Agent MuJoCo, and Level-Based Foraging challenges demonstrate that our method achieves high sample efficiency and exhibits superior performance compared to other baselines across a wide range of multi-agent learning tasks.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the complicated environment dynamics. Our model produces imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. Experimental results on StarCraft II micro-management, Multi-Agent MuJoCo, and Level-Based Foraging challenges demonstrate that our method achieves high sample efficiency and exhibits superior performance compared to other baselines across a wide range of multi-agent learning tasks.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2308.13661.pdf' target='_blank'>https://arxiv.org/pdf/2308.13661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Fu, Run Peng, Honglak Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13661">Go Beyond Imagination: Maximizing Episodic Reachability with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on locomotion tasks from DeepMind Control Suite.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2308.06498.pdf' target='_blank'>https://arxiv.org/pdf/2308.06498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiqi Chen, Jing Yu Lim, Kingsley Kuan, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06498">Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and beliefs on three partially-observable HRI tasks. Experiments show that our method significantly outperforms existing baselines and is able to infer visual observations available to other agent and their internal beliefs.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2307.14993.pdf' target='_blank'>https://arxiv.org/pdf/2307.14993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Chung, Ivan Anokhin, David Krueger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14993">Thinker: Learning to Plan and Act</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2306.09205.pdf' target='_blank'>https://arxiv.org/pdf/2306.09205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Rigter, Minqi Jiang, Ingmar Posner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09205">Reward-Free Curricula for Training Robust World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2306.04347.pdf' target='_blank'>https://arxiv.org/pdf/2306.04347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04347">World Models for Math Story Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2305.14078.pdf' target='_blank'>https://arxiv.org/pdf/2305.14078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Zhao, Wee Sun Lee, David Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14078">Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin, for complex, novel tasks. Further experiments and analyses on multiple tasks -- multiplication, multi-hop travel planning, object rearrangement -- suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2305.11358.pdf' target='_blank'>https://arxiv.org/pdf/2305.11358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Rios, Nicanor Quijano, Luis Felipe Giraldo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11358">Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social dilemmas are situations where groups of individuals can benefit from mutual cooperation but conflicting interests impede them from doing so. This type of situations resembles many of humanity's most critical challenges, and discovering mechanisms that facilitate the emergence of cooperative behaviors is still an open problem. In this paper, we study the behavior of self-interested rational agents that learn world models in a multi-agent reinforcement learning (RL) setting and that coexist in environments where social dilemmas can arise. Our simulation results show that groups of agents endowed with world models outperform all the other tested ones when dealing with scenarios where social dilemmas can arise. We exploit the world model architecture to qualitatively assess the learnt dynamics and confirm that each agent's world model is capable to encode information of the behavior of the changing environment and the other agent's actions. This is the first work that shows that world models facilitate the emergence of complex coordinated behaviors that enable interacting agents to ``understand'' both environmental and social dynamics.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2301.04104.pdf' target='_blank'>https://arxiv.org/pdf/2301.04104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04104">Mastering Diverse Domains through World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2510.04391.pdf' target='_blank'>https://arxiv.org/pdf/2510.04391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Ranjan, Brian Odegaard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04391">Internal World Models as Imagination Networks in Cognitive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2510.04391.pdf' target='_blank'>https://arxiv.org/pdf/2510.04391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saurabh Ranjan, Brian Odegaard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04391">Internal World Models as Imagination Networks in Cognitive Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2509.24559.pdf' target='_blank'>https://arxiv.org/pdf/2509.24559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Molinari, Leonardo Nevali, Saharsha Navani, Omar G. Younis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24559">Emergent World Representations in OpenVLA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action models (VLAs) trained with policy-based reinforcement learning (RL) encode complex behaviors without explicitly modeling environmental dynamics. However, it remains unclear whether VLAs implicitly learn world models, a hallmark of model-based RL. We propose an experimental methodology using embedding arithmetic on state representations to probe whether OpenVLA, the current state of the art in VLAs, contains latent knowledge of state transitions. Specifically, we measure the difference between embeddings of sequential environment states and test whether this transition vector is recoverable from intermediate model activations. Using linear and non linear probes trained on activations across layers, we find statistically significant predictive ability on state transitions exceeding baselines (embeddings), indicating that OpenVLA encodes an internal world model (as opposed to the probes learning the state transitions). We investigate the predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that the world model emerges as training progresses. Finally, we outline a pipeline leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2509.24559.pdf' target='_blank'>https://arxiv.org/pdf/2509.24559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Molinari, Leonardo Nevali, Saharsha Navani, Omar G. Younis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24559">Emergent World Representations in OpenVLA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision Language Action models (VLAs) trained with policy-based reinforcement learning (RL) encode complex behaviors without explicitly modeling environmental dynamics. However, it remains unclear whether VLAs implicitly learn world models, a hallmark of model-based RL. We propose an experimental methodology using embedding arithmetic on state representations to probe whether OpenVLA, the current state of the art in VLAs, contains latent knowledge of state transitions. Specifically, we measure the difference between embeddings of sequential environment states and test whether this transition vector is recoverable from intermediate model activations. Using linear and non linear probes trained on activations across layers, we find statistically significant predictive ability on state transitions exceeding baselines (embeddings), indicating that OpenVLA encodes an internal world model (as opposed to the probes learning the state transitions). We investigate the predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that the world model emerges as training progresses. Finally, we outline a pipeline leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2509.20998.pdf' target='_blank'>https://arxiv.org/pdf/2509.20998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panagiotis Michelakis, Yiannis Hadjiyiannis, Dimitrios Stamoulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20998">CORE: Full-Path Evaluation of LLM Agents Beyond Final State</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2509.20998.pdf' target='_blank'>https://arxiv.org/pdf/2509.20998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Panagiotis Michelakis, Yiannis Hadjiyiannis, Dimitrios Stamoulis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20998">CORE: Full-Path Evaluation of LLM Agents Beyond Final State</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating AI agents that solve real-world tasks through function-call sequences remains an open challenge. Existing agentic benchmarks often reduce evaluation to a binary judgment of the final state, overlooking critical aspects such as safety, efficiency, and intermediate correctness. We propose a framework based on deterministic finite automata (DFAs) that encodes tasks as sets of valid tool-use paths, enabling principled assessment of agent behavior in diverse world models. Building on this foundation, we introduce CORE, a suite of five metrics, namely Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that quantify alignment with expected execution patterns. Across diverse worlds, our method reveals important performance differences between agents that would otherwise appear equivalent under traditional final-state evaluation schemes.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2509.13389.pdf' target='_blank'>https://arxiv.org/pdf/2509.13389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos NÃºÃ±ez-Molina, VicenÃ§ GÃ³mez, Hector Geffner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13389">From Next Token Prediction to (STRIPS) World Models -- Preliminary Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of learning propositional STRIPS world models from action traces alone, using a deep learning architecture (transformers) and gradient descent. The task is cast as a supervised next token prediction problem where the tokens are the actions, and an action $a$ may follow an action sequence if the hidden effects of the previous actions do not make an action precondition of $a$ false. We show that a suitable transformer architecture can faithfully represent propositional STRIPS world models, and that the models can be learned from sets of random valid (positive) and invalid (negative) action sequences alone. A number of experiments are reported.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2509.13389.pdf' target='_blank'>https://arxiv.org/pdf/2509.13389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos NÃºÃ±ez-Molina, VicenÃ§ GÃ³mez, Hector Geffner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13389">From Next Token Prediction to (STRIPS) World Models -- Preliminary Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of learning propositional STRIPS world models from action traces alone, using a deep learning architecture (transformers) and gradient descent. The task is cast as a supervised next token prediction problem where the tokens are the actions, and an action $a$ may follow an action sequence if the hidden effects of the previous actions do not make an action precondition of $a$ false. We show that a suitable transformer architecture can faithfully represent propositional STRIPS world models, and that the models can be learned from sets of random valid (positive) and invalid (negative) action sequences alone. A number of experiments are reported.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2509.05263.pdf' target='_blank'>https://arxiv.org/pdf/2509.05263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05263">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2509.05263.pdf' target='_blank'>https://arxiv.org/pdf/2509.05263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, Shuang Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05263">LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2509.03636.pdf' target='_blank'>https://arxiv.org/pdf/2509.03636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline Maasch, John Kalantari, Kia Khezeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03636">CausalARC: Abstract Reasoning with Causal World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning requires adaptation to novel problem settings under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2509.03636.pdf' target='_blank'>https://arxiv.org/pdf/2509.03636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacqueline Maasch, John Kalantari, Kia Khezeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03636">CausalARC: Abstract Reasoning with Causal World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning requires adaptation to novel problem settings under limited data and distribution shift. This work introduces CausalARC: an experimental testbed for AI reasoning in low-data and out-of-distribution regimes, modeled after the Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is sampled from a fully specified causal world model, formally expressed as a structural causal model. Principled data augmentations provide observational, interventional, and counterfactual feedback about the world model in the form of few-shot, in-context learning demonstrations. As a proof-of-concept, we illustrate the use of CausalARC for four language model evaluation settings: (1) abstract reasoning with test-time training, (2) counterfactual reasoning with in-context learning, (3) program synthesis, and (4) causal discovery with logical reasoning.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2509.03345.pdf' target='_blank'>https://arxiv.org/pdf/2509.03345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxin Sun, Abulhair Saparov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03345">Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMs' inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occam's Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2509.03345.pdf' target='_blank'>https://arxiv.org/pdf/2509.03345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxin Sun, Abulhair Saparov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03345">Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reasoning is a core capability in artificial intelligence systems, for which large language models (LLMs) have recently shown remarkable progress. However, most work focuses exclusively on deductive reasoning, which is problematic since other types of reasoning are also essential in solving real-world problems, and they are less explored. This work focuses on evaluating LLMs' inductive and abductive reasoning capabilities. We introduce a programmable and synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example consists of an incomplete world model and a set of observations. The task for the intelligent agent is to produce hypotheses to explain observations under the incomplete world model to solve each reasoning example. We propose a new metric to evaluate the quality of hypotheses based on Occam's Razor. We evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs can perform inductive and abductive reasoning in simple scenarios, but struggle with complex world models and producing high-quality hypotheses, even with popular reasoning-enhancing techniques such as in-context learning and RLVR.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2508.20294.pdf' target='_blank'>https://arxiv.org/pdf/2508.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank RÃ¶der, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20294">Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2508.15013.pdf' target='_blank'>https://arxiv.org/pdf/2508.15013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadav Amir, Stas Tiomkin, Angela Langdon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15013">Goals and the Structure of Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Purposeful behavior is a hallmark of natural and artificial intelligence. Its acquisition is often believed to rely on world models, comprising both descriptive (what is) and prescriptive (what is desirable) aspects that identify and evaluate state of affairs in the world, respectively. Canonical computational accounts of purposeful behavior, such as reinforcement learning, posit distinct components of a world model comprising a state representation (descriptive aspect) and a reward function (prescriptive aspect). However, an alternative possibility, which has not yet been computationally formulated, is that these two aspects instead co-emerge interdependently from an agent's goal. Here, we describe a computational framework of goal-directed state representation in cognitive agents, in which the descriptive and prescriptive aspects of a world model co-emerge from agent-environment interaction sequences, or experiences. Drawing on Buddhist epistemology, we introduce a construct of goal-directed, or telic, states, defined as classes of goal-equivalent experience distributions. Telic states provide a parsimonious account of goal-directed learning in terms of the statistical divergence between behavioral policies and desirable experience features. We review empirical and theoretical literature supporting this novel perspective and discuss its potential to provide a unified account of behavioral, phenomenological and neural dimensions of purposeful behaviors across diverse substrates.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2508.11836.pdf' target='_blank'>https://arxiv.org/pdf/2508.11836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dave Goel, Matthew Guzdial, Anurag Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11836">Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models are defined as a compressed spatial and temporal learned representation of an environment. The learned representation is typically a neural network, making transfer of the learned environment dynamics and explainability a challenge. In this paper, we propose an approach, Finite Automata Extraction (FAE), that learns a neuro-symbolic world model from gameplay video represented as programs in a novel domain-specific language (DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more precise model of the environment and more general code than prior DSL-based approaches.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2508.10399.pdf' target='_blank'>https://arxiv.org/pdf/2508.10399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, Ping Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10399">Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied AI aims to develop intelligent systems with physical forms capable of perceiving, decision-making, acting, and learning in real-world environments, providing a promising way to Artificial General Intelligence (AGI). Despite decades of explorations, it remains challenging for embodied agents to achieve human-level intelligence for general-purpose tasks in open dynamic environments. Recent breakthroughs in large models have revolutionized embodied AI by enhancing perception, interaction, planning and learning. In this article, we provide a comprehensive survey on large model empowered embodied AI, focusing on autonomous decision-making and embodied learning. We investigate both hierarchical and end-to-end decision-making paradigms, detailing how large models enhance high-level planning, low-level execution, and feedback for hierarchical decision-making, and how large models enhance Vision-Language-Action (VLA) models for end-to-end decision making. For embodied learning, we introduce mainstream learning methodologies, elaborating on how large models enhance imitation learning and reinforcement learning in-depth. For the first time, we integrate world models into the survey of embodied AI, presenting their design methods and critical roles in enhancing decision-making and learning. Though solid advances have been achieved, challenges still exist, which are discussed at the end of this survey, potentially as the further research directions.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2508.06096.pdf' target='_blank'>https://arxiv.org/pdf/2508.06096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Jing, Abdeslam Boularias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06096">Bounding Distributional Shifts in World Modeling through Novelty Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work on visual world models shows significant promise in latent state dynamics obtained from pre-trained image backbones. However, most of the current approaches are sensitive to training quality, requiring near-complete coverage of the action and state space during training to prevent divergence during inference. To make a model-based planning algorithm more robust to the quality of the learned world model, we propose in this work to use a variational autoencoder as a novelty detector to ensure that proposed action trajectories during planning do not cause the learned model to deviate from the training data distribution. To evaluate the effectiveness of this approach, a series of experiments in challenging simulated robot environments was carried out, with the proposed method incorporated into a model-predictive control policy loop extending the DINO-WM architecture. The results clearly show that the proposed method improves over state-of-the-art solutions in terms of data efficiency.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2507.19272.pdf' target='_blank'>https://arxiv.org/pdf/2507.19272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Simon, Tae-Ho Kim, Seul-Ki Yeom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19272">Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial and temporal priors without optical flow or tracking. When pre-training on a single 2-hour video, our approach raises the mean Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while remaining a drop-in replacement for image-only pipelines. Our results highlight video self-distillation as a lightweight route to geometry-aware perception an essential ingredient for physically plausible world models and Physical AI.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2507.13340.pdf' target='_blank'>https://arxiv.org/pdf/2507.13340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqi Wang, Mrinal Verghese, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13340">Latent Policy Steering with Embodiment-Agnostic Pretrained World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2507.13340.pdf' target='_blank'>https://arxiv.org/pdf/2507.13340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqi Wang, Mrinal Verghese, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13340">Latent Policy Steering with Embodiment-Agnostic Pretrained World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2506.20134.pdf' target='_blank'>https://arxiv.org/pdf/2506.20134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ningwei Xie, Zizi Tian, Lei Yang, Xiao-Ping Zhang, Meng Guo, Jie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20134">From 2D to 3D Cognition: A Brief Survey of General World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have garnered increasing attention in the development of artificial general intelligence (AGI), serving as computational frameworks for learning representations of the external world and forecasting future states. While early efforts focused on 2D visual perception and simulation, recent 3D-aware generative world models have demonstrated the ability to synthesize geometrically consistent, interactive 3D environments, marking a shift toward 3D spatial cognition. Despite rapid progress, the field lacks systematic analysis to categorize emerging techniques and clarify their roles in advancing 3D cognitive world models. This survey addresses this need by introducing a conceptual framework, providing a structured and forward-looking review of world models transitioning from 2D perception to 3D cognition. Within this framework, we highlight two key technological drivers, particularly advances in 3D representations and the incorporation of world knowledge, as fundamental pillars. Building on these, we dissect three core cognitive capabilities that underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning, and 3D spatial interaction. We further examine the deployment of these capabilities in real-world applications, including embodied AI, autonomous driving, digital twin, and gaming/VR. Finally, we identify challenges across data, modeling, and deployment, and outline future directions for advancing more robust and generalizable 3D world models.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2506.08441.pdf' target='_blank'>https://arxiv.org/pdf/2506.08441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh N. Nhu, Sanghyun Son, Ming Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08441">Time-Aware World Model for Adaptive Prediction and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce the Time-Aware World Model (TAWM), a model-based approach that explicitly incorporates temporal dynamics. By conditioning on the time-step size, Ît, and training over a diverse range of Ît values -- rather than sampling at a fixed time-step -- TAWM learns both high- and low-frequency task dynamics across diverse control problems. Grounded in the information-theoretic insight that the optimal sampling rate depends on a system's underlying dynamics, this time-aware formulation improves both performance and data efficiency. Empirical evaluations show that TAWM consistently outperforms conventional models across varying observation rates in a variety of control tasks, using the same number of training samples and iterations. Our code can be found online at: github.com/anh-nn01/Time-Aware-World-Model.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2506.07450.pdf' target='_blank'>https://arxiv.org/pdf/2506.07450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Loo, Akshunn Trivedi, Malika Meghjani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07450">Efficient Generation of Diverse Cooperative Agents with World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A major bottleneck in the training process for Zero-Shot Coordination (ZSC) agents is the generation of partner agents that are diverse in collaborative conventions. Current Cross-play Minimization (XPM) methods for population generation can be very computationally expensive and sample inefficient as the training objective requires sampling multiple types of trajectories. Each partner agent in the population is also trained from scratch, despite all of the partners in the population learning policies of the same coordination task. In this work, we propose that simulated trajectories from the dynamics model of an environment can drastically speed up the training process for XPM methods. We introduce XPM-WM, a framework for generating simulated trajectories for XPM via a learned World Model (WM). We show XPM with simulated trajectories removes the need to sample multiple trajectories. In addition, we show our proposed method can effectively generate partners with diverse conventions that match the performance of previous methods in terms of SP population training reward as well as training partners for ZSC agents. Our method is thus, significantly more sample efficient and scalable to a larger number of partners.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2506.04828.pdf' target='_blank'>https://arxiv.org/pdf/2506.04828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Latyshev, Gregory Gorbov, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04828">Safe Planning and Policy Optimization via World Model Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) applications in real-world scenarios must prioritize safety and reliability, which impose strict constraints on agent behavior. Model-based RL leverages predictive world models for action planning and policy optimization, but inherent model inaccuracies can lead to catastrophic failures in safety-critical settings. We propose a novel model-based RL framework that jointly optimizes task performance and safety. To address world model errors, our method incorporates an adaptive mechanism that dynamically switches between model-based planning and direct policy execution. We resolve the objective mismatch problem of traditional model-based approaches using an implicit world model. Furthermore, our framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, consistently selecting actions that surpass safe policy suggestions in both performance and safety. Experiments demonstrate significant improvements over non-adaptive methods, showing that our approach optimizes safety and performance simultaneously rather than merely meeting minimum safety requirements. The proposed framework achieves robust performance on diverse safety-critical continuous control tasks, outperforming existing methods.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2506.02996.pdf' target='_blank'>https://arxiv.org/pdf/2506.02996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthieu Tehenan, Christian Bolivar Moya, Tenghai Long, Guang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02996">Linear Spatial World Models Emerge in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated emergent abilities across diverse tasks, raising the question of whether they acquire internal world models. In this work, we investigate whether LLMs implicitly encode linear spatial world models, which we define as linear representations of physical space and object configurations. We introduce a formal framework for spatial world models and assess whether such structure emerges in contextual embeddings. Using a synthetic dataset of object positions, we train probes to decode object positions and evaluate geometric consistency of the underlying space. We further conduct causal interventions to test whether these spatial representations are functionally used by the model. Our results provide empirical evidence that LLMs encode linear spatial world models.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2506.01392.pdf' target='_blank'>https://arxiv.org/pdf/2506.01392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junha Chun, Youngjoon Jeong, Taesup Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01392">Sparse Imagination for Efficient Visual World Model Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. However, ensuring the prediction accuracy of world models often demands substantial computational resources, posing a major challenge for real-time applications. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. Our method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to adaptively adjust the number of tokens processed based on the computational resource. By enabling sparse imagination (rollout), our approach significantly accelerates planning while maintaining high control fidelity. Experimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency, paving the way for the deployment of world models in real-time decision-making scenarios.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2505.08073.pdf' target='_blank'>https://arxiv.org/pdf/2505.08073.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, Mark O. Riedl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08073">Explainable Reinforcement Learning Agents Using World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2504.10006.pdf' target='_blank'>https://arxiv.org/pdf/2504.10006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Charvet, Sebastian Stein, Roderick Murray-Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10006">Improving Controller Generalization with Dimensionless Markov Decision Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Controllers trained with Reinforcement Learning tend to be very specialized and thus generalize poorly when their testing environment differs from their training one. We propose a Model-Based approach to increase generalization where both world model and policy are trained in a dimensionless state-action space. To do so, we introduce the Dimensionless Markov Decision Process ($Î $-MDP): an extension of Contextual-MDPs in which state and action spaces are non-dimensionalized with the Buckingham-$Î $ theorem. This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics. We provide a generic framework for this approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2503.20425.pdf' target='_blank'>https://arxiv.org/pdf/2503.20425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Alcedo, Pedro U. Lima, Rachid Alami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20425">Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2503.20425.pdf' target='_blank'>https://arxiv.org/pdf/2503.20425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Alcedo, Pedro U. Lima, Rachid Alami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20425">Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2503.02279.pdf' target='_blank'>https://arxiv.org/pdf/2503.02279.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Li, Yinhan Lin, Qin Luo, Lina Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02279">DreamerV3 for Traffic Signal Control: Hyperparameter Tuning and Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has evolved into a widely investigated technology for the development of smart TSC strategies. However, current RL algorithms necessitate excessive interaction with the environment to learn effective policies, making them impractical for large-scale tasks. The DreamerV3 algorithm presents compelling properties for policy learning. It summarizes general dynamics knowledge about the environment and enables the prediction of future outcomes of potential actions from past experience, reducing the interaction with the environment through imagination training. In this paper, a corridor TSC model is trained using the DreamerV3 algorithm to explore the benefits of world models for TSC strategy learning. In RL environment design, to manage congestion levels effectively, both the state and reward functions are defined based on queue length, and the action is designed to manage queue length efficiently. Using the SUMO simulation platform, the two hyperparameters (training ratio and model size) of the DreamerV3 algorithm were tuned and analyzed across different OD matrix scenarios. We discovered that choosing a smaller model size and initially attempting several medium training ratios can significantly reduce the time spent on hyperparameter tuning. Additionally, we found that the approach is generally applicable as it can solve two TSC task scenarios with the same hyperparameters. Regarding the claimed data-efficiency of the DreamerV3 algorithm, due to the significant fluctuation of the episode reward curve in the early stages of training, it can only be confirmed that larger model sizes exhibit modest data-efficiency, and no evidence was found that increasing the training ratio accelerates convergence.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2502.00706.pdf' target='_blank'>https://arxiv.org/pdf/2502.00706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivica Nikolic, Teodora Baluta, Prateek Saxena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00706">Model Provenance Testing for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2501.09056.pdf' target='_blank'>https://arxiv.org/pdf/2501.09056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sneheel Sarangi, Maha Elgarf, Hanan Salam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09056">Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Theory of Mind (ToM) is the ability to understand and reflect on the mental states of others. Although this capability is crucial for human interaction, testing on Large Language Models (LLMs) reveals that they possess only a rudimentary understanding of it. Although the most capable closed-source LLMs have come close to human performance on some ToM tasks, they still perform poorly on complex variations of the task that involve more structured reasoning. In this work, we utilize the concept of "pretend-play", or ``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'': an LLM-based inference algorithm that improves model performance on complex ToM tasks. We recursively simulate user perspectives and decompose the ToM task into a simpler set of functions: subject identification, question-reframing, world model updation, and knowledge availability. We test the algorithm on higher-order ToM tasks and a task testing for ToM capabilities in a conversational setting, demonstrating that our approach shows significant improvement across models compared to baseline methods while requiring minimal prompt tuning across tasks and no additional model training.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2412.06486.pdf' target='_blank'>https://arxiv.org/pdf/2412.06486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Catalin E. Brita, Stephan Bongers, Frans A. Oliehoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06486">SimuDICE: Offline Policy Optimization Through World Model Updates and DICE Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In offline reinforcement learning, deriving an effective policy from a pre-collected set of experiences is challenging due to the distribution mismatch between the target policy and the behavioral policy used to collect the data, as well as the limited sample size. Model-based reinforcement learning improves sample efficiency by generating simulated experiences using a learned dynamic model of the environment. However, these synthetic experiences often suffer from the same distribution mismatch. To address these challenges, we introduce SimuDICE, a framework that iteratively refines the initial policy derived from offline data using synthetically generated experiences from the world model. SimuDICE enhances the quality of these simulated experiences by adjusting the sampling probabilities of state-action pairs based on stationary DIstribution Correction Estimation (DICE) and the estimated confidence in the model's predictions. This approach guides policy improvement by balancing experiences similar to those frequently encountered with ones that have a distribution mismatch. Our experiments show that SimuDICE achieves performance comparable to existing algorithms while requiring fewer pre-collected experiences and planning steps, and it remains robust across varying data collection policies.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2412.02331.pdf' target='_blank'>https://arxiv.org/pdf/2412.02331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Arda Eren, Erhan Oztop
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02331">Sample Efficient Robot Learning in Supervised Effect Prediction Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In self-supervised robotic learning, agents acquire data through active interaction with their environment, incurring costs such as energy use, human oversight, and experimental time. To mitigate these, sample-efficient exploration is essential. While intrinsic motivation (IM) methods like learning progress (LP) are widely used in robotics, and active learning (AL) is well established for classification in machine learning, few frameworks address continuous, high-dimensional regression tasks typical of world model learning. We propose MUSEL (Model Uncertainty for Sample-Efficient Learning), a novel AL framework tailored for regression tasks in robotics, such as action-effect prediction. MUSEL introduces a model uncertainty metric that combines total predictive uncertainty, learning progress, and input diversity to guide data acquisition. We validate our approach using a Stochastic Variational Deep Kernel Learning (SVDKL) model in two robotic tabletop tasks. Experimental results demonstrate that MUSEL improves both learning accuracy and sample efficiency, validating its effectiveness in learning action effects and selecting informative samples.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2411.15042.pdf' target='_blank'>https://arxiv.org/pdf/2411.15042.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Ding, Ziming Wang, Yi Ding, Hongjie Lin, SuYang Xi, Chia Chao Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15042">Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing the challenge of ensuring safety in ever-changing and unpredictable environments, particularly in the swiftly advancing realm of autonomous driving in today's 5G wireless communication world, we present Navigation Secure (NavSecure). This vision-based navigation framework merges the strengths of world models with crucial safety-focused decision-making capabilities, enabling autonomous vehicles to navigate real-world complexities securely. Our approach anticipates potential threats and formulates safer routes by harnessing the predictive capabilities of world models, thus significantly reducing the need for extensive real-world trial-and-error learning. Additionally, our method empowers vehicles to autonomously learn and develop through continuous practice, ensuring the system evolves and adapts to new challenges. Incorporating radio frequency technology, NavSecure leverages 5G networks to enhance real-time data exchange, improving communication and responsiveness. Validated through rigorous experiments under simulation-to-real driving conditions, NavSecure has shown exceptional performance in safety-critical scenarios, such as sudden obstacle avoidance. Results indicate that NavSecure excels in key safety metrics, including collision prevention and risk reduction, surpassing other end-to-end methodologies. This framework not only advances autonomous driving safety but also demonstrates how world models can enhance decision-making in critical applications. NavSecure sets a new standard for developing more robust and trustworthy autonomous driving systems, capable of handling the inherent dynamics and uncertainties of real-world environments.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2411.02446.pdf' target='_blank'>https://arxiv.org/pdf/2411.02446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanlin Duan, Wensen Mao, He Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02446">Learning World Models for Unconstrained Goal Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent's replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for "World Models for Unconstrained Goal Navigation"). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any "key" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy's capacity to generalize across new goal settings.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2410.09252.pdf' target='_blank'>https://arxiv.org/pdf/2410.09252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09252">DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing a generalist scientific agent capable of performing tasks in laboratory settings to assist researchers has become a key goal in recent Artificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks are inherently more delicate and complex, requiring agents to possess a higher level of reasoning ability, structured and temporal understanding of their environment, and a strong emphasis on safety. Existing approaches often fail to address these multifaceted requirements. To tackle these challenges, we present DAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches, DAVIS incorporates structured and temporal memory, which enables model-based planning. Additionally, DAVIS implements an agentic, multi-turn retrieval system, similar to a human's inner monologue, allowing for a greater degree of reasoning over past experiences. DAVIS demonstrates substantially improved performance on the ScienceWorld benchmark comparing to previous approaches on 8 out of 9 elementary science subjects. In addition, DAVIS's World Model demonstrates competitive performance on the famous HotpotQA and MusiqueQA dataset for multi-hop question answering. To the best of our knowledge, DAVIS is the first RAG agent to employ an interactive retrieval method in a RAG pipeline.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2410.09252.pdf' target='_blank'>https://arxiv.org/pdf/2410.09252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09252">DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing a generalist scientific agent capable of performing tasks in laboratory settings to assist researchers has become a key goal in recent Artificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks are inherently more delicate and complex, requiring agents to possess a higher level of reasoning ability, structured and temporal understanding of their environment, and a strong emphasis on safety. Existing approaches often fail to address these multifaceted requirements. To tackle these challenges, we present DAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches, DAVIS incorporates structured and temporal memory, which enables model-based planning. Additionally, DAVIS implements an agentic, multi-turn retrieval system, similar to a human's inner monologue, allowing for a greater degree of reasoning over past experiences. DAVIS demonstrates substantially improved performance on the ScienceWorld benchmark comparing to previous approaches on 8 out of 9 elementary science subjects. In addition, DAVIS's World Model demonstrates competitive performance on the famous HotpotQA and MusiqueQA dataset for multi-hop question answering. To the best of our knowledge, DAVIS is the first RAG agent to employ an interactive retrieval method in a RAG pipeline.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2410.00258.pdf' target='_blank'>https://arxiv.org/pdf/2410.00258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lancelot Da Costa, TomÃ¡Å¡ GavenÄiak, David Hyland, Mandana Samiei, Cristian Dragos-Manta, Candice Pattisapu, Adeel Razi, Karl Friston
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00258">Possible Principles for Aligned Structure Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper offers a roadmap for the development of scalable aligned artificial intelligence (AI) from first principle descriptions of natural intelligence. In brief, a possible path toward scalable aligned AI rests upon enabling artificial agents to learn a good model of the world that includes a good model of our preferences. For this, the main objective is creating agents that learn to represent the world and other agents' world models; a problem that falls under structure learning (a.k.a. causal representation learning or model discovery). We expose the structure learning and alignment problems with this goal in mind, as well as principles to guide us forward, synthesizing various ideas across mathematics, statistics, and cognitive science. 1) We discuss the essential role of core knowledge, information geometry and model reduction in structure learning, and suggest core structural modules to learn a wide range of naturalistic worlds. 2) We outline a way toward aligned agents through structure learning and theory of mind. As an illustrative example, we mathematically sketch Asimov's Laws of Robotics, which prescribe agents to act cautiously to minimize the ill-being of other agents. We supplement this example by proposing refined approaches to alignment. These observations may guide the development of artificial intelligence in helping to scale existing -- or design new -- aligned structure learning systems.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2409.16663.pdf' target='_blank'>https://arxiv.org/pdf/2409.16663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David NistÃ©r, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16663">Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2408.08845.pdf' target='_blank'>https://arxiv.org/pdf/2408.08845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel de Marchi, Michael Kosorok, Scott de Marchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08845">Shapley Marginal Surplus for Strong Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Shapley values have seen widespread use in machine learning as a way to explain model predictions and estimate the importance of covariates. Accurately explaining models is critical in real-world models to both aid in decision making and to infer the properties of the true data-generating process (DGP). In this paper, we demonstrate that while model-based Shapley values might be accurate explainers of model predictions, machine learning models themselves are often poor explainers of the DGP even if the model is highly accurate. Particularly in the presence of interrelated or noisy variables, the output of a highly predictive model may fail to account for these relationships. This implies explanations of a trained model's behavior may fail to provide meaningful insight into the DGP. In this paper we introduce a novel variable importance algorithm, Shapley Marginal Surplus for Strong Models, that samples the space of possible models to come up with an inferential measure of feature importance. We compare this method to other popular feature importance methods, both Shapley-based and non-Shapley based, and demonstrate significant outperformance in inferential capabilities relative to other methods.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2408.06628.pdf' target='_blank'>https://arxiv.org/pdf/2408.06628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarek A. Elsharhawy, P. James Schuck, Shuo Liu, Luc Saikali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06628">Mathematical Optimization of Resolution Improvement in Structured Light data by Periodic Scanning Motion: Application for Feedback during Lunar Landing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research explores the enhancement of lunar landing precision through an advanced structured light system, integrating machine learning, Iterative Learning Control (ILC) and Structured Illumination Microscopy (SIM) techniques. By employing Moire fringe patterns for high-precision scanning maneuvers, the study addresses the limitations of conventional structured light systems. A nonlinear mathematical optimization model is developed to refine the world model, optimizing oscillation frequency and amplitude to improve resolution. The findings suggest that this approach can double the conventional resolution, promising significant advancements in the accuracy of lunar landings, with potential real-time application.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2407.11249.pdf' target='_blank'>https://arxiv.org/pdf/2407.11249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pantelis Vafidis, Aman Bhargava, Antonio Rangel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11249">Disentangling Representations through Multi-task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure (''disentangled'' or ''abstract'' representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along approximately orthogonal directions, thus facilitating feature-based generalization. We provide experimental and theoretical results guaranteeing the emergence of disentangled representations in agents that optimally solve multi-task evidence accumulation classification tasks, canonical in the neuroscience literature. The key conceptual finding is that, by producing accurate multi-task classification estimates, a system implicitly represents a set of coordinates specifying a disentangled representation of the underlying latent state of the data it receives. The theory provides conditions for the emergence of these representations in terms of noise, number of tasks, and evidence accumulation time. We experimentally validate these predictions in RNNs trained to multi-task, which learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization in predicting latent factors. We demonstrate the robustness of our framework across autoregressive architectures, decision boundary geometries and in tasks requiring classification confidence estimation. We find that transformers are particularly suited for disentangling representations, which might explain their unique world understanding abilities. Overall, our framework establishes a formal link between competence at multiple tasks and the formation of disentangled, interpretable world models in both biological and artificial systems, and helps explain why ANNs often arrive at human-interpretable concepts, and how they both may acquire exceptional zero-shot generalization capabilities.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2405.13798.pdf' target='_blank'>https://arxiv.org/pdf/2405.13798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Bell, Avinash Mudireddy, Ivan Johnson-Eversoll, Soura Dasgupta, Raghu Mudumbai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13798">Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We prove a new asymptotic un-equipartition property for the perplexity of long texts generated by a language model and present supporting experimental evidence from open-source models. Specifically we show that the logarithmic perplexity of any large text generated by a language model must asymptotically converge to the average entropy of its token distributions. This defines a ``typical set'' that all long synthetic texts generated by a language model must belong to. We refine the concept of ''typical set'' to include only grammatically correct texts. We then show that this refined typical set is a vanishingly small subset of all possible grammatically correct texts for a very general definition of grammar. This means that language models are strongly constrained in the range of their possible behaviors and outputs. We make no simplifying assumptions (such as stationarity) about the statistics of language model outputs, and therefore our results are directly applicable to practical real-world models without any approximations. We discuss possible applications of the typical set concept to problems such as detecting synthetic texts and membership inference in training datasets.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2405.13798.pdf' target='_blank'>https://arxiv.org/pdf/2405.13798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Bell, Avinash Mudireddy, Ivan Johnson-Eversoll, Soura Dasgupta, Raghu Mudumbai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13798">Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We prove a new asymptotic un-equipartition property for the perplexity of long texts generated by a language model and present supporting experimental evidence from open-source models. Specifically we show that the logarithmic perplexity of any large text generated by a language model must asymptotically converge to the average entropy of its token distributions. This defines a ``typical set'' that all long synthetic texts generated by a language model must belong to. We refine the concept of ''typical set'' to include only grammatically correct texts. We then show that this refined typical set is a vanishingly small subset of all possible grammatically correct texts for a very general definition of grammar. This means that language models are strongly constrained in the range of their possible behaviors and outputs. We make no simplifying assumptions (such as stationarity) about the statistics of language model outputs, and therefore our results are directly applicable to practical real-world models without any approximations. We discuss possible applications of the typical set concept to problems such as detecting synthetic texts and membership inference in training datasets.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2402.01695.pdf' target='_blank'>https://arxiv.org/pdf/2402.01695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, Karthik Narasimhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01695">Language-Guided World Models: A Model-Based Approach to AI Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the concept of Language-Guided World Models (LWMs) -- probabilistic models that can simulate environments by reading texts. Agents equipped with these models provide humans with more extensive and efficient control, allowing them to simultaneously alter agent behaviors in multiple tasks via natural verbal communication. In this work, we take initial steps in developing robust LWMs that can generalize to compositionally novel language descriptions. We design a challenging world modeling benchmark based on the game of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that require varying degrees of compositional generalization. Our experiments reveal the lack of generalizability of the state-of-the-art Transformer model, as it offers marginal improvements in simulation quality over a no-text baseline. We devise a more robust model by fusing the Transformer with the EMMA attention mechanism (Hanjie et al., 2021). Our model substantially outperforms the Transformer and approaches the performance of a model with an oracle semantic parsing and grounding capability. To demonstrate the practicality of this model in improving AI safety and transparency, we simulate a scenario in which the model enables an agent to present plans to a human before execution, and to revise plans based on their language feedback.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2401.03910.pdf' target='_blank'>https://arxiv.org/pdf/2401.03910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RaphaÃ«l MilliÃ¨re, Cameron Buckner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03910">A Philosophical Introduction to Language Models -- Part I: Continuity With Classic Debates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2312.09056.pdf' target='_blank'>https://arxiv.org/pdf/2312.09056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rudra P. K. Poudel, Harit Pandya, Stephan Liwicki, Roberto Cipolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09056">ReCoRe: Regularized Contrastive Representation Learning of World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the naÃ¯ve integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overcome this issue, we propose an intervention-invariant regularizer in the form of an auxiliary task such as depth prediction, image denoising, image segmentation, etc., that explicitly enforces invariance to style interventions. Our method outperforms current state-of-the-art model-based and model-free RL methods and significantly improves on out-of-distribution point navigation tasks evaluated on the iGibson benchmark. With only visual observations, we further demonstrate that our approach outperforms recent language-guided foundation models for point navigation, which is essential for deployment on robots with limited computation capabilities. Finally, we demonstrate that our proposed model excels at the sim-to-real transfer of its perception module on the Gibson benchmark.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2311.17593.pdf' target='_blank'>https://arxiv.org/pdf/2311.17593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rudra P. K. Poudel, Harit Pandya, Chao Zhang, Roberto Cipolla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17593">LanGWM: Language Grounded World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language.
  Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique.
  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach.
  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2311.06673.pdf' target='_blank'>https://arxiv.org/pdf/2311.06673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Wen, Songan Zhang, H. Eric Tseng, Huei Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06673">Dream to Adapt: Meta Reinforcement Learning by Latent Context Imagination and MDP Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Meta reinforcement learning (Meta RL) has been amply explored to quickly learn an unseen task by transferring previously learned knowledge from similar tasks. However, most state-of-the-art algorithms require the meta-training tasks to have a dense coverage on the task distribution and a great amount of data for each of them. In this paper, we propose MetaDreamer, a context-based Meta RL algorithm that requires less real training tasks and data by doing meta-imagination and MDP-imagination. We perform meta-imagination by interpolating on the learned latent context space with disentangled properties, as well as MDP-imagination through the generative world model where physical knowledge is added to plain VAE networks. Our experiments with various benchmarks show that MetaDreamer outperforms existing approaches in data efficiency and interpolated generalization.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2310.17178.pdf' target='_blank'>https://arxiv.org/pdf/2310.17178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonid Ugadiarov, Vitaliy Vorobyov, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17178">Relational Object-Centric Actor-Critic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advances in unsupervised object-centric representation learning have significantly improved its application to downstream tasks. Recent works highlight that disentangled object representations can aid policy learning in image-based, object-centric reinforcement learning tasks. This paper proposes a novel object-centric reinforcement learning algorithm that integrates actor-critic and model-based approaches by incorporating an object-centric world model within the critic. The world model captures the environment's data-generating process by predicting the next state and reward given the current state-action pair, where actions are interventions in the environment. In model-based reinforcement learning, world model learning can be interpreted as a causal induction problem, where the agent must learn the causal relationships underlying the environment's dynamics. We evaluate our method in a simulated 3D robotic environment and a 2D environment with compositional structure. As baselines, we compare against object-centric, model-free actor-critic algorithms and a state-of-the-art monolithic model-based algorithm. While the baselines show comparable performance in easier tasks, our approach outperforms them in more challenging scenarios with a large number of objects or more complex dynamics.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2308.02136.pdf' target='_blank'>https://arxiv.org/pdf/2308.02136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusuke Kato, Ryo Okumura, Tadahiro Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02136">World-Model-Based Control for Industrial box-packing of Multiple Objects using NewtonianVAE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The process of industrial box-packing, which involves the accurate placement of multiple objects, requires high-accuracy positioning and sequential actions. When a robot is tasked with placing an object at a specific location with high accuracy, it is important not only to have information about the location of the object to be placed, but also the posture of the object grasped by the robotic hand. Often, industrial box-packing requires the sequential placement of identically shaped objects into a single box. The robot's action should be determined by the same learned model. In factories, new kinds of products often appear and there is a need for a model that can easily adapt to them. Therefore, it should be easy to collect data to train the model. In this study, we designed a robotic system to automate real-world industrial tasks, employing a vision-based learning control model. We propose in-hand-view-sensitive Newtonian variational autoencoder (ihVS-NVAE), which employs an RGB camera to obtain in-hand postures of objects. We demonstrate that our model, trained for a single object-placement task, can handle sequential tasks without additional training. To evaluate efficacy of the proposed model, we employed a real robot to perform sequential industrial box-packing of multiple objects. Results showed that the proposed model achieved a 100% success rate in industrial box-packing tasks, thereby outperforming the state-of-the-art and conventional approaches, underscoring its superior effectiveness and potential in industrial tasks.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2305.14644.pdf' target='_blank'>https://arxiv.org/pdf/2305.14644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hemanth Manjunatha, Andrey Pak, Dimitar Filev, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14644">KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning World Models in Autonomous Driving Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving has received a great deal of attention in the automotive industry and is often seen as the future of transportation. The development of autonomous driving technology has been greatly accelerated by the growth of end-to-end machine learning techniques that have been successfully used for perception, planning, and control tasks. An important aspect of autonomous driving planning is knowing how the environment evolves in the immediate future and taking appropriate actions. An autonomous driving system should effectively use the information collected from the various sensors to form an abstract representation of the world to maintain situational awareness. For this purpose, deep learning models can be used to learn compact latent representations from a stream of incoming data. However, most deep learning models are trained end-to-end and do not incorporate any prior knowledge (e.g., from physics) of the vehicle in the architecture. In this direction, many works have explored physics-infused neural network (PINN) architectures to infuse physics models during training. Inspired by this observation, we present a Kalman filter augmented recurrent neural network architecture to learn the latent representation of the traffic flow using front camera images only. We demonstrate the efficacy of the proposed model in both imitation and reinforcement learning settings using both simulated and real-world datasets. The results show that incorporating an explicit model of the vehicle (states estimated using Kalman filtering) in the end-to-end learning significantly increases performance.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2304.06281.pdf' target='_blank'>https://arxiv.org/pdf/2304.06281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenli Xiao, Yiwei Lyu, John Dolan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06281">Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields without prior knowledge of the dynamics model. The proposed algorithm obtains an approximate world model by interacting with the environment during the early stage of exploration, making our MBDS enjoy formal safety guarantees with high probability. We demonstrate in simulations that our framework can surpass existing baselines in terms of safety guarantees and learning performance.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2302.08669.pdf' target='_blank'>https://arxiv.org/pdf/2302.08669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aastha Acharya, Rebecca Russell, Nisar R. Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08669">Learning to Forecast Aleatoric and Epistemic Uncertainties over Long Horizon Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Giving autonomous agents the ability to forecast their own outcomes and uncertainty will allow them to communicate their competencies and be used more safely. We accomplish this by using a learned world model of the agent system to forecast full agent trajectories over long time horizons. Real world systems involve significant sources of both aleatoric and epistemic uncertainty that compound and interact over time in the trajectory forecasts. We develop a deep generative world model that quantifies aleatoric uncertainty while incorporating the effects of epistemic uncertainty during the learning process. We show on two reinforcement learning problems that our uncertainty model produces calibrated outcome uncertainty estimates over the full trajectory horizon.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2301.10067.pdf' target='_blank'>https://arxiv.org/pdf/2301.10067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Latyshev, Aleksandr I. Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10067">Intrinsic Motivation in Model-based Reinforcement Learning: A Brief Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reinforcement learning research area contains a wide range of methods for solving the problems of intelligent agent control. Despite the progress that has been made, the task of creating a highly autonomous agent is still a significant challenge. One potential solution to this problem is intrinsic motivation, a concept derived from developmental psychology. This review considers the existing methods for determining intrinsic motivation based on the world model obtained by the agent. We propose a systematic approach to current research in this field, which consists of three categories of methods, distinguished by the way they utilize a world model in the agent's components: complementary intrinsic reward, exploration policy, and intrinsically motivated goals. The proposed unified framework describes the architecture of agents using a world model and intrinsic motivation to improve learning. The potential for developing new techniques in this area of research is also examined.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2207.11290.pdf' target='_blank'>https://arxiv.org/pdf/2207.11290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nandita Bhaskhar, Daniel L. Rubin, Christopher Lee-Messer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11290">TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection, and (2) data drift detection. We evaluate across diverse domains - audio and vision using public datasets and further benchmark our approach on challenging, real-world electroencephalograms (EEG) datasets for seizure detection. Our latent-space mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision), 73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10 points. We expose critical failures in popular baselines that remain insensitive to input semantic content, rendering them unfit for real-world model monitoring. We show that our sequential mistrust scores achieve high drift detection rates; over 90% of the streams show < 20% error for all domains. Through extensive qualitative and quantitative evaluations, we show that our mistrust scores are more robust and provide explainability for easy adoption into practice.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2205.10044.pdf' target='_blank'>https://arxiv.org/pdf/2205.10044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cristiano Capone, Pier Stanislao Paolucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10044">Towards biologically plausible Dreaming and Planning in recurrent spiking networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans and animals can learn new skills after practicing for a few hours, while current reinforcement learning algorithms require a large amount of data to achieve good performances. Recent model-based approaches show promising results by reducing the number of necessary interactions with the environment to learn a desirable policy. However, these methods require biological implausible ingredients, such as the detailed storage of older experiences, and long periods of offline learning. The optimal way to learn and exploit word-models is still an open question. Taking inspiration from biology, we suggest that dreaming might be an efficient expedient to use an inner model. We propose a two-module (agent and model) spiking neural network in which "dreaming" (living new experiences in a model-based simulated environment) significantly boosts learning. We also explore "planning", an online alternative to dreaming, that shows comparable performances. Importantly, our model does not require the detailed storage of experiences, and learns online the world-model and the policy. Moreover, we stress that our network is composed of spiking neurons, further increasing the biological plausibility and implementability in neuromorphic hardware.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2508.18507.pdf' target='_blank'>https://arxiv.org/pdf/2508.18507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dillon Z. Chen, Johannes Zenn, Tristan Cinquin, Sheila A. McIlraith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18507">Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the usage of language models (LMs) for planning over world models specified in the Planning Domain Definition Language (PDDL). We prompt LMs to generate Python programs that serve as generalised policies for solving PDDL problems from a given domain. Notably, our approach synthesises policies that are provably sound relative to the PDDL domain without reliance on external verifiers. We conduct experiments on competition benchmarks which show that our policies can solve more PDDL problems than PDDL planners and recent LM approaches within a fixed time and memory constraint. Our approach manifests in the LMPlan planner which can solve planning problems with several hundreds of relevant objects. Surprisingly, we observe that LMs used in our framework sometimes plan more effectively over PDDL problems written in meaningless symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1 o3). This finding challenges hypotheses that LMs reason over word semantics and memorise solutions from its training corpus, and is worth further exploration.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2508.00159.pdf' target='_blank'>https://arxiv.org/pdf/2508.00159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jobst Heitzig, Ram Potham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00159">Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2507.19855.pdf' target='_blank'>https://arxiv.org/pdf/2507.19855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Sharma, Ananya Gupta, Chengyu Wang, Chiamaka Adebayo, Jakub Kowalski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19855">Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs), despite their advanced linguistic capabilities, fundamentally lack an intuitive understanding of physical dynamics, which limits their effectiveness in real-world scenarios that require causal reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a novel framework designed to embed an explicit model of causal physics within an LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a new training objective called Causal Intervention Loss, encouraging the model to learn cause-and-effect relationships from multimodal data. By training the model to predict the outcomes of hypothetical interventions instead of merely capturing statistical correlations, CWMI develops a robust internal representation of physical laws. Experimental results show that CWMI significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench dataset. These findings demonstrate that inducing a causal world model is a critical step toward more reliable and generalizable AI systems.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2507.15521.pdf' target='_blank'>https://arxiv.org/pdf/2507.15521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cole Robertson, Philip Wolff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15521">LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Do large language models (LLMs) construct and manipulate internal world models, or do they rely solely on statistical associations represented as output layer token probabilities? We adapt cognitive science methodologies from human mental models research to test LLMs on pulley system problems using TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical advantage (MA). State-of-the-art models performed marginally but significantly above chance, and their estimates correlated significantly with ground-truth MA. Significant correlations between number of pulleys and model estimates suggest that models employed a pulley counting heuristic, without necessarily simulating pulley systems to derive precise values. Study 2 tested this by probing whether LLMs represent global features crucial to MA estimation. Models evaluated a functionally connected pulley system against a fake system with randomly placed components. Without explicit cues, models identified the functional system as having greater MA with F1=0.8, suggesting LLMs could represent systems well enough to differentiate jumbled from functional systems. Study 3 built on this by asking LLMs to compare functional systems with matched systems which were connected up but which transferred no force to the weight; LLMs identified the functional system with F1=0.46, suggesting random guessing. Insofar as they may generalize, these findings are compatible with the notion that LLMs manipulate internal world models, sufficient to exploit statistical associations between pulley count and MA (Study 1), and to approximately represent system components' spatial relations (Study 2). However, they may lack the facility to reason over nuanced structural connectivity (Study 3). We conclude by advocating the utility of cognitive scientific methods to evaluate the world-modeling capacities of artificial intelligence systems.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2507.13871.pdf' target='_blank'>https://arxiv.org/pdf/2507.13871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehul Anand, Shishir Kolathaya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13871">Safety Certification in the Latent space using Control Barrier Functions and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesising safe controllers from visual data typically requires extensive supervised labelling of safety-critical data, which is often impractical in real-world settings. Recent advances in world models enable reliable prediction in latent spaces, opening new avenues for scalable and data-efficient safe control. In this work, we introduce a semi-supervised framework that leverages control barrier certificates (CBCs) learned in the latent space of a world model to synthesise safe visuomotor policies. Our approach jointly learns a neural barrier function and a safe controller using limited labelled data, while exploiting the predictive power of modern vision transformers for latent dynamics modelling.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2506.23434.pdf' target='_blank'>https://arxiv.org/pdf/2506.23434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianran Liu, Shengwen Zhao, Nicholas Rhinehart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23434">Towards foundational LiDAR world models with efficient latent flow matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based world models offer more structured and geometry-aware representations than their image-based counterparts. However, existing LiDAR world models are narrowly trained; each model excels only in the domain for which it was built. Can we develop LiDAR world models that exhibit strong transferability across multiple domains? We conduct the first systematic domain transfer study across three demanding scenarios: (i) outdoor to indoor generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii) non-semantic to semantic transfer. Given different amounts of fine-tuning data, our experiments show that a single pre-trained model can achieve up to 11% absolute improvement (83\% relative) over training from scratch and outperforms training from scratch in 30/36 of our comparisons. This transferability of dynamic learning significantly reduces the reliance on manually annotated data for semantic occupancy forecasting: our method exceed the previous semantic occupancy forecasting models with only 5% of the labeled training data required by prior models. We also observed inefficiencies of current LiDAR world models, mainly through their under-compression of LiDAR data and inefficient training objectives. To address this, we propose a latent conditional flow matching (CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy using only half the training data and a compression ratio 6 times higher than that of prior methods. Our model achieves SOTA performance on future-trajectory-conditioned semantic occupancy forecasting while being 23x more computationally efficient (a 28x FPS speedup); and achieves SOTA performance on semantic occupancy forecasting while being 2x more computationally efficient (a 1.1x FPS speedup).
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2506.18537.pdf' target='_blank'>https://arxiv.org/pdf/2506.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18537">Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents' evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2506.17263.pdf' target='_blank'>https://arxiv.org/pdf/2506.17263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massimiliano Tamborski, David Abel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17263">Memory Allocation in Resource-Constrained Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Resource constraints can fundamentally change both learning and decision-making. We explore how memory constraints influence an agent's performance when navigating unknown environments using standard reinforcement learning algorithms. Specifically, memory-constrained agents face a dilemma: how much of their limited memory should be allocated to each of the agent's internal processes, such as estimating a world model, as opposed to forming a plan using that model? We study this dilemma in MCTS- and DQN-based algorithms and examine how different allocations of memory impact performance in episodic and continual learning settings.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2506.17103.pdf' target='_blank'>https://arxiv.org/pdf/2506.17103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shruti Sadanand Dongare, Amun Kharel, Jonathan Samuel, Xiaona Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17103">TransDreamerV3: Implanting Transformer In DreamerV3</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder. The model is designed to improve memory and decision-making capabilities in complex environments. We conducted experiments on Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved performance over DreamerV3, particularly in the Atari-Freeway and Crafter tasks. While issues in the Minecraft task and limited training across all tasks were noted, TransDreamerV3 displays advancement in world model-based reinforcement learning, leveraging transformer architectures.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2506.13761.pdf' target='_blank'>https://arxiv.org/pdf/2506.13761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanruo Ning, Kuan Fang, Wei-Chiu Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13761">Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in open-world robot manipulation have been largely driven by vision-language models (VLMs). While these models exhibit strong generalization ability in high-level planning, they struggle to predict low-level robot controls due to limited physical-world understanding. To address this issue, we propose a model predictive control framework for open-world manipulation that combines the semantic reasoning capabilities of VLMs with physically-grounded, interactive digital twins of the real-world environments. By constructing and simulating the digital twins, our approach generates feasible motion trajectories, simulates corresponding outcomes, and prompts the VLM with future observations to evaluate and select the most suitable outcome based on language instructions of the task. To further enhance the capability of pre-trained VLMs in understanding complex scenes for robotic control, we leverage the flexible rendering capabilities of the digital twin to synthesize the scene at various novel, unoccluded viewpoints. We validate our approach on a diverse set of complex manipulation tasks, demonstrating superior performance compared to baseline methods for language-conditioned robotic control using VLMs.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2506.13252.pdf' target='_blank'>https://arxiv.org/pdf/2506.13252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaspar Rothenfusser, Bekk Blando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13252">Vector Ontologies as an LLM world view extraction method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2506.09499.pdf' target='_blank'>https://arxiv.org/pdf/2506.09499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas J. Ringstrom, Paul R. Schrater
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09499">A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free Markov Decision Process. Rather than a value function, OKBEs directly construct and optimize a predictive map called a state-time option kernel (STOK) to maximize the probability of completing a goal while avoiding constraint violations. STOKs are compositional, modular, and interpretable initiation-to-termination transition kernels for policies in the Options Framework of Reinforcement Learning. This means: 1) STOKs can be composed using Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple policies over long horizons, 2) high-dimensional STOKs can be represented and computed efficiently in a factorized and reconfigurable form, and 3) STOKs record the probabilities of semantically interpretable goal-success and constraint-violation events, needed for formal verification. Given a high-dimensional state-transition model for an intractable planning problem, we can decompose it with local STOKs and goal-conditioned policies that are aggregated into a factorized goal kernel, making it possible to forward-plan at the level of goals in high-dimensions to solve the problem. These properties lead to highly flexible agents that can rapidly synthesize meta-policies, reuse planning representations across many tasks, and justify goals using empowerment, an intrinsic motivation function. We argue that reward-maximization is in conflict with the properties of compositionality, modularity, and interpretability. Alternatively, OKBEs facilitate these properties to support verifiable long-horizon planning and intrinsic motivation that scales to dynamic high-dimensional world-models.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2506.03424.pdf' target='_blank'>https://arxiv.org/pdf/2506.03424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03424">DistRAG: Towards Distance-Based Spatial Reasoning in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real world tasks where Large Language Models (LLMs) can be used require spatial reasoning, like Point of Interest (POI) recommendation and itinerary planning. However, on their own LLMs lack reliable spatial reasoning capabilities, especially about distances. To address this problem, we develop a novel approach, DistRAG, that enables an LLM to retrieve relevant spatial information not explicitly learned during training. Our method encodes the geodesic distances between cities and towns in a graph and retrieves a context subgraph relevant to the question. Using this technique, our method enables an LLM to answer distance-based reasoning questions that it otherwise cannot answer. Given the vast array of possible places an LLM could be asked about, DistRAG offers a flexible first step towards providing a rudimentary `world model' to complement the linguistic knowledge held in LLMs.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2506.01529.pdf' target='_blank'>https://arxiv.org/pdf/2506.01529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Delliaux, Nguyen-Khanh Vu, Vincent FranÃ§ois-Lavet, Elise van der Pol, Emmanuel Rachelson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01529">Learning Abstract World Models with a Group-Structured Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning meaningful abstract models of Markov Decision Processes (MDPs) is crucial for improving generalization from limited data. In this work, we show how geometric priors can be imposed on the low-dimensional representation manifold of a learned transition model. We incorporate known symmetric structures via appropriate choices of the latent space and the associated group actions, which encode prior knowledge about invariances in the environment. In addition, our framework allows the embedding of additional unstructured information alongside these symmetries. We show experimentally that this leads to better predictions of the latent transition model than fully unstructured approaches, as well as better learning on downstream RL tasks, in environments with rotational and translational features, including in first-person views of 3D environments. Additionally, our experiments show that this leads to simpler and more disentangled representations. The full code is available on GitHub to ensure reproducibility.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2506.01182.pdf' target='_blank'>https://arxiv.org/pdf/2506.01182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Qasim Ali, Aditya Sridhar, Shahbuland Matiana, Alex Wong, Mohammad Al-Sharman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01182">Humanoid World Models: Open World Foundation Models for Humanoid Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid robots, with their human-like form, are uniquely suited for interacting in environments built for people. However, enabling humanoids to reason, plan, and act in complex open-world settings remains a challenge. World models, models that predict the future outcome of a given action, can support these capabilities by serving as a dynamics model in long-horizon planning and generating synthetic data for policy learning. We introduce Humanoid World Models (HWM), a family of lightweight, open-source models that forecast future egocentric video conditioned on humanoid control tokens. We train two types of generative models, Masked Transformers and Flow-Matching, on 100 hours of humanoid demonstrations. Additionally, we explore architectural variants with different attention mechanisms and parameter-sharing strategies. Our parameter-sharing techniques reduce model size by 33-53% with minimal impact on performance or visual fidelity. HWMs are designed to be trained and deployed in practical academic and small-lab settings, such as 1-2 GPUs.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2506.00138.pdf' target='_blank'>https://arxiv.org/pdf/2506.00138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reece Keller, Alyn Tornell, Felix Pei, Xaq Pitkow, Leo Kozachkov, Aran Nayebi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00138">Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in sparse reward and reward-free environments, including class of methods known as intrinsic motivation, exhibit inconsistent exploration patterns and thus fail to produce robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in unconstrained, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed to capture robust autonomous exploration observed in animals. Our method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. We demonstrate that artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2505.20964.pdf' target='_blank'>https://arxiv.org/pdf/2505.20964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Bennis, Salem Lahlou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20964">Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The trajectories of 6G and AI are set for a creative collision. However, current visions for 6G remain largely incremental evolutions of 5G, while progress in AI is hampered by brittle, data-hungry models that lack robust reasoning capabilities. This paper argues for a foundational paradigm shift, moving beyond the purely technical level of communication toward systems capable of semantic understanding and effective, goal-oriented interaction. We propose a unified research vision rooted in the principles of System-2 cognition, built upon three pillars: Abstraction, enabling agents to learn meaningful world models from raw sensorimotor data; Compositionality, providing the algebraic tools to combine learned concepts and subsystems; and Emergent Communication, allowing intelligent agents to create their own adaptive and grounded languages. By integrating these principles, we lay the groundwork for truly intelligent systems that can reason, adapt, and collaborate, unifying advances in wireless communications, machine learning, and robotics under a single coherent framework.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2505.16787.pdf' target='_blank'>https://arxiv.org/pdf/2505.16787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Sundar, Chunbo Luo, Xiaoyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16787">Enter the Void - Planning to Seek Entropy When Reward is Scarce</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. MBRL methods have progressed by largely prioritising the actor; optimising the world model learning has been neglected meanwhile. Improving the fidelity of the world model and reducing its time to convergence can yield significant downstream benefits, one of which is improving the ensuing performance of any actor it may train. We propose a novel approach that anticipates and actively seeks out high-entropy states using short-horizon latent predictions generated by the world model, offering a principled alternative to traditional curiosity-driven methods that chase once-novel states well after they were stumbled into. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multi step plans after every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the weighting between reward and entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to just Dreamer as a proof of concept. Our method finishes the Miniworld procedurally generated mazes 50% faster than base Dreamer at convergence and the policy trained in imagination converges in only 60% of the environment steps that base Dreamer needs.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2505.15589.pdf' target='_blank'>https://arxiv.org/pdf/2505.15589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlos Stein Brito, Daniel McNamee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15589">World Models as Reference Trajectories for Rapid Motor Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2505.13696.pdf' target='_blank'>https://arxiv.org/pdf/2505.13696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizhan He, Maxime Daigle, Pouya Bashivan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13696">Building spatial world models from sparse transitional episodic memories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many animals possess a remarkable capacity to rapidly construct flexible mental models of their environments. These world models are crucial for ethologically relevant behaviors such as navigation, exploration, and planning. The ability to form episodic memories and make inferences based on these sparse experiences is believed to underpin the efficiency and adaptability of these models in the brain. Here, we ask: Can a neural network learn to construct a spatial model of its surroundings from sparse and disjoint episodic memories? We formulate the problem in a simulated world and propose a novel framework, the Episodic Spatial World Model (ESWM), as a potential answer. We show that ESWM is highly sample-efficient, requiring minimal observations to construct a robust representation of the environment. It is also inherently adaptive, allowing for rapid updates when the environment changes. In addition, we demonstrate that ESWM readily enables near-optimal strategies for exploring novel environments and navigating between arbitrary points, all without the need for additional training.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2505.11772.pdf' target='_blank'>https://arxiv.org/pdf/2505.11772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Chen, Youngmin Ko, Zeyu Zhang, Catherine Cho, Sunny Chung, Mauro GiuffrÃ©, Dennis L. Shung, Bradly C. Stadie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11772">LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2505.03176.pdf' target='_blank'>https://arxiv.org/pdf/2505.03176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hafez Ghaemi, Eilif Muller, Shahab Bakhtiari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03176">seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current self-supervised algorithms commonly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by enforcing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm often limits the flexibility of learned representations for downstream adaptation by creating performance trade-offs between high-level invariance-demanding tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we proposes \emph{seq-JEPA}, a world modeling framework that introduces architectural inductive biases into joint-embedding predictive architectures to resolve this trade-off. Without relying on dual equivariance predictors or loss terms, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to specified transformations and another invariant to them. To do so, our model processes short sequences of different views (observations) of inputs. Each encoded view is concatenated with an embedding of the relative transformation (action) that produces the next observation in the sequence. These view-action pairs are passed through a transformer encoder that outputs an aggregate representation. A predictor head then conditions this aggregate representation on the upcoming action to predict the representation of the next observation. Empirically, seq-JEPA demonstrates strong performance on both equivariant and invariant benchmarks without sacrificing one for the other. Furthermore, it excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2504.19077.pdf' target='_blank'>https://arxiv.org/pdf/2504.19077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mitchell Goff, Greg Hogan, George Hotz, Armand du Parc Locmaria, Kacper Raczy, Harald SchÃ¤fer, Adeeb Shihadeh, Weixing Zhang, Yassine Yousfi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19077">Learning to Drive from a World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most self-driving systems rely on hand-coded perception outputs and engineered driving rules. Learning directly from human driving data with an end-to-end method can allow for a training architecture that is simpler and scales well with compute and data.
  In this work, we propose an end-to-end training architecture that uses real driving data to train a driving policy in an on-policy simulator. We show two different methods of simulation, one with reprojective simulation and one with a learned world model. We show that both methods can be used to train a policy that learns driving behavior without any hand-coded driving rules. We evaluate the performance of these policies in a closed-loop simulation and when deployed in a real-world advanced driver-assistance system.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2504.15125.pdf' target='_blank'>https://arxiv.org/pdf/2504.15125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruben Laukkonen, Fionn Inglis, Shamil Chandaria, Lars Sandved-Smith, Edmundo Lopez-Sola, Jakob Hohwy, Jonathan Gold, Adam Elwood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15125">Contemplative Artificial Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence (AI) improves, traditional alignment strategies may falter in the face of unpredictable self-improvement, hidden subgoals, and the sheer complexity of intelligent systems. Inspired by contemplative wisdom traditions, we show how four axiomatic principles can instil a resilient Wise World Model in AI systems. First, mindfulness enables self-monitoring and recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal fixation and relaxes rigid priors. Third, non-duality dissolves adversarial self-other boundaries. Fourth, boundless care motivates the universal reduction of suffering. We find that prompting AI to reflect on these principles improves performance on the AILuminate Benchmark (d=.96) and boosts cooperation and joint-reward on the Prisoner's Dilemma task (d=7+). We offer detailed implementation strategies at the level of architectures, constitutions, and reinforcement on chain-of-thought. For future systems, active inference may offer the self-organizing and dynamic coupling capabilities needed to enact Contemplative AI in embodied agents.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2504.11419.pdf' target='_blank'>https://arxiv.org/pdf/2504.11419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Jin, Liu Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11419">Embodied World Models Emerge from Navigational Task in Open-Ended Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial reasoning in partially observable environments has often been approached through passive predictive models, yet theories of embodied cognition suggest that genuinely useful representations arise only when perception is tightly coupled to action. Here we ask whether a recurrent agent, trained solely by sparse rewards to solve procedurally generated planar mazes, can autonomously internalize metric concepts such as direction, distance and obstacle layout. After training, the agent consistently produces near-optimal paths in unseen mazes, behavior that hints at an underlying spatial model. To probe this possibility, we cast the closed agent-environment loop as a hybrid dynamical system, identify stable limit cycles in its state space, and characterize behavior with a Ridge Representation that embeds whole trajectories into a common metric space. Canonical correlation analysis exposes a robust linear alignment between neural and behavioral manifolds, while targeted perturbations of the most informative neural dimensions sharply degrade navigation performance. Taken together, these dynamical, representational, and causal signatures show that sustained sensorimotor interaction is sufficient for the spontaneous emergence of compact, embodied world models, providing a principled path toward interpretable and transferable navigation policies.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2504.07095.pdf' target='_blank'>https://arxiv.org/pdf/2504.07095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenjie Hao, Weyl Lu, Yifan Xu, Yubei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07095">Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2504.04608.pdf' target='_blank'>https://arxiv.org/pdf/2504.04608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fernando Rosas, Alexander Boyd, Manuel Baltieri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04608">AI in a vat: Fundamental limits of efficient world modelling for agent sandboxing and interpretability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work proposes using world models to generate controlled virtual environments in which AI agents can be tested before deployment to ensure their reliability and safety. However, accurate world models often have high computational demands that can severely restrict the scope and depth of such assessments. Inspired by the classic `brain in a vat' thought experiment, here we investigate ways of simplifying world models that remain agnostic to the AI agent under evaluation. By following principles from computational mechanics, our approach reveals a fundamental trade-off in world model construction between efficiency and interpretability, demonstrating that no single world model can optimise all desirable characteristics. Building on this trade-off, we identify procedures to build world models that either minimise memory requirements, delineate the boundaries of what is learnable, or allow tracking causes of undesirable outcomes. In doing so, this work establishes fundamental limits in world modelling, leading to actionable guidelines that inform core design choices related to effective agent evaluation.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2503.21232.pdf' target='_blank'>https://arxiv.org/pdf/2503.21232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Bheemaiah, Seungyong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21232">Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inability of autonomous vehicles (AVs) to infer the material properties of obstacles limits their decision-making capacity. While AVs rely on sensor systems such as cameras, LiDAR, and radar to detect obstacles, this study suggests combining sensors with a knowledge graph (KG)-based world model to improve AVs' comprehension of physical material qualities. Beyond sensor data, AVs can infer qualities such as malleability, density, and elasticity using a semantic KG that depicts the relationships between obstacles and their attributes. Using the CARLA autonomous driving simulator, we evaluated AV performance with and without KG integration. The findings demonstrate that the KG-based method improves obstacle management, which allows AVs to use material qualities to make better decisions about when to change lanes or apply emergency braking. For example, the KG-integrated AV changed lanes for hard impediments like traffic cones and successfully avoided collisions with flexible items such as plastic bags by passing over them. Compared to the control system, the KG framework demonstrated improved responsiveness to obstacles by resolving conflicting sensor data, causing emergency stops for 13.3% more cases. In addition, our method exhibits a 6.6% higher success rate in lane-changing maneuvers in experimental scenarios, particularly for larger, high-impact obstacles. While we focus particularly on autonomous driving, our work demonstrates the potential of KG-based world models to improve decision-making in embodied AI systems and scale to other domains, including robotics, healthcare, and environmental simulation.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2503.21047.pdf' target='_blank'>https://arxiv.org/pdf/2503.21047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremias Ferrao, Rafael Cunha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21047">World Model Agents with Change-Based Intrinsic Motivation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse reward environments pose a significant challenge for reinforcement learning due to the scarcity of feedback. Intrinsic motivation and transfer learning have emerged as promising strategies to address this issue. Change Based Exploration Transfer (CBET), a technique that combines these two approaches for model-free algorithms, has shown potential in addressing sparse feedback but its effectiveness with modern algorithms remains understudied. This paper provides an adaptation of CBET for world model algorithms like DreamerV3 and compares the performance of DreamerV3 and IMPALA agents, both with and without CBET, in the sparse reward environments of Crafter and Minigrid. Our tabula rasa results highlight the possibility of CBET improving DreamerV3's returns in Crafter but the algorithm attains a suboptimal policy in Minigrid with CBET further reducing returns. In the same vein, our transfer learning experiments show that pre-training DreamerV3 with intrinsic rewards does not immediately lead to a policy that maximizes extrinsic rewards in Minigrid. Overall, our results suggest that CBET provides a positive impact on DreamerV3 in more complex environments like Crafter but may be detrimental in environments like Minigrid. In the latter case, the behaviours promoted by CBET in DreamerV3 may not align with the task objectives of the environment, leading to reduced returns and suboptimal policies.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2503.19815.pdf' target='_blank'>https://arxiv.org/pdf/2503.19815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Miconi, Kevin McKee, Yicong Zheng, Jed McCaleb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19815">Thinking agents for zero-shot generalization to qualitatively novel tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent organisms can solve truly novel problems which they have never encountered before, either in their lifetime or their evolution. An important component of this capacity is the ability to ``think'', that is, to mentally manipulate objects, concepts and behaviors in order to plan and evaluate possible solutions to novel problems, even without environment interaction. To generate problems that are truly qualitatively novel, while still solvable zero-shot (by mental simulation), we use the combinatorial nature of environments: we train the agent while withholding a specific combination of the environment's elements. The novel test task, based on this combination, is thus guaranteed to be truly novel, while still mentally simulable since the agent has been exposed to each individual element (and their pairwise interactions) during training. We propose a method to train agents endowed with world models to make use their mental simulation abilities, by selecting tasks based on the difference between the agent's pre-thinking and post-thinking performance. When tested on the novel, withheld problem, the resulting agent successfully simulated alternative scenarios and used the resulting information to guide its behavior in the actual environment, solving the novel task in a single real-environment trial (zero-shot).
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2503.02552.pdf' target='_blank'>https://arxiv.org/pdf/2503.02552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Domberg, Georg Schildbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02552">World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based controllers are often purposefully kept out of real-world applications due to concerns about their safety and reliability. We explore how state-of-the-art world models in Model-Based Reinforcement Learning can be utilized beyond the training phase to ensure a deployed policy only operates within regions of the state-space it is sufficiently familiar with. This is achieved by continuously monitoring discrepancies between a world model's predictions and observed system behavior during inference. It allows for triggering appropriate measures, such as an emergency stop, once an error threshold is surpassed. This does not require any task-specific knowledge and is thus universally applicable. Simulated experiments on established robot control tasks show the effectiveness of this method, recognizing changes in local robot geometry and global gravitational magnitude. Real-world experiments using an agile quadcopter further demonstrate the benefits of this approach by detecting unexpected forces acting on the vehicle. These results indicate how even in new and adverse conditions, safe and reliable operation of otherwise unpredictable learning-based controllers can be achieved.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2503.01255.pdf' target='_blank'>https://arxiv.org/pdf/2503.01255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyi Hu, Qiao Sun, Bailin He, Haojie Liu, Xueyi Zhang, Chunpeng lu, Jiangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01255">Impact of Static Friction on Sim2Real in Robotic Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotic reinforcement learning, the Sim2Real gap remains a critical challenge. However, the impact of Static friction on Sim2Real has been underexplored. Conventional domain randomization methods typically exclude Static friction from their parameter space. In our robotic reinforcement learning task, such conventional domain randomization approaches resulted in significantly underperforming real-world models. To address this Sim2Real challenge, we employed Actuator Net as an alternative to conventional domain randomization. While this method enabled successful transfer to flat-ground locomotion, it failed on complex terrains like stairs. To further investigate physical parameters affecting Sim2Real in robotic joints, we developed a control-theoretic joint model and performed systematic parameter identification. Our analysis revealed unexpectedly high friction-torque ratios in our robotic joints. To mitigate its impact, we implemented Static friction-aware domain randomization for Sim2Real. Recognizing the increased training difficulty introduced by friction modeling, we proposed a simple and novel solution to reduce learning complexity. To validate this approach, we conducted comprehensive Sim2Sim and Sim2Real experiments comparing three methods: conventional domain randomization (without Static friction), Actuator Net, and our Static friction-aware domain randomization. All experiments utilized the Rapid Motor Adaptation (RMA) algorithm. Results demonstrated that our method achieved superior adaptive capabilities and overall performance.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2502.13200.pdf' target='_blank'>https://arxiv.org/pdf/2502.13200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alana Santana, Paula P. Costa, Esther L. Colombini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13200">Learning To Explore With Predictive World Model Via Self-Supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous artificial agents must be able to learn behaviors in complex environments without humans to design tasks and rewards. Designing these functions for each environment is not feasible, thus, motivating the development of intrinsic reward functions. In this paper, we propose using several cognitive elements that have been neglected for a long time to build an internal world model for an intrinsically motivated agent. Our agent performs satisfactory iterations with the environment, learning complex behaviors without needing previously designed reward functions. We used 18 Atari games to evaluate what cognitive skills emerge in games that require reactive and deliberative behaviors. Our results show superior performance compared to the state-of-the-art in many test cases with dense and sparse rewards.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2502.04249.pdf' target='_blank'>https://arxiv.org/pdf/2502.04249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Walters, Rafael Kaufmann, Justice Sefas, Thomas Kopinski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04249">Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the Free Energy Principle as a foundation for measuring risk in agentic and multi-agent systems. From these principles we introduce a Cumulative Risk Exposure metric that is flexible to differing contexts and needs. We contrast this to other popular theories for safe AI that hinge on massive amounts of data or describing arbitrarily complex world models. In our framework, stakeholders need only specify their preferences over system outcomes, providing straightforward and transparent decision rules for risk governance and mitigation. This framework naturally accounts for uncertainty in both world model and preference model, allowing for decision-making that is epistemically and axiologically humble, parsimonious, and future-proof. We demonstrate this novel approach in a simplified autonomous vehicle environment with multi-agent vehicles whose driving policies are mediated by gatekeepers that evaluate, in an online fashion, the risk to the collective safety in their neighborhood, and intervene through each vehicle's policy when appropriate. We show that the introduction of gatekeepers in an AV fleet, even at low penetration, can generate significant positive externalities in terms of increased system safety.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2501.14622.pdf' target='_blank'>https://arxiv.org/pdf/2501.14622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Vujinovic, Aleksandar Kovacevic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14622">ACT-JEPA: Novel Joint-Embedding Predictive Architecture for Efficient Policy Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning efficient representations for decision-making policies is a challenge in imitation learning (IL). Current IL methods require expert demonstrations, which are expensive to collect. Consequently, they often have underdeveloped world models. Self-supervised learning (SSL) offers an alternative by allowing models to learn from diverse, unlabeled data, including failures. However, SSL methods often operate in raw input space, making them inefficient. In this work, we propose ACT-JEPA, a novel architecture that integrates IL and SSL to enhance policy representations. We train a policy to predict (1) action sequences and (2) abstract observation sequences. The first objective uses action chunking to improve action prediction and reduce compounding errors. The second objective extends this idea of chunking by predicting abstract observation sequences. We utilize Joint-Embedding Predictive Architecture to predict in abstract representation space, allowing the model to filter out irrelevant details, improve efficiency, and develop a robust world model. Our experiments show that ACT-JEPA improves the quality of representations by learning temporal environment dynamics. Additionally, the model's ability to predict abstract observation sequences results in representations that effectively generalize to action sequence prediction. ACT-JEPA performs on par with established baselines across a range of decision-making tasks.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2411.17438.pdf' target='_blank'>https://arxiv.org/pdf/2411.17438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruben van Bergen, Justus HÃ¼botter, Pablo Lanillos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17438">Object-centric proto-symbolic behavioural reasoning from pixels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2411.16075.pdf' target='_blank'>https://arxiv.org/pdf/2411.16075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shogo Ohmae, Keiko Ohmae
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16075">The brain versus AI: World-model-based versatile circuit computation underlying diverse functions in the neocortex and cerebellum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI's significant recent advances using general-purpose circuit computations offer a potential window into how the neocortex and cerebellum of the brain are able to achieve a diverse range of functions across sensory, cognitive, and motor domains, despite their uniform circuit structures. However, comparing the brain and AI is challenging unless clear similarities exist, and past reviews have been limited to comparison of brain-inspired vision AI and the visual neocortex. Here, to enable comparisons across diverse functional domains, we subdivide circuit computation into three elements -- circuit structure, input/outputs, and the learning algorithm -- and evaluate the similarities for each element. With this novel approach, we identify wide-ranging similarities and convergent evolution in the brain and AI, providing new insights into key concepts in neuroscience. Furthermore, inspired by processing mechanisms of AI, we propose a new theory that integrates established neuroscience theories, particularly the theories of internal models and the mirror neuron system. Both the neocortex and cerebellum predict future world events from past information and learn from prediction errors, thereby acquiring models of the world. These models enable three core processes: (1) Prediction -- generating future information, (2) Understanding -- interpreting the external world via compressed and abstracted sensory information, and (3) Generation -- repurposing the future-information generation mechanism to produce other types of outputs. The universal application of these processes underlies the ability of the neocortex and cerebellum to accomplish diverse functions with uniform circuits. Our systematic approach, insights, and theory promise groundbreaking advances in understanding the brain.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2411.12304.pdf' target='_blank'>https://arxiv.org/pdf/2411.12304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuya Horibe, Naoto Yoshida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12304">Emergence of Implicit World Models from Mortal Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We discuss the possibility of world models and active exploration as emergent properties of open-ended behavior optimization in autonomous agents. In discussing the source of the open-endedness of living things, we start from the perspective of biological systems as understood by the mechanistic approach of theoretical biology and artificial life. From this perspective, we discuss the potential of homeostasis in particular as an open-ended objective for autonomous agents and as a general, integrative extrinsic motivation. We then discuss the possibility of implicitly acquiring a world model and active exploration through the internal dynamics of a network, and a hypothetical architecture for this, by combining meta-reinforcement learning, which assumes domain adaptation as a system that achieves robust homeostasis.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2411.10171.pdf' target='_blank'>https://arxiv.org/pdf/2411.10171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anant Garg, K Madhava Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10171">Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World Model-based Reinforcement Learning (WMRL) enables sample efficient policy learning by reducing the need for online interactions which can potentially be costly and unsafe, especially for autonomous driving. However, existing world models often suffer from low prediction fidelity and compounding one-step errors, leading to policy degradation over long horizons. Additionally, traditional RL policies, often deterministic or single Gaussian-based, fail to capture the multi-modal nature of decision-making in complex driving scenarios. To address these challenges, we propose Imagine-2-Drive, a novel WMRL framework that integrates a high-fidelity world model with a multi-modal diffusion-based policy actor. It consists of two key components: DiffDreamer, a diffusion-based world model that generates future observations simultaneously, mitigating error accumulation, and DPA (Diffusion Policy Actor), a diffusion-based policy that models diverse and multi-modal trajectory distributions. By training DPA within DiffDreamer, our method enables robust policy learning with minimal online interactions. We evaluate our method in CARLA using standard driving benchmarks and demonstrate that it outperforms prior world model baselines, improving Route Completion and Success Rate by 15% and 20% respectively.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2410.07525.pdf' target='_blank'>https://arxiv.org/pdf/2410.07525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Fang, Guiliang Liu, Wei Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07525">Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical decisions and treatment, such as excessive dosages or abrupt changes, often due to agents overlooking common-sense constraints. Consequently, Constrained Reinforcement Learning (CRL) is a natural choice for safe decisions. However, specifying the exact cost function is inherently difficult in healthcare. Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising approach that infers constraints from expert demonstrations. ICRL algorithms model Markovian decisions in an interactive environment. These settings do not align with the practical requirement of a decision-making system in healthcare, where decisions rely on historical treatment recorded in an offline dataset. To tackle these issues, we propose the Constraint Transformer (CT). Specifically, 1) we utilize a causal attention mechanism to incorporate historical decisions and observations into the constraint modeling, while employing a Non-Markovian layer for weighted constraints to capture critical states. 2) A generative world model is used to perform exploratory data augmentation, enabling offline RL methods to simulate unsafe decision sequences. In multiple medical scenarios, empirical results demonstrate that CT can capture unsafe states and achieve strategies that approximate lower mortality rates, reducing the occurrence probability of unsafe behaviors.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2409.10553.pdf' target='_blank'>https://arxiv.org/pdf/2409.10553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kirill Krinkin, Tatiana Berlenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10553">"Flipped" University: LLM-Assisted Lifelong Learning Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of artificial intelligence technologies, particularly Large Language Models (LLMs), has revolutionized the landscape of lifelong learning. This paper introduces a conceptual framework for a self-constructed lifelong learning environment supported by LLMs. It highlights the inadequacies of traditional education systems in keeping pace with the rapid deactualization of knowledge and skills. The proposed framework emphasizes the transformation from institutionalized education to personalized, self-driven learning. It leverages the natural language capabilities of LLMs to provide dynamic and adaptive learning experiences, facilitating the creation of personal intellectual agents that assist in knowledge acquisition. The framework integrates principles of lifelong learning, including the necessity of building personal world models, the dual modes of learning (training and exploration), and the creation of reusable learning artifacts. Additionally, it underscores the importance of curiosity-driven learning and reflective practices in maintaining an effective learning trajectory. The paper envisions the evolution of educational institutions into "flipped" universities, focusing on supporting global knowledge consistency rather than merely structuring and transmitting knowledge.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2409.01427.pdf' target='_blank'>https://arxiv.org/pdf/2409.01427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianci Gao, Konstantin A. Neusypin, Dmitry D. Dmitriev, Bo Yang, Shengren Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01427">Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On policy reinforcement learning (RL) methods such as PPO are attractive for continuous control but suffer from poor sample efficiency in costly, high dimensional settings. We present a strictly on policy framework that treats a conditional diffusion model as an adaptable action prior rather than a policy or world model. The prior is pre trained on logged data and used online only at sampling time to propose actions at current on policy states. Two lightweight mechanisms - value guided proposal generation (energy re weighting and in process gradient guidance) and a soft prior KL - regularize the actor via a small auxiliary imitation loss while keeping all PPO updates strictly on on-policy rollouts. To adapt the prior without heavy compute, we apply parameter efficient tuning (PET) that updates only adapters/LoRA, yielding a dual proximal view: policy KL is constrained by PPO and prior KL by PET. Across eight MuJoCo tasks under a shared 1.0M step budget, our method improves early learning (ALC@40) in 3/4 settings and matches or exceeds final return on 6/8 tasks with only 15-30% wall clock overhead. Ablations show that freezing the prior degrades performance and removing value guidance slows early learning; t SNE analyses confirm that value guidance concentrates proposals in high Q regions. Results indicate that an adaptable diffusion action prior is a practical way to boost on policy PPO under tight interaction budgets.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2408.14915.pdf' target='_blank'>https://arxiv.org/pdf/2408.14915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baran Hashemi, Roderic G. Corominas, Alessandro Giacchetto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14915">Can Transformers Do Enumerative Geometry?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How can Transformers model and learn enumerative geometry? What is a robust procedure for using Transformers in abductive knowledge discovery within a mathematician-machine collaboration? In this work, we introduce a Transformer-based approach to computational enumerative geometry, specifically targeting the computation of $Ï$-class intersection numbers on the moduli space of curves. By reformulating the problem as a continuous optimization task, we compute intersection numbers across a wide value range from $10^{-45}$ to $10^{45}$. To capture the recursive nature inherent in these intersection numbers, we propose the Dynamic Range Activator (DRA), a new activation function that enhances the Transformer's ability to model recursive patterns and handle severe heteroscedasticity. Given precision requirements for computing the intersections, we quantify the uncertainty of the predictions using Conformal Prediction with a dynamic sliding window adaptive to the partitions of equivalent number of marked points. To the best of our knowledge, there has been no prior work on modeling recursive functions with such a high-variance and factorial growth. Beyond simply computing intersection numbers, we explore the enumerative "world-model" of Transformers. Our interpretability analysis reveals that the network is implicitly modeling the Virasoro constraints in a purely data-driven manner. Moreover, through abductive hypothesis testing, probing, and causal inference, we uncover evidence of an emergent internal representation of the the large-genus asymptotic of $Ï$-class intersection numbers. These findings suggest that the network internalizes the parameters of the asymptotic closed-form and the polynomiality phenomenon of intersection numbers in a non-linear manner. This opens up new possibilities in inferring asymptotic closed-form expressions directly from limited amount of data.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2406.17837.pdf' target='_blank'>https://arxiv.org/pdf/2406.17837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Menary, Samuel Kaski, Andre Freitas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17837">Transformer Normalisation Layers and the Independence of Semantic Subspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works have shown that transformers can solve contextual reasoning tasks by internally executing computational graphs called circuits. Circuits often use attention to logically match information from subspaces of the representation, e.g. using position-in-sequence to identify the previous token. In this work, we consider a semantic subspace to be any independent subspace of the latent representation that can fully determine an attention distribution. We show that Pre-Norm, the placement of normalisation layer used by state-of-the-art transformers, violates this ability unless the model learns a strict representation structure of orthogonal spheres. This is because it causes linear subspaces to interfere through their common normalisation factor. Theoretically, we analyse circuit stability by modelling this interference as random noise on the $L_2$-norms of the query/key/value vectors, predicting a phenomenon of circuit collapse when sparse-attention shifts to a different token. Empirically, we investigate the sensitivity of real-world models trained for mathematical addition, observing a 1% rate of circuit collapse when the norms are artificially perturbed by $\lesssim$10%. We contrast Pre-Norm with QKV-Norm, which places normalisation after the attention head's linear operators. Theoretically this relaxes the representational constraints. Empirically we observe comparable in-distribution but worse out-of-distribution performance.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2406.13600.pdf' target='_blank'>https://arxiv.org/pdf/2406.13600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edan Toledo, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13600">CoDreamer: Communication-Based Decentralised World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sample efficiency is a critical challenge in reinforcement learning. Model-based RL has emerged as a solution, but its application has largely been confined to single-agent scenarios. In this work, we introduce CoDreamer, an extension of the Dreamer algorithm for multi-agent environments. CoDreamer leverages Graph Neural Networks for a two-level communication system to tackle challenges such as partial observability and inter-agent cooperation. Communication is separately utilised within the learned world models and within the learned policies of each agent to enhance modelling and task-solving. We show that CoDreamer offers greater expressive power than a naive application of Dreamer, and we demonstrate its superiority over baseline methods across various multi-agent environments.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2406.00483.pdf' target='_blank'>https://arxiv.org/pdf/2406.00483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin Schiewer, Anand Subramoney, Laurenz Wiskott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00483">Exploring the limits of Hierarchical World Models in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hierarchical model-based reinforcement learning (HMBRL) aims to combine the benefits of better sample efficiency of model based reinforcement learning (MBRL) with the abstraction capability of hierarchical reinforcement learning (HRL) to solve complex tasks efficiently. While HMBRL has great potential, it still lacks wide adoption. In this work we describe a novel HMBRL framework and evaluate it thoroughly. To complement the multi-layered decision making idiom characteristic for HRL, we construct hierarchical world models that simulate environment dynamics at various levels of temporal abstraction. These models are used to train a stack of agents that communicate in a top-down manner by proposing goals to their subordinate agents. A significant focus of this study is the exploration of a static and environment agnostic temporal abstraction, which allows concurrent training of models and agents throughout the hierarchy. Unlike most goal-conditioned H(MB)RL approaches, it also leads to comparatively low dimensional abstract actions. Although our HMBRL approach did not outperform traditional methods in terms of final episode returns, it successfully facilitated decision making across two levels of abstraction using compact, low dimensional abstract actions. A central challenge in enhancing our method's performance, as uncovered through comprehensive experimentation, is model exploitation on the abstract level of our world model stack. We provide an in depth examination of this issue, discussing its implications for the field and suggesting directions for future research to overcome this challenge. By sharing these findings, we aim to contribute to the broader discourse on refining HMBRL methodologies and to assist in the development of more effective autonomous learning systems for complex decision-making environments.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2405.19878.pdf' target='_blank'>https://arxiv.org/pdf/2405.19878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Fang, Tian Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19878">Learning from Random Demonstrations: Offline Reinforcement Learning with Importance-Sampled Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models such as diffusion have been employed as world models in offline reinforcement learning to generate synthetic data for more effective learning. Existing work either generates diffusion models one-time prior to training or requires additional interaction data to update it. In this paper, we propose a novel approach for offline reinforcement learning with closed-loop policy evaluation and world-model adaptation. It iteratively leverages a guided diffusion world model to directly evaluate the offline target policy with actions drawn from it, and then performs an importance-sampled world model update to adaptively align the world model with the updated policy. We analyzed the performance of the proposed method and provided an upper bound on the return gap between our method and the real environment under an optimal policy. The result sheds light on various factors affecting learning performance. Evaluations in the D4RL environment show significant improvement over state-of-the-art baselines, especially when only random or medium-expertise demonstrations are available -- thus requiring improved alignment between the world model and offline policy evaluation.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2404.17350.pdf' target='_blank'>https://arxiv.org/pdf/2404.17350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Roshdi, Julian Petzold, Mostafa Wahby, Hussein Ebrahim, Mladen Berekovic, Heiko Hamann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17350">On the Road to Clarity: Exploring Explainable AI for World Models in a Driver Assistance System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Autonomous Driving (AD) transparency and safety are paramount, as mistakes are costly. However, neural networks used in AD systems are generally considered black boxes. As a countermeasure, we have methods of explainable AI (XAI), such as feature relevance estimation and dimensionality reduction. Coarse graining techniques can also help reduce dimensionality and find interpretable global patterns. A specific coarse graining method is Renormalization Groups from statistical physics. It has previously been applied to Restricted Boltzmann Machines (RBMs) to interpret unsupervised learning. We refine this technique by building a transparent backbone model for convolutional variational autoencoders (VAE) that allows mapping latent values to input features and has performance comparable to trained black box VAEs. Moreover, we propose a custom feature map visualization technique to analyze the internal convolutional layers in the VAE to explain internal causes of poor reconstruction that may lead to dangerous traffic scenarios in AD applications. In a second key contribution, we propose explanation and evaluation techniques for the internal dynamics and feature relevance of prediction networks. We test a long short-term memory (LSTM) network in the computer vision domain to evaluate the predictability and in future applications potentially safety of prediction models. We showcase our methods by analyzing a VAE-LSTM world model that predicts pedestrian perception in an urban traffic situation.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2404.09557.pdf' target='_blank'>https://arxiv.org/pdf/2404.09557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Fu, Jochen Seemann, Caspar Hanselaar, Tim Beurskens, Andrei Terechko, Emilia Silvas, Maurice Heemels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09557">Characterization and Mitigation of Insufficiencies in Automated Driving Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated Driving (AD) systems have the potential to increase safety, comfort and energy efficiency. Recently, major automotive companies have started testing and validating AD systems (ADS) on public roads. Nevertheless, the commercial deployment and wide adoption of ADS have been moderate, partially due to system functional insufficiencies (FI) that undermine passenger safety and lead to hazardous situations on the road. FIs are defined in ISO 21448 Safety Of The Intended Functionality (SOTIF). FIs are insufficiencies in sensors, actuators and algorithm implementations, including neural networks and probabilistic calculations. Examples of FIs in ADS include inaccurate ego-vehicle localization on the road, incorrect prediction of a cyclist maneuver, unreliable detection of a pedestrian, etc.
  The main goal of our study is to formulate a generic architectural design pattern, which is compatible with existing methods and ADS, to improve FI mitigation and enable faster commercial deployment of ADS. First, we studied the 2021 autonomous vehicles disengagement reports published by the California Department of Motor Vehicles (DMV). The data clearly show that disengagements are five times more often caused by FIs rather than by system faults. We then made a comprehensive list of insufficiencies and their characteristics by analyzing over 10 hours of publicly available road test videos. In particular, we identified insufficiency types in four major categories: world model, motion plan, traffic rule, and operational design domain. The insufficiency characterization helps making the SOTIF analyses of triggering conditions more systematic and comprehensive.
  Based on our FI characterization, simulation experiments and literature survey, we define a novel generic architectural design pattern Daruma to dynamically select the channel that is least likely to have a FI at the moment.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2403.13827.pdf' target='_blank'>https://arxiv.org/pdf/2403.13827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Krayani, Khalid Khan, Lucio Marcenaro, Mario Marchese, Carlo Regazzoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13827">Self-Supervised Path Planning in UAV-aided Wireless Networks based on Active Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel self-supervised path-planning method for UAV-aided networks. First, we employed an optimizer to solve training examples offline and then used the resulting solutions as demonstrations from which the UAV can learn the world model to understand the environment and implicitly discover the optimizer's policy. UAV equipped with the world model can make real-time autonomous decisions and engage in online planning using active inference. During planning, UAV can score different policies based on the expected surprise, allowing it to choose among alternative futures. Additionally, UAV can anticipate the outcomes of its actions using the world model and assess the expected surprise in a self-supervised manner. Our method enables quicker adaptation to new situations and better performance than traditional RL, leading to broader generalizability.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2401.00057.pdf' target='_blank'>https://arxiv.org/pdf/2401.00057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kandan Ramakrishnan, R. James Cotton, Xaq Pitkow, Andreas S. Tolias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00057">Generalization properties of contrastive world models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work on object-centric world models aim to factorize representations in terms of objects in a completely unsupervised or self-supervised manner. Such world models are hypothesized to be a key component to address the generalization problem. While self-supervision has shown improved performance however, OOD generalization has not been systematically and explicitly tested. In this paper, we conduct an extensive study on the generalization properties of contrastive world model. We systematically test the model under a number of different OOD generalization scenarios such as extrapolation to new object attributes, introducing new conjunctions or new attributes. Our experiments show that the contrastive world model fails to generalize under the different OOD tests and the drop in performance depends on the extent to which the samples are OOD. When visualizing the transition updates and convolutional feature maps, we observe that any changes in object attributes (such as previously unseen colors, shapes, or conjunctions of color and shape) breaks down the factorization of object representations. Overall, our work highlights the importance of object-centric representations for generalization and current models are limited in their capacity to learn such representations required for human-level generalization.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2312.05361.pdf' target='_blank'>https://arxiv.org/pdf/2312.05361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quentin RV. Ferry, Joshua Ching, Takashi Kawai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05361">Emergence and Function of Abstract Representations in Self-Supervised Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence relies in part on our brains' ability to create abstract mental models that succinctly capture the hidden blueprint of our reality. Such abstract world models notably allow us to rapidly navigate novel situations by generalizing prior knowledge, a trait deep learning systems have historically struggled to replicate. However, the recent shift from supervised to self-supervised objectives, combined with expressive transformer-based architectures, have yielded powerful foundation models that appear to learn versatile representations that can support a wide range of downstream tasks. This promising development raises the intriguing possibility of such models developing in silico abstract world models. We test this hypothesis by studying the inner workings of small-scale transformers trained to reconstruct partially masked visual scenes generated from a simple blueprint. We show that the network develops intermediate abstract representations, or abstractions, that encode all semantic features of the dataset. These abstractions manifest as low-dimensional manifolds where the embeddings of semantically related tokens transiently converge, thus allowing for the generalization of downstream computations. Using precise manipulation experiments, we demonstrate that abstractions are central to the network's decision-making process. Our research also suggests that these abstractions are compositionally structured, exhibiting features like contextual independence and part-whole relationships that mirror the compositional nature of the dataset. Finally, we introduce a Language-Enhanced Architecture (LEA) designed to encourage the network to articulate its computations. We find that LEA develops an abstraction-centric language that can be easily interpreted, allowing us to more readily access and steer the network's decision-making process.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2311.13335.pdf' target='_blank'>https://arxiv.org/pdf/2311.13335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Wang, Changlin Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13335">Quantum learning and essential cognition under the traction of meta-characteristics in an open world</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence has made significant progress in the Close World problem, being able to accurately recognize old knowledge through training and classification. However, AI faces significant challenges in the Open World problem, as it involves a new and unknown exploration journey. AI is not inherently proactive in exploration, and its challenge lies in not knowing how to approach and adapt to the unknown world. How do humans acquire knowledge of the unknown world. Humans identify new knowledge through intrinsic cognition. In the process of recognizing new colors, the cognitive cues are different from known color features and involve hue, saturation, brightness, and other characteristics. When AI encounters objects with different features in the new world, it faces another challenge: where are the distinguishing features between influential features of new and old objects? AI often mistakes a new world's brown bear for a known dog because it has not learned the differences in feature distributions between knowledge systems. This is because things in the new and old worlds have different units and dimensions for their features. This paper proposes an open-world model and elemental feature system that focuses on fundamentally recognizing the distribution differences in objective features between the new and old worlds. The quantum tunneling effect of learning ability in the new and old worlds is realized through the tractive force of meta-characteristic. The outstanding performance of the model system in learning new knowledge (using pedestrian re-identification datasets as an example) demonstrates that AI has acquired the ability to recognize the new world with an accuracy of $96.71\%$ at most and has gained the capability to explore new knowledge, similar to humans.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2311.01180.pdf' target='_blank'>https://arxiv.org/pdf/2311.01180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>K. de Vos, E. Torta, H. Bruyninckx, C. A. Lopez Martinez, M. J. G. van de Molengraft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01180">Automatic Configuration of Multi-Agent Model Predictive Controllers based on Semantic Graph World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a shared semantic map architecture to construct and configure Model Predictive Controllers (MPC) dynamically, that solve navigation problems for multiple robotic agents sharing parts of the same environment. The navigation task is represented as a sequence of semantically labeled areas in the map, that must be traversed sequentially, i.e. a route. Each semantic label represents one or more constraints on the robots' motion behaviour in that area. The advantages of this approach are: (i) an MPC-based motion controller in each individual robot can be (re-)configured, at runtime, with the locally and temporally relevant parameters; (ii) the application can influence, also at runtime, the navigation behaviour of the robots, just by adapting the semantic labels; and (iii) the robots can reason about their need for coordination, through analyzing over which horizon in time and space their routes overlap. The paper provides simulations of various representative situations, showing that the approach of runtime configuration of the MPC drastically decreases computation time, while retaining task execution performance similar to an approach in which each robot always includes all other robots in its MPC computations.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2310.07582.pdf' target='_blank'>https://arxiv.org/pdf/2310.07582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dean S. Hazineh, Zechen Zhang, Jeffery Chiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07582">Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2310.04276.pdf' target='_blank'>https://arxiv.org/pdf/2310.04276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilker Yildirim, L. A. Paul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04276">From task structures to world models: What do LLMs know?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In what sense does a large language model have knowledge? The answer to this question extends beyond the capabilities of a particular AI system, and challenges our assumptions about the nature of knowledge and intelligence. We answer by granting LLMs "instrumental knowledge"; knowledge defined by a certain set of abilities. We then ask how such knowledge is related to the more ordinary, "worldly" knowledge exhibited by human agents, and explore this in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge, and suggest such recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2308.09721.pdf' target='_blank'>https://arxiv.org/pdf/2308.09721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongcong Chen, Ting Zeng, Xingyue Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09721">A new solution and concrete implementation steps for Artificial General Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new approach to building a artificial general intelligence with self awareness, which includes: (1) a new method to implement attention mechanisms; (2) a way to give machines self-demands; (3) how to form a value evaluation system compatible with the network; (4) a way to create the world models; (5) how to realize a top-down, hierarchical thinking decision-making chain; (6) a way to achieve general decision-making and response capabilities; (7) a way for a machine to directly obtain human experience through language. In the paper, we first analyze some of the shortcomings of current LLMs (Large Language Model) and propose ideas for improvement. Then we analyze why our scheme can solve the above problems and provide detailed steps for implementing our scheme. In chapter 4, we have presented a step-by-step mplementation roadmap. And in chapter 5, we have presented a specific implementation demonstration. In chapter 6, we analyze the advantages and disadvantages of our scheme and propose further research directions. In this article, we have put forward how to create genuine artificial general intelligence step by step. It can handle data of all modalities in a unified form and can directly understand the experience that humans already possess through language, thus avoiding the problem that reinforcement learning is required for every decision-making process.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2307.09205.pdf' target='_blank'>https://arxiv.org/pdf/2307.09205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Feng, Sara Magliacane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09205">Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them in classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2306.04440.pdf' target='_blank'>https://arxiv.org/pdf/2306.04440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaesung Yoo, Fernanda de la Torre, Guangyu Robert Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04440">Dual policy as self-model for planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster inference than using model-free policy, promotes better exploration, and could learn a comprehensive understanding of its own behaviors, at the cost of distilling a new network apart from the model-free policy.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2306.02572.pdf' target='_blank'>https://arxiv.org/pdf/2306.02572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Dawid, Yann LeCun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02572">Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack Level 5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun's proposal, that is, in the hierarchical joint embedding predictive architecture (H-JEPA).
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2305.17198.pdf' target='_blank'>https://arxiv.org/pdf/2305.17198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Barde, Jakob Foerster, Derek Nowrouzezahrai, Amy Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17198">A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training multiple agents to coordinate is an essential problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to what we call the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two issues at which current offline MARL algorithms fail. Concretely, we reveal that the prevalent model-free methods are severely deficient and cannot handle coordination-intensive offline multi-agent tasks in either toy or MuJoCo domains. To address this setback, we emphasize the importance of inter-agent interactions and propose the very first model-based offline MARL method. Our resulting algorithm, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO) generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. This simple model-based solution solves the coordination-intensive offline tasks, significantly outperforming the prevalent model-free methods even under severe partial observability and with learned world models.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2305.14223.pdf' target='_blank'>https://arxiv.org/pdf/2305.14223.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Max Olan Smith, Michael P. Wellman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14223">Co-Learning Empirical Games and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Game-based decision-making involves reasoning over both world dynamics and strategic interactions among the agents. Typically, empirical models capturing these respective aspects are learned and used separately. We investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning. We demonstrate these benefits first independently, then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When compared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO is found to compute lower regret solutions on partially observable general-sum games. In our experiments, Dyna-PSRO also requires substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting player-game interaction data is a cost-limiting factor.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2305.04750.pdf' target='_blank'>https://arxiv.org/pdf/2305.04750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Shrestha, Chetan Reddy, Hanxi Wan, Yulun Zhuang, Ram Vasudevan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04750">Sense, Imagine, Act: Multimodal Perception Improves Model-Based Reinforcement Learning for Head-to-Head Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model-based reinforcement learning (MBRL) techniques have recently yielded promising results for real-world autonomous racing using high-dimensional observations. MBRL agents, such as Dreamer, solve long-horizon tasks by building a world model and planning actions by latent imagination. This approach involves explicitly learning a model of the system dynamics and using it to learn the optimal policy for continuous control over multiple timesteps. As a result, MBRL agents may converge to sub-optimal policies if the world model is inaccurate. To improve state estimation for autonomous racing, this paper proposes a self-supervised sensor fusion technique that combines egocentric LiDAR and RGB camera observations collected from the F1TENTH Gym. The zero-shot performance of MBRL agents is empirically evaluated on unseen tracks and against a dynamic obstacle. This paper illustrates that multimodal perception improves robustness of the world model without requiring additional training data. The resulting multimodal Dreamer agent safely avoided collisions and won the most races compared to other tested baselines in zero-shot head-to-head autonomous racing.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2304.00188.pdf' target='_blank'>https://arxiv.org/pdf/2304.00188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>GrÃ©goire Sergeant-Perthuis, Nils Ruet, David Rudrauf, Dimitri Ognibene, Yvain Tisserand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00188">Influence of the Geometry of the world model on Curiosity Based Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human spatial awareness, 3-D projective geometry structures information integration and action planning through perspective taking within an internal representation space. The way different perspectives are related and transform a world model defines a specific perception and imagination scheme. In mathematics, such collection of transformations corresponds to a 'group', whose 'actions' characterize the geometry of a space. Imbuing world models with a group structure may capture different agents' spatial awareness and affordance schemes. We used group action as a special class of policies for perspective-dependent control. We explored how such geometric structure impacts agents' behavior, comparing how the Euclidean versus projective groups act on epistemic value in active inference, drive curiosity, and exploration behaviors. We formally demonstrate and simulate how the groups induce distinct behaviors in a simple search task. The projective group's nonlinear magnification of information transformed epistemic value according to the choice of frame, generating behaviors of approach toward an object of interest. The projective group structure within the agent's world model contains the Projective Consciousness Model, which is know to capture key features of consciousness. On the other hand, the Euclidean group had no effect on epistemic value : no action was better than the initial idle state. In structuring a priori an agent's internal representation, we show how geometry can play a key role in information integration and action planning.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2303.10930.pdf' target='_blank'>https://arxiv.org/pdf/2303.10930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Wiedholz, Stefanie Wucherer, Simon Dietrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10930">Semantic 3D scene segmentation for robotic assembly process execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting robot programmes to changes in the environment is a well-known industry problem, and it is the reason why many tedious tasks are not automated in small and medium-sized enterprises (SMEs). A semantic world model of a robot's previously unknown environment created from point clouds is one way for these companies to automate assembly tasks that are typically performed by humans. The semantic segmentation of point clouds for robot manipulators or cobots in industrial environments has received little attention due to a lack of suitable datasets. This paper describes a pipeline for creating synthetic point clouds for specific use cases in order to train a model for point cloud semantic segmentation. We show that models trained with our data achieve high per-class accuracy (> 90%) for semantic point cloud segmentation on unseen real-world data. Our approach is applicable not only to the 3D camera used in training data generation but also to other depth cameras based on different technologies. The application tested in this work is a industry-related peg-in-the-hole process. With our approach the necessity of user assistance during a robot's commissioning can be reduced to a minimum.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2302.04798.pdf' target='_blank'>https://arxiv.org/pdf/2302.04798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreea Deac, ThÃ©ophane Weber, George Papamakarios
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04798">Equivariant MuZero</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning repeatedly succeeds in closed, well-defined domains such as games (Chess, Go, StarCraft). The next frontier is real-world scenarios, where setups are numerous and varied. For this, agents need to learn the underlying rules governing the environment, so as to robustly generalise to conditions that differ from those they were trained on. Model-based reinforcement learning algorithms, such as the highly successful MuZero, aim to accomplish this by learning a world model. However, leveraging a world model has not consistently shown greater generalisation capabilities compared to model-free alternatives. In this work, we propose improving the data efficiency and generalisation capabilities of MuZero by explicitly incorporating the symmetries of the environment in its world-model architecture. We prove that, so long as the neural networks used by MuZero are equivariant to a particular symmetry group acting on the environment, the entirety of MuZero's action-selection algorithm will also be equivariant to that group. We evaluate Equivariant MuZero on procedurally-generated MiniPacman and on Chaser from the ProcGen suite: training on a set of mazes, and then testing on unseen rotated versions, demonstrating the benefits of equivariance. Further, we verify that our performance improvements hold even when only some of the components of Equivariant MuZero obey strict equivariance, which highlights the robustness of our construction.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2301.03793.pdf' target='_blank'>https://arxiv.org/pdf/2301.03793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tatsuya Sakai, Takayuki Nagai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03793">Estimation of User's World Model Using Graph2vec</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To obtain advanced interaction between autonomous robots and users, robots should be able to distinguish their state space representations (i.e., world models). Herein, a novel method was proposed for estimating the user's world model based on queries. In this method, the agent learns the distributed representation of world models using graph2vec and generates concept activation vectors that represent the meaning of queries in the latent space. Experimental results revealed that the proposed method can estimate the user's world model more efficiently than the simple method of using the ``AND'' search of queries.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2211.13051.pdf' target='_blank'>https://arxiv.org/pdf/2211.13051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Frans, Phillip Isola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13051">Powderworld: A Platform for Understanding Generalization via Rich Task Distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2208.07462.pdf' target='_blank'>https://arxiv.org/pdf/2208.07462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Espuny DÃ­az, Patrick Morris, Guillem Perarnau, Oriol Serra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.07462">Speeding up random walk mixing by starting from a uniform vertex</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The theory of rapid mixing random walks plays a fundamental role in the study of modern randomised algorithms. Usually, the mixing time is measured with respect to the worst initial position. It is well known that the presence of bottlenecks in a graph hampers mixing and, in particular, starting inside a small bottleneck significantly slows down the diffusion of the walk in the first steps of the process. The average mixing time is defined to be the mixing time starting at a uniformly random vertex and hence is not sensitive to the slow diffusion caused by these bottlenecks.
  In this paper we provide a general framework to show logarithmic average mixing time for random walks on graphs with small bottlenecks. The framework is especially effective on certain families of random graphs with heterogeneous properties. We demonstrate its applicability on two random models for which the mixing time was known to be of order $(\log n)^2$, speeding up the mixing to order $\log n$. First, in the context of smoothed analysis on connected graphs, we show logarithmic average mixing time for randomly perturbed graphs of bounded degeneracy. A particular instance is the Newman-Watts small-world model. Second, we show logarithmic average mixing time for supercritically percolated expander graphs. When the host graph is complete, this application gives an alternative proof that the average mixing time of the giant component in the supercritical ErdÅs-RÃ©nyi graph is logarithmic.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2208.03936.pdf' target='_blank'>https://arxiv.org/pdf/2208.03936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taisuke Kobayashi, Ryoma Watanuki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03936">Sparse Representation Learning with Modified q-VAE towards Minimal Realization of World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extraction of low-dimensional latent space from high-dimensional observation data is essential to construct a real-time robot controller with a world model on the extracted latent space. However, there is no established method for tuning the dimension size of the latent space automatically, suffering from finding the necessary and sufficient dimension size, i.e. the minimal realization of the world model. In this study, we analyze and improve Tsallis-based variational autoencoder (q-VAE), and reveal that, under an appropriate configuration, it always facilitates making the latent space sparse. Even if the dimension size of the pre-specified latent space is redundant compared to the minimal realization, this sparsification collapses unnecessary dimensions, allowing for easy removal of them. We experimentally verified the benefits of the sparsification by the proposed method that it can easily find the necessary and sufficient six dimensions for a reaching task with a mobile manipulator that requires a six-dimensional state space. Moreover, by planning with such a minimal-realization world model learned in the extracted dimensions, the proposed method was able to exert a more optimal action sequence in real-time, reducing the reaching accomplishment time by around 20 %. The attached video is uploaded on youtube: https://youtu.be/-QjITrnxaRs
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2510.10325.pdf' target='_blank'>https://arxiv.org/pdf/2510.10325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Walid Abdela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10325">KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The seamless integration of physical and digital environments in Cyber-Physical Systems(CPS), particularly within Industry 4.0, presents significant challenges stemming from system heterogeneity and complexity. Traditional approaches often rely on rigid, data-centric solutions like co-simulation frameworks or brittle point-to-point middleware bridges, which lack the semantic richness and flexibility required for intelligent, autonomous coordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent Infrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS leverages a centralized Knowledge Graph (KG) as a dynamic, shared world model, providing a common semantic foundation for a Multi-Agent System(MAS). Autonomous agents, representing both physical and digital components, query this KG for decision-making and update it with real-time state information. The infrastructure features a model-driven architecture which facilitates the automatic generation of agents from semantic descriptions, thereby simplifying system extension and maintenance. By abstracting away underlying communication protocols and providing a unified, intelligent coordination mechanism, KG-MAS offers a robust, scalable, and flexible solution for coupling heterogeneous physical and digital robotic environments.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2510.03727.pdf' target='_blank'>https://arxiv.org/pdf/2510.03727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03727">Bridging the Gap Between Multimodal Foundation Models and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2510.03727.pdf' target='_blank'>https://arxiv.org/pdf/2510.03727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03727">Bridging the Gap Between Multimodal Foundation Models and World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2509.12387.pdf' target='_blank'>https://arxiv.org/pdf/2509.12387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Zayaan S
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12387">Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2509.12387.pdf' target='_blank'>https://arxiv.org/pdf/2509.12387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Zayaan S
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12387">Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern deep learning models excel at pattern recognition but remain fundamentally limited by their reliance on spurious correlations, leading to poor generalization and a demand for massive datasets. We argue that a key ingredient for human-like intelligence-robust, sample-efficient learning-stems from an understanding of causal mechanisms. In this work, we introduce Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer the latent causal structure of a task distribution. CSML comprises three key modules: a perception module that maps raw inputs to disentangled symbolic representations; a differentiable causal induction module that discovers the underlying causal graph governing these symbols and a graph-based reasoning module that leverages this graph to make predictions. By meta-learning a shared causal world model across a distribution of tasks, CSML can rapidly adapt to novel tasks, including those requiring reasoning about interventions and counterfactuals, from only a handful of examples. We introduce CausalWorld, a new physics-based benchmark designed to test these capabilities. Our experiments show that CSML dramatically outperforms state-of-the-art meta-learning and neuro-symbolic baselines, particularly on tasks demanding true causal inference.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2509.04633.pdf' target='_blank'>https://arxiv.org/pdf/2509.04633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04633">The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2509.04633.pdf' target='_blank'>https://arxiv.org/pdf/2509.04633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04633">The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2508.15859.pdf' target='_blank'>https://arxiv.org/pdf/2508.15859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tadahiro Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15859">Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This commentary extends the discussion by Parr et al. on memory and attention beyond individual cognitive systems. From the perspective of the Collective Predictive Coding (CPC) hypothesis -- a framework for understanding these faculties and the emergence of language at the group level -- we introduce a hypothetical idea: that language, with its embedded distributional semantics, serves as a collectively formed external representation. CPC generalises the concepts of individual memory and attention to the collective level. This offers a new perspective on how shared linguistic structures, which may embrace collective world models learned through next-word prediction, emerge from and shape group-level cognition.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2508.05619.pdf' target='_blank'>https://arxiv.org/pdf/2508.05619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05619">The Missing Reward: Active Inference in the Era of Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience,'' where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIF's principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2506.12795.pdf' target='_blank'>https://arxiv.org/pdf/2506.12795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Bennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12795">Resilient-native and Intelligent NextG Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Just like power, water and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient to unforeseen events, able to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Despite its critical importance, resilience remains an elusive concept, with its mathematical foundations still underdeveloped. Unlike robustness and reliability, resilience is premised on the fact that disruptions will inevitably happen. Resilience, in terms of elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents (or networks) that can flexibly expand their states, hypotheses and course of actions, by transforming through real-time adaptation and reconfiguration. This constant situational awareness and vigilance of adapting world models and counterfactually reasoning about potential system failures and the corresponding best responses, is a core aspect of resilience. This article seeks to first define resilience and disambiguate it from reliability and robustness, before delving into the mathematics of resilience. Finally, the article concludes by presenting nuanced metrics and discussing trade-offs tailored to the unique characteristics of network resilience.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2506.12161.pdf' target='_blank'>https://arxiv.org/pdf/2506.12161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio Ferreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12161">Meta-Learning and Synthetic Data for Automated Pretraining and Finetuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing number of pretrained models in Machine Learning (ML) presents significant challenges for practitioners. Given a new dataset, they need to determine the most suitable deep learning (DL) pipeline, consisting of the pretrained model and the hyperparameters for finetuning to it. Moreover, as models grow in scale, the increasing reliance on real-world data poses a bottleneck for training and requires leveraging data more effectively. Addressing the first challenge often involves manual model selection and hyperparameter tuning. At the same time, as models grow larger and more and more of the available human-generated data is being used for training, data augmentation and synthetic data become critical elements. Automated machine learning offers a path to address these challenges but is traditionally designed for tabular data and classical ML methods. This dissertation adopts meta-learning to extend automated machine learning to the deep learning domain. We propose empirical approaches to automate DL pipeline selection for Computer Vision tasks using prior task knowledge to learn surrogate models for pipeline ranking. Extending these methods to the language domain, we learn to finetune large language models. As a result, we show that our approach can outperform finetuning foundation models. Additionally, we meta-learn data augmentation and synthetic data to enhance performance in up-stream and down-stream tasks. We empirically show the underestimated importance of data augmentation when using Self-Supervised Learning and meta-learn advanced data augmentation strategies. Leveraging synthetic data, we also propose to meta-learn neural synthetic data generators as proxies for Reinforcement Learning (RL) environments. Additionally, we learn a multiple-environment world model in an in-context learning fashion by purely using synthetic, randomly sampled data.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2504.03861.pdf' target='_blank'>https://arxiv.org/pdf/2504.03861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrii Zahorodnii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03861">Improving World Models using Deep Supervision with Linear Probes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing effective world models is crucial for creating artificial agents that can reason about and navigate complex environments. In this paper, we investigate a deep supervision technique for encouraging the development of a world model in a network trained end-to-end to predict the next observation. While deep supervision has been widely applied for task-specific learning, our focus is on improving the world models. Using an experimental environment based on the Flappy Bird game, where the agent receives only LIDAR measurements as observations, we explore the effect of adding a linear probe component to the network's loss function. This additional term encourages the network to encode a subset of the true underlying world features into its hidden state. Our experiments demonstrate that this supervision technique improves both training and test performance, enhances training stability, and results in more easily decodable world features -- even for those world features which were not included in the training. Furthermore, we observe a reduced distribution drift in networks trained with the linear probe, particularly during high-variability phases of the game (flying between successive pipe encounters). Including the world features loss component roughly corresponded to doubling the model size, suggesting that the linear probe technique is particularly beneficial in compute-limited settings or when aiming to achieve the best performance with smaller models. These findings contribute to our understanding of how to develop more robust and sophisticated world models in artificial agents, paving the way for further advancements in this field.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2503.08872.pdf' target='_blank'>https://arxiv.org/pdf/2503.08872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cameron Redovian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08872">Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We integrate a meta-reinforcement learning algorithm with the DreamerV3 architecture to improve load balancing in operating systems. This approach enables rapid adaptation to dynamic workloads with minimal retraining, outperforming the Advantage Actor-Critic (A2C) algorithm in standard and adaptive trials. It demonstrates robust resilience to catastrophic forgetting, maintaining high performance under varying workload distributions and sizes. These findings have important implications for optimizing resource management and performance in modern operating systems. By addressing the challenges posed by dynamic and heterogeneous workloads, our approach advances the adaptability and efficiency of reinforcement learning in real-world system management tasks.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2503.07600.pdf' target='_blank'>https://arxiv.org/pdf/2503.07600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rolf Pfister
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07600">A Representationalist, Functionalist and Naturalistic Conception of Intelligence as a Foundation for AGI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The article analyses foundational principles relevant to the creation of artificial general intelligence (AGI). Intelligence is understood as the ability to create novel skills that allow to achieve goals under previously unknown conditions. To this end, intelligence utilises reasoning methods such as deduction, induction and abduction as well as other methods such as abstraction and classification to develop a world model. The methods are applied to indirect and incomplete representations of the world, which are obtained through perception, for example, and which do not depict the world but only correspond to it. Due to these limitations and the uncertain and contingent nature of reasoning, the world model is constructivist. Its value is functionally determined by its viability, i.e., its potential to achieve the desired goals. In consequence, meaning is assigned to representations by attributing them a function that makes it possible to achieve a goal. This representational and functional conception of intelligence enables a naturalistic interpretation that does not presuppose mental features, such as intentionality and consciousness, which are regarded as independent of intelligence. Based on a phenomenological analysis, it is shown that AGI can gain a more fundamental access to the world than humans, although it is limited by the No Free Lunch theorems, which require assumptions to be made.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2503.03529.pdf' target='_blank'>https://arxiv.org/pdf/2503.03529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David S. Johnson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03529">Higher Stakes, Healthier Trust? An Application-Grounded Approach to Assessing Healthy Trust in High-Stakes Human-AI Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-AI collaboration is increasingly promoted to improve high-stakes decision-making, yet its benefits have not been fully realized. Application-grounded evaluations are needed to better evaluate methods for improving collaboration but often require domain experts, making studies costly and limiting their generalizability. Current evaluation methods are constrained by limited public datasets and reliance on proxy tasks. To address these challenges, we propose an application-grounded framework for large-scale, online evaluations of vision-based decision-making tasks. The framework introduces Blockies, a parametric approach for generating datasets of simulated diagnostic tasks, offering control over the traits and biases in the data used to train real-world models. These tasks are designed to be easy to learn but difficult to master, enabling participation by non-experts. The framework also incorporates storytelling and monetary incentives to manipulate perceived task stakes. An initial empirical study demonstrated that the high-stakes condition significantly reduced healthy distrust of AI, despite longer decision-making times. These findings underscore the importance of perceived stakes in fostering healthy distrust and demonstrate the framework's potential for scalable evaluation of high-stakes Human-AI collaboration.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2503.00727.pdf' target='_blank'>https://arxiv.org/pdf/2503.00727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maijunxian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00727">From Understanding the World to Intervening in It: A Unified Multi-Scale Framework for Embodied Cognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose AUKAI, an Adaptive Unified Knowledge-Action Intelligence for embodied cognition that seamlessly integrates perception, memory, and decision-making via multi-scale error feedback. Interpreting AUKAI as an embedded world model, our approach simultaneously predicts state transitions and evaluates intervention utility. The framework is underpinned by rigorous theoretical analysis drawn from convergence theory, optimal control, and Bayesian inference, which collectively establish conditions for convergence, stability, and near-optimal performance. Furthermore, we present a hybrid implementation that combines the strengths of neural networks with symbolic reasoning modules, thereby enhancing interpretability and robustness. Finally, we demonstrate the potential of AUKAI through a detailed application in robotic navigation and obstacle avoidance, and we outline comprehensive experimental plans to validate its effectiveness in both simulated and real-world environments.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2412.10908.pdf' target='_blank'>https://arxiv.org/pdf/2412.10908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagi Eppel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10908">Do large language vision models understand 3D shapes?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision language models (LVLM) are the leading A.I approach for achieving a general visual understanding of the world. Models such as GPT, Claude, Gemini, and LLama can use images to understand and analyze complex visual scenes. 3D objects and shapes are the basic building blocks of the world, recognizing them is a fundamental part of human perception. The goal of this work is to test whether LVLMs truly understand 3D shapes by testing the models ability to identify and match objects of the exact same 3D shapes but with different orientations and materials/textures. A large number of test images were created using CGI with a huge number of highly diverse objects, materials, and scenes. The results of this test show that the ability of such models to match 3D shapes is significantly below humans but much higher than random guesses. Suggesting that the models have gained some abstract understanding of 3D shapes but still trail far beyond humans in this task. Mainly it seems that the models can easily identify the same object with a different orientation as well as matching identical 3D shapes of the same orientation but with different materials and textures. However, when both the object material and orientation are changed, all models perform poorly relative to humans. Code and benchmark are available.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2411.15234.pdf' target='_blank'>https://arxiv.org/pdf/2411.15234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mackenzie Weygandt Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15234">Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Biological intelligence is inherently adaptive -- animals continually adjust their actions based on environmental feedback. However, creating adaptive artificial intelligence (AI) remains a major challenge. The next frontier is to go beyond traditional AI to develop "adaptive intelligence," defined here as harnessing insights from biological intelligence to build agents that can learn online, generalize, and rapidly adapt to changes in their environment. Recent advances in neuroscience offer inspiration through studies that increasingly focus on how animals naturally learn and adapt their world models. In this Perspective, I will review the behavioral and neural foundations of adaptive biological intelligence, the parallel progress in AI, and explore brain-inspired approaches for building more adaptive algorithms.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2409.18676.pdf' target='_blank'>https://arxiv.org/pdf/2409.18676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lancelot Da Costa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18676">Toward Universal and Interpretable World Models for Open-ended Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a generic, compositional and interpretable class of generative world models that supports open-ended learning agents. This is a sparse class of Bayesian networks capable of approximating a broad range of stochastic processes, which provide agents with the ability to learn world models in a manner that may be both interpretable and computationally scalable. This approach integrating Bayesian structure learning and intrinsically motivated (model-based) planning enables agents to actively develop and refine their world models, which may lead to developmental learning and more robust, adaptive behavior.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2407.10311.pdf' target='_blank'>https://arxiv.org/pdf/2407.10311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianqiu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10311">Sora and V-JEPA Have Not Learned The Complete Real World Model -- A Philosophical Analysis of Video AIs Through the Theory of Productive Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sora from Open AI has shown exceptional performance, yet it faces scrutiny over whether its technological prowess equates to an authentic comprehension of reality. Critics contend that it lacks a foundational grasp of the world, a deficiency V-JEPA from Meta aims to amend with its joint embedding approach. This debate is vital for steering the future direction of Artificial General Intelligence(AGI). We enrich this debate by developing a theory of productive imagination that generates a coherent world model based on Kantian philosophy. We identify three indispensable components of the coherent world model capable of genuine world understanding: representations of isolated objects, an a priori law of change across space and time, and Kantian categories. Our analysis reveals that Sora is limited because of its oversight of the a priori law of change and Kantian categories, flaws that are not rectifiable through scaling up the training. V-JEPA learns the context-dependent aspect of the a priori law of change. Yet it fails to fully comprehend Kantian categories and incorporate experience, leading us to conclude that neither system currently achieves a comprehensive world understanding. Nevertheless, each system has developed components essential to advancing an integrated AI productive imagination-understanding engine. Finally, we propose an innovative training framework for an AI productive imagination-understanding engine, centered around a joint embedding system designed to transform disordered perceptual input into a structured, coherent world model. Our philosophical analysis pinpoints critical challenges within contemporary video AI technologies and a pathway toward achieving an AI system capable of genuine world understanding, such that it can be applied for reasoning and planning in the future.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2407.07845.pdf' target='_blank'>https://arxiv.org/pdf/2407.07845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Della Penna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07845">Natural Language Mechanisms via Self-Resolution with Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Practical mechanisms often limit agent reports to constrained formats like trades or orderings, potentially limiting the information agents can express. We propose a novel class of mechanisms that elicit agent reports in natural language and leverage the world-modeling capabilities of large language models (LLMs) to select outcomes and assign payoffs. We identify sufficient conditions for these mechanisms to be incentive-compatible and efficient as the LLM being a good enough world model and a strong inter-agent information over-determination condition. We show situations where these LM-based mechanisms can successfully aggregate information in signal structures on which prediction markets fail.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2404.16078.pdf' target='_blank'>https://arxiv.org/pdf/2404.16078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaisakh Shaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16078">Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machines that can replicate human intelligence with type 2 reasoning capabilities should be able to reason at multiple levels of spatio-temporal abstractions and scales using internal world models. Devising formalisms to develop such internal world models, which accurately reflect the causal hierarchies inherent in the dynamics of the real world, is a critical research challenge in the domains of artificial intelligence and machine learning. This thesis identifies several limitations with the prevalent use of state space models (SSMs) as internal world models and propose two new probabilistic formalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address these drawbacks. The structure of graphical models in both formalisms facilitates scalable exact probabilistic inference using belief propagation, as well as end-to-end learning via backpropagation through time. This approach permits the development of scalable, adaptive hierarchical world models capable of representing nonstationary dynamics across multiple temporal abstractions and scales. Moreover, these probabilistic formalisms integrate the concept of uncertainty in world states, thus improving the system's capacity to emulate the stochastic nature of the real world and quantify the confidence in its predictions. The thesis also discuss how these formalisms are in line with related neuroscience literature on Bayesian brain hypothesis and predicitive processing. Our experiments on various real and simulated robots demonstrate that our formalisms can match and in many cases exceed the performance of contemporary transformer variants in making long-range future predictions. We conclude the thesis by reflecting on the limitations of our current models and suggesting directions for future research.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2403.15498.pdf' target='_blank'>https://arxiv.org/pdf/2403.15498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Karvonen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15498">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2402.10992.pdf' target='_blank'>https://arxiv.org/pdf/2402.10992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Holger Lyre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10992">"Understanding AI": Semantic Grounding in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2402.06046.pdf' target='_blank'>https://arxiv.org/pdf/2402.06046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philip Koopman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.06046">Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. Is-sues stem not just from the loss events themselves, but also from how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. External investigation reports provide raw material describing the incident and critique the company's response from a regulatory point of view, but exclude safety engineering recommendations from scope. We highlight specific facts and relationships among events by tying together different pieces of the external report material. We then explore safety lessons that might be learned related to: recognizing and responding to nearby mishaps, building an accurate world model of a post-collision scenario, the in-adequacy of a so-called "minimal risk condition" strategy in complex situations, poor organizational discipline in responding to a mishap, overly aggressive post-collision automation choices that made a bad situation worse, and a reluctance to admit to a mishap causing much worse organizational harm down-stream.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2308.16307.pdf' target='_blank'>https://arxiv.org/pdf/2308.16307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjae Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16307">Implementation Of MNIST Dataset Learning Using Analog Circuit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There have been many attempts to implement neural networks in the analog circuit. Most of them had a lot of input terms, and most studies implemented neural networks in the analog circuit through a circuit simulation program called Spice to avoid the need to design chips at a high cost and implement circuits directly to input them. In this study, we will implement neural networks using a capacitor and diode and use microcontrollers (Arduino Mega 2560 R3 boards) to drive real-world models and analyze the results.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2307.13473.pdf' target='_blank'>https://arxiv.org/pdf/2307.13473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Awadelrahman M. A. Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13473">Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.
  The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.
  The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases from a main phase under focus, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study unveils the dynamic and iterative nature of the MLOps workflow.
  The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.
  Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2306.09179.pdf' target='_blank'>https://arxiv.org/pdf/2306.09179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09179">Neural World Models for Computer Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans navigate in their environment by learning a mental model of the world through passive observation and active interaction. Their world model allows them to anticipate what might happen next and act accordingly with respect to an underlying objective. Such world models hold strong promises for planning in complex environments like in autonomous driving. A human driver, or a self-driving system, perceives their surroundings with their eyes or their cameras. They infer an internal representation of the world which should: (i) have spatial memory (e.g. occlusions), (ii) fill partially observable or noisy inputs (e.g. when blinded by sunlight), and (iii) be able to reason about unobservable events probabilistically (e.g. predict different possible futures). They are embodied intelligent agents that can predict, plan, and act in the physical world through their world model. In this thesis we present a general framework to train a world model and a policy, parameterised by deep neural networks, from camera observations and expert demonstrations. We leverage important computer vision concepts such as geometry, semantics, and motion to scale world models to complex urban driving scenes.
  First, we propose a model that predicts important quantities in computer vision: depth, semantic segmentation, and optical flow. We then use 3D geometry as an inductive bias to operate in the bird's-eye view space. We present for the first time a model that can predict probabilistic future trajectories of dynamic agents in bird's-eye view from 360Â° surround monocular cameras only. Finally, we demonstrate the benefits of learning a world model in closed-loop driving. Our model can jointly predict static scene, dynamic scene, and ego-behaviour in an urban driving environment.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2302.08590.pdf' target='_blank'>https://arxiv.org/pdf/2302.08590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Schlangen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08590">What A Situated Language-Using Agent Must be Able to Do: A Top-Down Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Even in our increasingly text-intensive times, the primary site of language use is situated, co-present interaction. It is primary ontogenetically and phylogenetically, and it is arguably also still primary in negotiating everyday social situations. Situated interaction is also the final frontier of Natural Language Processing, where, compared to the area of text processing, very little progress has been made in the past decade, and where a myriad of practical applications is waiting to be unlocked. While the usual approach in the field is to reach, bottom-up, for the ever next "adjacent possible", in this paper I attempt a top-down analysis of what the demands are that unrestricted situated interaction makes on the participating agent, and suggest ways in which this analysis can structure computational models and research on them. Specifically, I discuss representational demands (the building up and application of world model, language model, situation model, discourse model, and agent model) and what I call anchoring processes (incremental processing, incremental learning, conversational grounding, multimodal grounding) that bind the agent to the here, now, and us.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2110.09442.pdf' target='_blank'>https://arxiv.org/pdf/2110.09442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Robinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.09442">Goal Agnostic Planning using Maximum Likelihood Paths in Hypergraph World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a hypergraph--based machine learning algorithm, a datastructure--driven maintenance method, and a planning algorithm based on a probabilistic application of Dijkstra's algorithm. Together, these form a goal agnostic automated planning engine for an autonomous learning agent which incorporates beneficial properties of both classical Machine Learning and traditional Artificial Intelligence. We prove that the algorithm determines optimal solutions within the problem space, mathematically bound learning performance, and supply a mathematical model analyzing system state progression through time yielding explicit predictions for learning curves, goal achievement rates, and response to abstractions and uncertainty. To validate performance, we exhibit results from applying the agent to three archetypal planning problems, including composite hierarchical domains, and highlight empirical findings which illustrate properties elucidated in the analysis.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2104.03611.pdf' target='_blank'>https://arxiv.org/pdf/2104.03611.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Pastor-Escuredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.03611">Digital Epidemiology: A review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The epidemiology has recently witnessed great advances based on computational models. Its scope and impact are getting wider thanks to the new data sources feeding analytical frameworks and models. Besides traditional variables considered in epidemiology, large-scale social patterns can be now integrated in real time with multi-source data bridging the gap between different scales. In a hyper-connected world, models and analysis of interactions and social behaviors are key to understand and stop outbreaks. Big Data along with apps are enabling for validating and refining models with real world data at scale, as well as new applications and frameworks to map and track diseases in real time or optimize the necessary resources and interventions such as testing and vaccination strategies. Digital epidemiology is positioning as a discipline necessary to control epidemics and implement actionable protocols and policies. In this review we address the research areas configuring current digital epidemiology: transmission and propagation models and descriptions based on human networks and contact tracing, mobility analysis and spatio-temporal propagation of infectious diseases and infodemics that comprises the study of information and knowledge propagation. Digital epidemiology has the potential to create new operational mechanisms for prevention and mitigation, monitoring of the evolution of epidemics, assessing their impact and evaluating the pharmaceutical and non-pharmaceutical measures to fight the outbreaks. Epidemics have to be approached from the lens of complexity as they require systemic solutions. Opportunities and challenges to tackle epidemics more effectively and with a human-centered vision are here discussed.
